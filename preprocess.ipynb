{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded008cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "def resample_to_fixed_cell_size(src_path, dst_path, crs, width, height, cell_size=30):\n",
    "    \"\"\"\n",
    "    Resample a raster to fixed dimensions and convert to float32 with -9999 nodata.\n",
    "    \n",
    "    Args:\n",
    "        src_path: Path to the source raster\n",
    "        dst_path: Path to save the resampled raster\n",
    "        crs: Target coordinate reference system\n",
    "        width: Fixed width to use for all rasters\n",
    "        height: Fixed height to use for all rasters\n",
    "        cell_size: Target cell size in CRS units\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(src_path) as src:\n",
    "            # Store original nodata value\n",
    "            original_nodata = src.nodata\n",
    "            \n",
    "            # Calculate bounds in the target CRS\n",
    "            west, south, east, north = rasterio.warp.transform_bounds(\n",
    "                src.crs, crs, *src.bounds\n",
    "            )\n",
    "            \n",
    "            # Use the specified fixed dimensions\n",
    "            # Note: width and height are already ensured to be divisible by 128 in the calling function\n",
    "            \n",
    "            # Calculate the transformation matrix\n",
    "            dst_transform = rasterio.transform.from_bounds(\n",
    "                west, south, east, north, width, height\n",
    "            )\n",
    "            \n",
    "            # Update profile for the new raster\n",
    "            dst_kwargs = src.profile.copy()\n",
    "            dst_kwargs.update({\n",
    "                'crs': crs,\n",
    "                'transform': dst_transform,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'dtype': 'int16',  # Set dtype to float32\n",
    "                'nodata': 0    # Set nodata to -9999\n",
    "            })\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            \n",
    "            # Create the new raster\n",
    "            with rasterio.open(dst_path, 'w', **dst_kwargs) as dst:\n",
    "                # Reproject and resample\n",
    "                for i in range(1, src.count + 1):\n",
    "                    # Read the source band\n",
    "                    source_data = src.read(i)\n",
    "                    \n",
    "                    # Create a destination array filled with the nodata value\n",
    "                    dest_data = np.full(\n",
    "                        (dst_kwargs['height'], dst_kwargs['width']), \n",
    "                        0, \n",
    "                        dtype='int16'\n",
    "                    )\n",
    "                    \n",
    "                    # Reproject with specified parameters\n",
    "                    reproject(\n",
    "                        source=source_data,\n",
    "                        destination=dest_data,\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=dst_transform,\n",
    "                        dst_crs=crs,\n",
    "                        src_nodata=original_nodata,\n",
    "                        dst_nodata=-0,\n",
    "                        resampling=Resampling.nearest,\n",
    "                        num_threads=4\n",
    "                    )\n",
    "                    \n",
    "                    # Write the result\n",
    "                    dst.write(dest_data, i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {src_path}: {e}\")\n",
    "\n",
    "def processScene(albedo_files):\n",
    "    # Process each scene\n",
    "    for albedo_path in tqdm(albedo_files, desc='Preprocessing images...'):\n",
    "        scene_dir = os.path.dirname(albedo_path)\n",
    "        scene_files = [f for f in os.listdir(scene_dir) if os.path.isfile(os.path.join(scene_dir, f))]\n",
    "        \n",
    "        # Get all raster paths for this scene\n",
    "        raster_paths = []\n",
    "        for raster_file in scene_files:\n",
    "            src_path = os.path.join(scene_dir, raster_file)\n",
    "            raster_paths.append(src_path)\n",
    "        \n",
    "        # Get reference CRS from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            reference_crs = src.crs\n",
    "        \n",
    "        # Find minimum dimensions across all rasters in the scene\n",
    "        min_width = float('inf')\n",
    "        min_height = float('inf')\n",
    "        cell_size = 30\n",
    "        for path in raster_paths:\n",
    "            try:\n",
    "                with rasterio.open(path) as scene_src:\n",
    "                    # Calculate bounds in the target CRS\n",
    "                    west, south, east, north = rasterio.warp.transform_bounds(\n",
    "                        scene_src.crs, reference_crs, *scene_src.bounds\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate dimensions based on cell size\n",
    "                    width = max(int(round((east - west) / cell_size)), 1)\n",
    "                    height = max(int(round((north - south) / cell_size)), 1)\n",
    "                    \n",
    "                    # Update minimum dimensions\n",
    "                    min_width = min(min_width, width)\n",
    "                    min_height = min(min_height, height)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading dimensions from {path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Ensure dimensions are divisible by 128\n",
    "        min_width = ((min_width + 127) // 128) * 128\n",
    "        min_height = ((min_height + 127) // 128) * 128\n",
    "        \n",
    "        # Process each raster with the standardized dimensions\n",
    "        for k, src_path in enumerate(raster_paths):\n",
    "            dst_path = src_path.replace('Cities/', 'Cities_Preprocessed/')        \n",
    "            dst_path = dst_path.replace('DEM_2014/', 'DEM_2014_Preprocessed/') \n",
    "            dst_path = dst_path.replace('Dataset/', 'ML/')\n",
    "            # Skip if already processed\n",
    "            if os.path.exists(dst_path):\n",
    "                continue\n",
    "            \n",
    "            resample_to_fixed_cell_size(src_path, dst_path, reference_crs, min_width, min_height, cell_size)\n",
    "\n",
    "def preprocessImages(data_dir: str, debug: bool):\n",
    "    albedo_files = []\n",
    "    dem_files = []\n",
    "    \n",
    "    # Find all albedo files from 2014\n",
    "    for file_path in tqdm(get_file_paths(data_dir), desc='Gathering scenes (Preprocessing)...'):\n",
    "        date = file_path.split('/')[-2]\n",
    "        if 'DEM.tif' in file_path:\n",
    "            dem_files.append(file_path)\n",
    "        if debug and date[:4] not in [\"2013\", \"2014\"]:#, \"2015\", \"2016\", \"2017\"]:\n",
    "            # print(date[:4])\n",
    "            continue\n",
    "        if 'albedo' in file_path:\n",
    "            albedo_files.append(file_path)\n",
    "    processScene(albedo_files)\n",
    "    processScene(dem_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ef7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def calculate_nodata_percentage(tiff_path):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of NODATA values in a TIFF file using rioxarray.\n",
    "    \n",
    "    Args:\n",
    "        tiff_path (str): Path to the TIFF file\n",
    "        \n",
    "    Returns:\n",
    "        float: Percentage of NODATA values (0-100)\n",
    "        None: If file cannot be processed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the dataset with rioxarray\n",
    "        dataset = rxr.open_rasterio(tiff_path, masked=True)\n",
    "        \n",
    "        if dataset is None:\n",
    "            logger.warning(f\"Could not open file: {tiff_path}\")\n",
    "            return None\n",
    "        \n",
    "        total_pixels = 0\n",
    "        nodata_pixels = 0\n",
    "        \n",
    "        # Process each band (dimension in xarray)\n",
    "        for band_idx in range(dataset.sizes['band']):\n",
    "            band_data = dataset.isel(band=band_idx)\n",
    "            \n",
    "            # Count total pixels in this band\n",
    "            band_total_pixels = band_data.size\n",
    "            total_pixels += band_total_pixels\n",
    "            \n",
    "            # Count NODATA pixels\n",
    "            # rioxarray automatically masks NODATA values when masked=True\n",
    "            if hasattr(band_data, 'mask') and band_data.mask is not None:\n",
    "                # Count masked (NODATA) pixels\n",
    "                band_nodata_pixels = np.sum(band_data.mask)\n",
    "            else:\n",
    "                # Check for NaN values if no mask is present\n",
    "                band_nodata_pixels = np.sum(np.isnan(band_data.values))\n",
    "            \n",
    "            nodata_pixels += band_nodata_pixels\n",
    "        \n",
    "        # Close the dataset\n",
    "        dataset.close()\n",
    "        \n",
    "        # Calculate percentage\n",
    "        if total_pixels > 0:\n",
    "            percentage = (nodata_pixels / total_pixels) * 100\n",
    "            return percentage\n",
    "        else:\n",
    "            return 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {tiff_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_all_tiff_files(root_directory):\n",
    "    \"\"\"\n",
    "    Recursively find all TIFF files in the given directory and its subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        root_directory (str): Root directory to search\n",
    "        \n",
    "    Returns:\n",
    "        list: List of paths to TIFF files\n",
    "    \"\"\"\n",
    "    tiff_extensions = ['*.tif', '*.tiff', '*.TIF', '*.TIFF']\n",
    "    tiff_files = []\n",
    "    \n",
    "    root_path = Path(root_directory)\n",
    "    \n",
    "    for extension in tiff_extensions:\n",
    "        # Use recursive glob to find all files with this extension\n",
    "        pattern = f\"**/{extension}\"\n",
    "        files = list(root_path.glob(pattern))\n",
    "        tiff_files.extend([str(f) for f in files])\n",
    "    \n",
    "    return tiff_files\n",
    "\n",
    "def calculate_mean_nodata_percentage(dataset_path):\n",
    "    \"\"\"\n",
    "    Calculate the mean percentage of NODATA values across all TIFF files in a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistics\n",
    "    \"\"\"\n",
    "    logger.info(f\"Searching for TIFF files in: {dataset_path}\")\n",
    "    \n",
    "    # Find all TIFF files\n",
    "    tiff_files = find_all_tiff_files(dataset_path)\n",
    "    \n",
    "    if not tiff_files:\n",
    "        logger.warning(\"No TIFF files found in the specified directory.\")\n",
    "        return {\n",
    "            'mean_nodata_percentage': 0.0,\n",
    "            'total_files': 0,\n",
    "            'processed_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'individual_percentages': []\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Found {len(tiff_files)} TIFF files\")\n",
    "    \n",
    "    # Calculate NODATA percentage for each file\n",
    "    nodata_percentages = []\n",
    "    failed_files = 0\n",
    "    \n",
    "    for i, tiff_path in enumerate(tiff_files, 1):\n",
    "        logger.info(f\"Processing file {i}/{len(tiff_files)}: {os.path.basename(tiff_path)}\")\n",
    "        \n",
    "        percentage = calculate_nodata_percentage(tiff_path)\n",
    "        \n",
    "        if percentage is not None:\n",
    "            nodata_percentages.append(percentage)\n",
    "            logger.info(f\"  NODATA percentage: {percentage:.2f}%\")\n",
    "        else:\n",
    "            failed_files += 1\n",
    "            logger.warning(f\"  Failed to process file\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if nodata_percentages:\n",
    "        mean_percentage = np.mean(nodata_percentages)\n",
    "        std_percentage = np.std(nodata_percentages)\n",
    "        min_percentage = np.min(nodata_percentages)\n",
    "        max_percentage = np.max(nodata_percentages)\n",
    "        \n",
    "        results = {\n",
    "            'mean_nodata_percentage': mean_percentage,\n",
    "            'std_nodata_percentage': std_percentage,\n",
    "            'min_nodata_percentage': min_percentage,\n",
    "            'max_nodata_percentage': max_percentage,\n",
    "            'total_files': len(tiff_files),\n",
    "            'processed_files': len(nodata_percentages),\n",
    "            'failed_files': failed_files,\n",
    "            'individual_percentages': nodata_percentages\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"\\n--- RESULTS ---\")\n",
    "        logger.info(f\"Total files found: {results['total_files']}\")\n",
    "        logger.info(f\"Successfully processed: {results['processed_files']}\")\n",
    "        logger.info(f\"Failed to process: {results['failed_files']}\")\n",
    "        logger.info(f\"Mean NODATA percentage: {results['mean_nodata_percentage']:.2f}%\")\n",
    "        logger.info(f\"Standard deviation: {results['std_nodata_percentage']:.2f}%\")\n",
    "        logger.info(f\"Minimum NODATA percentage: {results['min_nodata_percentage']:.2f}%\")\n",
    "        logger.info(f\"Maximum NODATA percentage: {results['max_nodata_percentage']:.2f}%\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        logger.error(\"No files were successfully processed.\")\n",
    "        return {\n",
    "            'mean_nodata_percentage': 0.0,\n",
    "            'total_files': len(tiff_files),\n",
    "            'processed_files': 0,\n",
    "            'failed_files': failed_files,\n",
    "            'individual_percentages': []\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the NODATA percentage calculator.\n",
    "    \"\"\"\n",
    "    # You can modify this path to point to your dataset directory\n",
    "    dataset_path = \"./Data/Dataset/Cities\"\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        logger.error(f\"Directory does not exist: {dataset_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.isdir(dataset_path):\n",
    "        logger.error(f\"Path is not a directory: {dataset_path}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate mean NODATA percentage\n",
    "    results = calculate_mean_nodata_percentage(dataset_path)\n",
    "        \n",
    "    with open(\"out.txt\", 'a') as f:\n",
    "        f.write(\"TIFF NODATA Percentage Analysis Results\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Dataset Path: {dataset_path}\\n\")\n",
    "        f.write(f\"Total files found: {results['total_files']}\\n\")\n",
    "        f.write(f\"Successfully processed: {results['processed_files']}\\n\")\n",
    "        f.write(f\"Failed to process: {results['failed_files']}\\n\")\n",
    "        f.write(f\"Mean NODATA percentage: {results['mean_nodata_percentage']:.2f}%\\n\")\n",
    "        if 'std_nodata_percentage' in results:\n",
    "            f.write(f\"Standard deviation: {results['std_nodata_percentage']:.2f}%\\n\")\n",
    "            f.write(f\"Minimum NODATA percentage: {results['min_nodata_percentage']:.2f}%\\n\")\n",
    "            f.write(f\"Maximum NODATA percentage: {results['max_nodata_percentage']:.2f}%\\n\")\n",
    "        f.write(f\"\\nIndividual file percentages:\\n\")\n",
    "        for i, percentage in enumerate(results['individual_percentages'], 1):\n",
    "            f.write(f\"File {i}: {percentage:.2f}%\\n\")\n",
    "        \n",
    "        logger.info(f\"Results saved to: out.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c259b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessImages(\"./Data/Dataset\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def interpolateLinearScenePixels(data_dir=\"./Data/ML/Cities_Preprocessed\", monthSpan=6, tolerance_for_missing=0.4):\n",
    "    '''\n",
    "    Take scenes from Cities_Preprocessed and perform linear interpolation for missing scenes\n",
    "    within month spans. For each missing month, interpolate between the nearest original \n",
    "    scenes before and after it, doing linear interpolation per pixel.\n",
    "    '''\n",
    "    output_dir = \"./Data/ML/Cities_Processed\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # File types to process\n",
    "    file_types = ['albedo.tif', 'blue.tif', 'green.tif', 'LST.tif', \n",
    "                  'ndbi.tif', 'ndvi.tif', 'ndwi.tif', 'red.tif']\n",
    "    \n",
    "    # Get list of cities\n",
    "    cities = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    # Iterate through each city with progress bar\n",
    "    for city_name in tqdm(cities, desc='Processing cities'):\n",
    "        city_path = os.path.join(data_dir, city_name)\n",
    "        \n",
    "        # First, copy all original scenes to output\n",
    "        output_city_path = os.path.join(output_dir, city_name)\n",
    "        if os.path.exists(output_city_path):\n",
    "            shutil.rmtree(output_city_path)\n",
    "        shutil.copytree(city_path, output_city_path)\n",
    "        \n",
    "        # Collect all scene dates and paths\n",
    "        original_scenes = []\n",
    "        for scene_folder in os.listdir(city_path):\n",
    "            scene_path = os.path.join(city_path, scene_folder)\n",
    "            if not os.path.isdir(scene_path):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                date_str = scene_folder\n",
    "                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "                original_scenes.append((date_obj, scene_folder, scene_path))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Sort scenes by date\n",
    "        original_scenes.sort(key=lambda x: x[0])\n",
    "        \n",
    "        if len(original_scenes) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Generate monthly grid starting from the first original scene's month (not January)\n",
    "        start_date = original_scenes[0][0]\n",
    "        end_date = original_scenes[-1][0]\n",
    "        \n",
    "        # Start from the month of the first original scene, not January\n",
    "        start_year = start_date.year\n",
    "        start_month = start_date.month\n",
    "        end_year = end_date.year\n",
    "        end_month = end_date.month\n",
    "        \n",
    "        # Create a monthly grid from the first scene's month to the last scene's month\n",
    "        first_original_date = original_scenes[0][0]\n",
    "        current_date = first_original_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "        monthly_grid = []\n",
    "        \n",
    "        # Only create grid from first original scene month to last original scene month\n",
    "        while (current_date.year < end_year) or (current_date.year == end_year and current_date.month <= end_month):\n",
    "            monthly_grid.append(current_date)\n",
    "            current_date = current_date + relativedelta(months=1)\n",
    "        \n",
    "        # Track which months have original data (not interpolated)\n",
    "        original_months = set()\n",
    "        for date_obj, _, _ in original_scenes:\n",
    "            month_key = date_obj.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "            original_months.add(month_key)\n",
    "        \n",
    "        # Track which months we've already interpolated to avoid duplicates\n",
    "        interpolated_months = set()\n",
    "        \n",
    "        # Count total interpolation tasks for progress bar\n",
    "        total_tasks = 0\n",
    "        for start_idx in range(len(monthly_grid) - monthSpan + 1):\n",
    "            span_months = monthly_grid[start_idx:start_idx + monthSpan]\n",
    "            original_count = sum(1 for month in span_months if month in original_months)\n",
    "            missing_count = monthSpan - original_count\n",
    "            missing_ratio = missing_count / monthSpan\n",
    "            missing_months_in_span = [month for month in span_months \n",
    "                                    if month not in original_months and month not in interpolated_months]\n",
    "            \n",
    "            if missing_ratio <= tolerance_for_missing and original_count > 0 and len(missing_months_in_span) > 0:\n",
    "                total_tasks += len(missing_months_in_span) * len(file_types)\n",
    "        \n",
    "        # Create a mapping from month to original scene for quick lookup\n",
    "        month_to_scene = {}\n",
    "        for date_obj, scene_folder, original_scene_path in original_scenes:\n",
    "            scene_month = date_obj.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "            # Use the copied scene path in Cities_Processed instead of the original path\n",
    "            copied_scene_path = os.path.join(output_city_path, scene_folder)\n",
    "            month_to_scene[scene_month] = (date_obj, scene_folder, copied_scene_path)\n",
    "        \n",
    "        # Iterate through all possible monthSpan windows\n",
    "        with tqdm(total=total_tasks, desc=f'Interpolating {city_name}', leave=False) as pbar:\n",
    "            for start_idx in range(len(monthly_grid) - monthSpan + 1):\n",
    "                span_months = monthly_grid[start_idx:start_idx + monthSpan]\n",
    "                \n",
    "                # Count how many months in this span have original data\n",
    "                original_count = sum(1 for month in span_months if month in original_months)\n",
    "                missing_count = monthSpan - original_count\n",
    "                missing_ratio = missing_count / monthSpan\n",
    "                \n",
    "                # Check if there are any missing months in this span that we haven't interpolated yet\n",
    "                missing_months_in_span = [month for month in span_months \n",
    "                                        if month not in original_months and month not in interpolated_months]\n",
    "                \n",
    "                # If missing ratio is within tolerance and we have missing months to fill\n",
    "                if missing_ratio <= tolerance_for_missing and original_count > 0 and len(missing_months_in_span) > 0:\n",
    "                    \n",
    "                    # For each missing month, find the nearest original scenes before and after\n",
    "                    for missing_month in missing_months_in_span:\n",
    "                        \n",
    "                        # Find the nearest original scenes before and after this missing month\n",
    "                        before_scene = None\n",
    "                        after_scene = None\n",
    "                        \n",
    "                        # Look for the closest original scene before this missing month\n",
    "                        for month in reversed(monthly_grid):\n",
    "                            if month < missing_month and month in original_months:\n",
    "                                before_scene = month_to_scene[month]\n",
    "                                break\n",
    "                        \n",
    "                        # Look for the closest original scene after this missing month\n",
    "                        for month in monthly_grid:\n",
    "                            if month > missing_month and month in original_months:\n",
    "                                after_scene = month_to_scene[month]\n",
    "                                break\n",
    "                        \n",
    "                        # Skip if we don't have both before and after scenes\n",
    "                        if before_scene is None or after_scene is None:\n",
    "                            pbar.update(len(file_types))\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate temporal weights for linear interpolation\n",
    "                        before_date = before_scene[0]\n",
    "                        after_date = after_scene[0]\n",
    "                        missing_date = missing_month.replace(day=15, hour=12, minute=0, second=0)\n",
    "                        \n",
    "                        # Calculate interpolation weights based on temporal distance\n",
    "                        total_duration = (after_date - before_date).total_seconds()\n",
    "                        if total_duration == 0:\n",
    "                            pbar.update(len(file_types))\n",
    "                            continue\n",
    "                        \n",
    "                        missing_duration = (missing_date - before_date).total_seconds()\n",
    "                        weight_after = missing_duration / total_duration\n",
    "                        weight_before = 1.0 - weight_after\n",
    "                        \n",
    "                        # For each file type, perform linear interpolation\n",
    "                        for file_type in file_types:\n",
    "                            try:\n",
    "                                # Load the before and after scenes\n",
    "                                before_file = os.path.join(before_scene[2], file_type)\n",
    "                                after_file = os.path.join(after_scene[2], file_type)\n",
    "                                \n",
    "                                if not (os.path.exists(before_file) and os.path.exists(after_file)):\n",
    "                                    pbar.update(1)\n",
    "                                    continue\n",
    "                                \n",
    "                                before_data = rxr.open_rasterio(before_file, chunks=True)\n",
    "                                after_data = rxr.open_rasterio(after_file, chunks=True)\n",
    "                                \n",
    "                                # Mask nodata values (0 is nodata)\n",
    "                                before_masked = before_data.where(before_data != 0)\n",
    "                                after_masked = after_data.where(after_data != 0)\n",
    "                                \n",
    "                                # Perform linear interpolation per pixel\n",
    "                                # Only interpolate where both before and after have valid data\n",
    "                                valid_mask = (~before_masked.isnull()) & (~after_masked.isnull())\n",
    "                                \n",
    "                                # Linear interpolation: before * weight_before + after * weight_after\n",
    "                                interpolated = (before_masked * weight_before + after_masked * weight_after)\n",
    "                                \n",
    "                                # Where either before or after is nodata, set result to nodata\n",
    "                                interpolated = interpolated.where(valid_mask, 0)\n",
    "                                \n",
    "                                # Ensure data type consistency and set nodata\n",
    "                                interpolated = interpolated.astype('int16')\n",
    "                                interpolated.rio.write_nodata(0, inplace=True)\n",
    "                                \n",
    "                                # Create output scene directory\n",
    "                                scene_folder_name = missing_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "                                output_scene_path = os.path.join(output_city_path, scene_folder_name)\n",
    "                                os.makedirs(output_scene_path, exist_ok=True)\n",
    "                                \n",
    "                                # Save interpolated file\n",
    "                                output_file_path = os.path.join(output_scene_path, file_type)\n",
    "                                interpolated.rio.to_raster(output_file_path, dtype='int16', nodata=0)\n",
    "                                \n",
    "                                # Clean up memory\n",
    "                                before_data.close()\n",
    "                                after_data.close()\n",
    "                                del before_data, after_data, before_masked, after_masked, interpolated\n",
    "                                \n",
    "                                pbar.update(1)\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                pbar.update(1)\n",
    "                                continue\n",
    "                    \n",
    "                    # Mark these months as interpolated\n",
    "                    interpolated_months.update(missing_months_in_span)\n",
    "        \n",
    "        # Save list of interpolated months to a text file\n",
    "        if interpolated_months:\n",
    "            interpolated_file_path = os.path.join(\"./Data/ML/\", \"interpolated.txt\")\n",
    "            with open(interpolated_file_path, 'a') as f:\n",
    "                # Sort interpolated months chronologically\n",
    "                sorted_interpolated = sorted(interpolated_months)\n",
    "                for month in sorted_interpolated:\n",
    "                    # Format as timestamp similar to scene folder names\n",
    "                    timestamp = month.replace(day=15, hour=12, minute=0, second=0).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "                    f.write(f\"{city_name}/{timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolateLinearScenePixels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def validate_linear_interpolation(data_dir=\"./Data/Dataset/Cities_Processed\", \n",
    "                                interpolated_file_path=\"./Data/Dataset/interpolated.txt\",\n",
    "                                sample_pixels=100, \n",
    "                                tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Validate that interpolated scenes are indeed linear interpolations between their temporal neighbors.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Directory containing processed cities with interpolated scenes\n",
    "    - interpolated_file_path: Path to the global interpolated.txt file\n",
    "    - sample_pixels: Number of random pixels to test per scene (to avoid memory issues)\n",
    "    - tolerance: Numerical tolerance for floating point comparisons\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with validation results for each city\n",
    "    \"\"\"\n",
    "    \n",
    "    file_types = ['albedo.tif', 'blue.tif', 'green.tif', 'LST.tif', \n",
    "                  'ndbi.tif', 'ndvi.tif', 'ndwi.tif', 'red.tif']\n",
    "    \n",
    "    # Read the global interpolated.txt file\n",
    "    if not os.path.exists(interpolated_file_path):\n",
    "        print(f\"Global interpolated.txt not found at {interpolated_file_path}\")\n",
    "        return {}\n",
    "    \n",
    "    with open(interpolated_file_path, 'r') as f:\n",
    "        interpolated_entries = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Group interpolated entries by city\n",
    "    city_interpolated = {}\n",
    "    for entry in interpolated_entries:\n",
    "        if '/' in entry:\n",
    "            city_name, timestamp = entry.split('/', 1)\n",
    "            if city_name not in city_interpolated:\n",
    "                city_interpolated[city_name] = []\n",
    "            city_interpolated[city_name].append(timestamp)\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # Process each city that has interpolated scenes\n",
    "    for city_name, interpolated_timestamps in tqdm(city_interpolated.items(), desc='Validating cities'):\n",
    "        city_path = os.path.join(data_dir, city_name)\n",
    "        \n",
    "        if not os.path.isdir(city_path):\n",
    "            print(f\"City directory not found: {city_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all scenes in the city\n",
    "        all_scenes = []\n",
    "        for scene_folder in os.listdir(city_path):\n",
    "            scene_path = os.path.join(city_path, scene_folder)\n",
    "            if os.path.isdir(scene_path) and scene_folder != \"interpolated.txt\":\n",
    "                try:\n",
    "                    date_obj = datetime.fromisoformat(scene_folder.replace('Z', '+00:00'))\n",
    "                    all_scenes.append((date_obj, scene_folder, scene_path))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        # Sort scenes by date\n",
    "        all_scenes.sort(key=lambda x: x[0])\n",
    "        \n",
    "        city_results = {\n",
    "            'total_interpolated': len(interpolated_timestamps),\n",
    "            'validated_scenes': 0,\n",
    "            'failed_scenes': 0,\n",
    "            'validation_details': []\n",
    "        }\n",
    "        \n",
    "        # Validate each interpolated scene\n",
    "        for interp_timestamp in tqdm(interpolated_timestamps, desc=f'Validating {city_name}', leave=False):\n",
    "            interp_date = datetime.fromisoformat(interp_timestamp.replace('Z', '+00:00'))\n",
    "            \n",
    "            # Find the interpolated scene\n",
    "            interp_scene_path = None\n",
    "            for date_obj, scene_folder, scene_path in all_scenes:\n",
    "                if scene_folder == interp_timestamp:\n",
    "                    interp_scene_path = scene_path\n",
    "                    break\n",
    "            \n",
    "            if interp_scene_path is None:\n",
    "                print(f\"Could not find interpolated scene {interp_timestamp} for {city_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Find the nearest original scenes before and after\n",
    "            before_scene = None\n",
    "            after_scene = None\n",
    "            \n",
    "            # Find closest original scene before\n",
    "            for date_obj, scene_folder, scene_path in reversed(all_scenes):\n",
    "                if (date_obj < interp_date and \n",
    "                    scene_folder not in interpolated_timestamps):\n",
    "                    before_scene = (date_obj, scene_folder, scene_path)\n",
    "                    break\n",
    "            \n",
    "            # Find closest original scene after\n",
    "            for date_obj, scene_folder, scene_path in all_scenes:\n",
    "                if (date_obj > interp_date and \n",
    "                    scene_folder not in interpolated_timestamps):\n",
    "                    after_scene = (date_obj, scene_folder, scene_path)\n",
    "                    break\n",
    "            \n",
    "            if before_scene is None or after_scene is None:\n",
    "                print(f\"Could not find before/after scenes for {interp_timestamp} in {city_name}\")\n",
    "                city_results['failed_scenes'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Calculate expected interpolation weights\n",
    "            before_date = before_scene[0]\n",
    "            after_date = after_scene[0]\n",
    "            total_duration = (after_date - before_date).total_seconds()\n",
    "            missing_duration = (interp_date - before_date).total_seconds()\n",
    "            weight_after = missing_duration / total_duration\n",
    "            weight_before = 1.0 - weight_after\n",
    "            \n",
    "            scene_validation = {\n",
    "                'timestamp': interp_timestamp,\n",
    "                'before_scene': before_scene[1],\n",
    "                'after_scene': after_scene[1],\n",
    "                'weight_before': weight_before,\n",
    "                'weight_after': weight_after,\n",
    "                'file_results': {}\n",
    "            }\n",
    "            \n",
    "            scene_valid = True\n",
    "            \n",
    "            # Test each file type\n",
    "            for file_type in file_types:\n",
    "                before_file = os.path.join(before_scene[2], file_type)\n",
    "                after_file = os.path.join(after_scene[2], file_type)\n",
    "                interp_file = os.path.join(interp_scene_path, file_type)\n",
    "                \n",
    "                if not all(os.path.exists(f) for f in [before_file, after_file, interp_file]):\n",
    "                    scene_validation['file_results'][file_type] = {'status': 'missing_files'}\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Load the data\n",
    "                    before_data = rxr.open_rasterio(before_file, chunks=True)\n",
    "                    after_data = rxr.open_rasterio(after_file, chunks=True)\n",
    "                    interp_data = rxr.open_rasterio(interp_file, chunks=True)\n",
    "                    \n",
    "                    # Get data shape\n",
    "                    height, width = before_data.shape[1], before_data.shape[2]\n",
    "                    \n",
    "                    # Sample random pixels to test\n",
    "                    np.random.seed(42)  # For reproducibility\n",
    "                    test_pixels = min(sample_pixels, height * width)\n",
    "                    \n",
    "                    if height * width > 0:\n",
    "                        # Generate random pixel coordinates\n",
    "                        pixel_indices = np.random.choice(height * width, test_pixels, replace=False)\n",
    "                        row_indices = pixel_indices // width\n",
    "                        col_indices = pixel_indices % width\n",
    "                        \n",
    "                        # Extract pixel values\n",
    "                        before_values = before_data.values[0, row_indices, col_indices]\n",
    "                        after_values = after_data.values[0, row_indices, col_indices]\n",
    "                        interp_values = interp_data.values[0, row_indices, col_indices]\n",
    "                        \n",
    "                        # Test linear interpolation\n",
    "                        valid_mask = (before_values != 0) & (after_values != 0) & (interp_values != 0)\n",
    "                        \n",
    "                        if np.sum(valid_mask) > 0:\n",
    "                            expected_values = (before_values[valid_mask] * weight_before + \n",
    "                                             after_values[valid_mask] * weight_after)\n",
    "                            actual_values = interp_values[valid_mask]\n",
    "                            \n",
    "                            # Check if values match within tolerance\n",
    "                            differences = np.abs(expected_values - actual_values)\n",
    "                            max_diff = np.max(differences)\n",
    "                            mean_diff = np.mean(differences)\n",
    "                            \n",
    "                            is_valid = max_diff <= tolerance\n",
    "                            \n",
    "                            scene_validation['file_results'][file_type] = {\n",
    "                                'status': 'valid' if is_valid else 'invalid',\n",
    "                                'tested_pixels': np.sum(valid_mask),\n",
    "                                'max_difference': float(max_diff),\n",
    "                                'mean_difference': float(mean_diff),\n",
    "                                'tolerance': tolerance\n",
    "                            }\n",
    "                            \n",
    "                            if not is_valid:\n",
    "                                scene_valid = False\n",
    "                                print(f\"Validation failed for {city_name}/{interp_timestamp}/{file_type}\")\n",
    "                                print(f\"  Max difference: {max_diff}, Mean difference: {mean_diff}\")\n",
    "                        else:\n",
    "                            scene_validation['file_results'][file_type] = {\n",
    "                                'status': 'no_valid_pixels'\n",
    "                            }\n",
    "                    \n",
    "                    # Clean up\n",
    "                    before_data.close()\n",
    "                    after_data.close()\n",
    "                    interp_data.close()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    scene_validation['file_results'][file_type] = {\n",
    "                        'status': 'error',\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "                    scene_valid = False\n",
    "            \n",
    "            if scene_valid:\n",
    "                city_results['validated_scenes'] += 1\n",
    "            else:\n",
    "                city_results['failed_scenes'] += 1\n",
    "            \n",
    "            city_results['validation_details'].append(scene_validation)\n",
    "        \n",
    "        validation_results[city_name] = city_results\n",
    "        \n",
    "        # Print summary for this city\n",
    "        total = city_results['validated_scenes'] + city_results['failed_scenes']\n",
    "        if total > 0:\n",
    "            success_rate = city_results['validated_scenes'] / total * 100\n",
    "            print(f\"{city_name}: {city_results['validated_scenes']}/{total} scenes validated successfully ({success_rate:.1f}%)\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def print_validation_summary(validation_results):\n",
    "    \"\"\"Print a summary of validation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_cities = len(validation_results)\n",
    "    total_interpolated = sum(r['total_interpolated'] for r in validation_results.values())\n",
    "    total_validated = sum(r['validated_scenes'] for r in validation_results.values())\n",
    "    total_failed = sum(r['failed_scenes'] for r in validation_results.values())\n",
    "    \n",
    "    print(f\"Cities processed: {total_cities}\")\n",
    "    print(f\"Total interpolated scenes: {total_interpolated}\")\n",
    "    print(f\"Successfully validated: {total_validated}\")\n",
    "    print(f\"Failed validation: {total_failed}\")\n",
    "    \n",
    "    if total_interpolated > 0:\n",
    "        success_rate = total_validated / (total_validated + total_failed) * 100\n",
    "        print(f\"Overall success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"\\nPer-city results:\")\n",
    "    for city, results in validation_results.items():\n",
    "        total = results['validated_scenes'] + results['failed_scenes']\n",
    "        if total > 0:\n",
    "            rate = results['validated_scenes'] / total * 100\n",
    "            print(f\"  {city}: {results['validated_scenes']}/{total} ({rate:.1f}%)\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run validation\n",
    "    results = validate_linear_interpolation(\n",
    "        data_dir=\"./Data/ML/Cities_Processed\",\n",
    "        interpolated_file_path=\"./Data/ML/interpolated.txt\",\n",
    "        sample_pixels=100,\n",
    "        tolerance=1.0  # Allow 1 unit difference due to integer rounding\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print_validation_summary(results)\n",
    "    \n",
    "    # Optionally save detailed results\n",
    "    import json\n",
    "    with open('validation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8bb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def check_tif_divisibility(tif_path, divisor=128):\n",
    "    \"\"\"\n",
    "    Check if a TIF file's dimensions are divisible by a given value.\n",
    "    \n",
    "    Args:\n",
    "        tif_path (str): Path to the TIF file\n",
    "        divisor (int): Value to check divisibility against\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the check\n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "    \n",
    "    height_remainder = height % divisor\n",
    "    width_remainder = width % divisor\n",
    "    \n",
    "    return {\n",
    "        'is_divisible': (height_remainder == 0 and width_remainder == 0),\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'height_remainder': height_remainder,\n",
    "        'width_remainder': width_remainder\n",
    "    }\n",
    "\n",
    "result = check_tif_divisibility(\"/root/projects/STAC/Data/Dataset/DEM_2014_Preprocessed/Abilene_TX/DEM.tif\")\n",
    "if result['is_divisible']:\n",
    "    print(f\"TIF is divisible by 128: {result['height']}x{result['width']}\")\n",
    "else:\n",
    "    print(f\"TIF not divisible by 128. Dimensions: {result['height']}x{result['width']}\")\n",
    "    print(f\"Remainders: {result['height_remainder']}x{result['width_remainder']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def list_files_in_folder(folder_path):\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if\n",
    "             os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return files\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths\n",
    "file_list = []\n",
    "allAlbedoPixelFiles = []\n",
    "allDEMPixelFiles = []\n",
    "for filePath in get_file_paths('./Data/Dataset'):\n",
    "    if 'albedo' in filePath:\n",
    "        allAlbedoPixelFiles.append(filePath)\n",
    "    if 'DEM' in filePath:\n",
    "        allDEMPixelFiles.append(filePath)\n",
    "for xPath in tqdm(allAlbedoPixelFiles, desc=\"Packing to dictionary...\"):\n",
    "    sceneFiles = list_files_in_folder(os.path.dirname(os.path.abspath(xPath)))\n",
    "    rasterDict = {}\n",
    "    for rasterPath in sceneFiles:\n",
    "        rasterName = rasterPath.split('/')[-1]\n",
    "        rasterDict[rasterName] = rasterPath\n",
    "    file_list.append(rasterDict)\n",
    "for xPath in tqdm(allDEMPixelFiles, desc=\"Packing to dictionary...\"):\n",
    "    sceneFiles = list_files_in_folder(os.path.dirname(os.path.abspath(xPath)))\n",
    "    rasterDict = {}\n",
    "    for rasterPath in sceneFiles:\n",
    "        rasterName = rasterPath.split('/')[-1]\n",
    "        rasterDict[rasterName] = rasterPath\n",
    "    file_list.append(rasterDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d18f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import json\n",
    "\n",
    "def get_tif_ranges(data_list):\n",
    "    ranges = {\n",
    "        'red': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'ndwi': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'ndvi': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'ndbi': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'LST': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'green': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'blue': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'DEM': {'min': float('inf'), 'max': float('-inf')},\n",
    "        'albedo': {'min': float('inf'), 'max': float('-inf')}\n",
    "    }\n",
    "\n",
    "    for item in tqdm(data_list, desc=\"Getting Ranges\"):\n",
    "        for tif_type, path in item.items():\n",
    "            with rasterio.open(path) as src:\n",
    "                data = src.read(1)\n",
    "                tif_name = tif_type.split('.')[0]\n",
    "                minimum = min(ranges[tif_name]['min'], float(data[data != 0].min()))\n",
    "                maximum = max(ranges[tif_name]['max'], float(data.max()))\n",
    "                ranges[tif_name]['min'] = minimum\n",
    "                ranges[tif_name]['max'] = maximum\n",
    "    return ranges\n",
    "\n",
    "ranges = get_tif_ranges(file_list)\n",
    "print(json.dumps(ranges, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a06175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.transform import from_bounds\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "def convert_to_tiles(data_dir: str, tile_size: int = 128, overlap: int = 0):\n",
    "    \"\"\"\n",
    "    Divide preprocessed rasters into 128x128 tiles while maintaining coordinate systems.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to Dataset directory containing Cities_Preprocessed and DEM_2014_Preprocessed\n",
    "        tile_size: Size of each tile (default 128x128)\n",
    "        overlap: Overlap between tiles in pixels (default 0)\n",
    "    \n",
    "    Output structure:\n",
    "        Dataset/\n",
    "         Cities_Tiles/\n",
    "            <city>/\n",
    "               <timestamp>/\n",
    "                  LST_row_0_col_0.tif\n",
    "                  red_row_0_col_1.tif\n",
    "                  ...\n",
    "         DEM_2014_Tiles/\n",
    "             <city>/\n",
    "                DEM_row_0_col_0.tif\n",
    "                ...\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Calculate stride (step size between tiles)\n",
    "    stride = tile_size - overlap\n",
    "    \n",
    "    print(f\"Converting to {tile_size}x{tile_size} tiles with {overlap}px overlap...\")\n",
    "    \n",
    "    # Process Cities data\n",
    "    cities_preprocessed = data_path / \"Cities_Processed\"\n",
    "    cities_tiles = data_path / \"Cities_Tiles\"\n",
    "    \n",
    "    if cities_preprocessed.exists():\n",
    "        print(\"Processing Cities data...\")\n",
    "        _process_cities_tiles(cities_preprocessed, cities_tiles, tile_size, stride)\n",
    "    \n",
    "    # Process DEM data\n",
    "    dem_preprocessed = data_path / \"DEM_2014_Preprocessed\"\n",
    "    dem_tiles = data_path / \"DEM_2014_Tiles\"\n",
    "    \n",
    "    if dem_preprocessed.exists():\n",
    "        print(\"Processing DEM data...\")\n",
    "        _process_dem_tiles(dem_preprocessed, dem_tiles, tile_size, stride)\n",
    "    \n",
    "    print(\"Tiling complete!\")\n",
    "\n",
    "def _process_cities_tiles(cities_preprocessed: Path, cities_tiles: Path, tile_size: int, stride: int):\n",
    "    \"\"\"Process Cities_Preprocessed into tiles\"\"\"\n",
    "    \n",
    "    for city_dir in tqdm(list(cities_preprocessed.iterdir()), desc=\"Processing cities\"):\n",
    "        if not city_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        city_name = city_dir.name\n",
    "        output_city_dir = cities_tiles / city_name\n",
    "        \n",
    "        # Process each timestamp\n",
    "        for timestamp_dir in city_dir.iterdir():\n",
    "            if not timestamp_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            timestamp = timestamp_dir.name\n",
    "            output_timestamp_dir = output_city_dir / timestamp\n",
    "            output_timestamp_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Get all .tif files in this timestamp\n",
    "            tif_files = list(timestamp_dir.glob(\"*.tif\"))\n",
    "            \n",
    "            if not tif_files:\n",
    "                continue\n",
    "            \n",
    "            # Use first file to determine grid dimensions and spatial reference\n",
    "            reference_file = tif_files[0]\n",
    "            \n",
    "            with rasterio.open(reference_file) as src:\n",
    "                height, width = src.height, src.width\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "                \n",
    "                # Calculate number of tiles\n",
    "                n_rows = math.ceil((height - tile_size) / stride) + 1 if height > tile_size else 1\n",
    "                n_cols = math.ceil((width - tile_size) / stride) + 1 if width > tile_size else 1\n",
    "                \n",
    "                # Ensure we don't go beyond image boundaries\n",
    "                n_rows = min(n_rows, math.ceil(height / stride))\n",
    "                n_cols = min(n_cols, math.ceil(width / stride))\n",
    "            \n",
    "            # Process each .tif file in this timestamp\n",
    "            for tif_file in tif_files:\n",
    "                band_name = tif_file.stem  # e.g., 'LST', 'red', 'ndvi', etc.\n",
    "                \n",
    "                with rasterio.open(tif_file) as src:\n",
    "                    # Create tiles for this band\n",
    "                    for row in range(n_rows):\n",
    "                        for col in range(n_cols):\n",
    "                            # Calculate tile boundaries\n",
    "                            start_row = row * stride\n",
    "                            start_col = col * stride\n",
    "                            \n",
    "                            # Ensure we don't exceed image boundaries\n",
    "                            end_row = min(start_row + tile_size, height)\n",
    "                            end_col = min(start_col + tile_size, width)\n",
    "                            \n",
    "                            # Adjust start positions if tile would be too small\n",
    "                            if end_row - start_row < tile_size:\n",
    "                                start_row = max(0, end_row - tile_size)\n",
    "                            if end_col - start_col < tile_size:\n",
    "                                start_col = max(0, end_col - tile_size)\n",
    "                            \n",
    "                            # Final tile dimensions\n",
    "                            tile_height = end_row - start_row\n",
    "                            tile_width = end_col - start_col\n",
    "                            \n",
    "                            # Skip if tile is too small\n",
    "                            if tile_height < tile_size // 2 or tile_width < tile_size // 2:\n",
    "                                continue\n",
    "                            \n",
    "                            # Create window for reading\n",
    "                            window = Window(start_col, start_row, tile_width, tile_height)\n",
    "                            \n",
    "                            # Read tile data\n",
    "                            tile_data = src.read(window=window)\n",
    "                            \n",
    "                            # Calculate new transform for this tile\n",
    "                            tile_transform = rasterio.windows.transform(window, src.transform)\n",
    "                            \n",
    "                            # Pad tile to exact tile_size if necessary\n",
    "                            if tile_height != tile_size or tile_width != tile_size:\n",
    "                                padded_data = np.full(\n",
    "                                    (src.count, tile_size, tile_size), \n",
    "                                    src.nodata if src.nodata is not None else 0, \n",
    "                                    dtype=tile_data.dtype\n",
    "                                )\n",
    "                                padded_data[:, :tile_height, :tile_width] = tile_data\n",
    "                                tile_data = padded_data\n",
    "                                \n",
    "                                # Adjust transform for padding (keeping upper-left corner the same)\n",
    "                                # No transform adjustment needed since we're padding bottom/right\n",
    "                            \n",
    "                            # Create output filename\n",
    "                            tile_filename = f\"{band_name}_row_{row:03d}_col_{col:03d}.tif\"\n",
    "                            tile_path = output_timestamp_dir / tile_filename\n",
    "                            \n",
    "                            # Skip if tile already exists\n",
    "                            if tile_path.exists():\n",
    "                                continue\n",
    "                            \n",
    "                            # Update profile for tile\n",
    "                            tile_profile = src.profile.copy()\n",
    "                            tile_profile.update({\n",
    "                                'height': tile_size,\n",
    "                                'width': tile_size,\n",
    "                                'transform': tile_transform,\n",
    "                                'crs': crs\n",
    "                            })\n",
    "                            \n",
    "                            # Write tile\n",
    "                            with rasterio.open(tile_path, 'w', **tile_profile) as dst:\n",
    "                                dst.write(tile_data)\n",
    "\n",
    "def _process_dem_tiles(dem_preprocessed: Path, dem_tiles: Path, tile_size: int, stride: int):\n",
    "    \"\"\"Process DEM_2014_Preprocessed into tiles\"\"\"\n",
    "    \n",
    "    for city_dir in tqdm(list(dem_preprocessed.iterdir()), desc=\"Processing DEM\"):\n",
    "        if not city_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        city_name = city_dir.name\n",
    "        output_city_dir = dem_tiles / city_name\n",
    "        output_city_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get DEM file\n",
    "        dem_file = city_dir / \"DEM.tif\"\n",
    "        if not dem_file.exists():\n",
    "            continue\n",
    "        \n",
    "        with rasterio.open(dem_file) as src:\n",
    "            height, width = src.height, src.width\n",
    "            transform = src.transform\n",
    "            crs = src.crs\n",
    "            \n",
    "            # Calculate number of tiles\n",
    "            n_rows = math.ceil((height - tile_size) / stride) + 1 if height > tile_size else 1\n",
    "            n_cols = math.ceil((width - tile_size) / stride) + 1 if width > tile_size else 1\n",
    "            \n",
    "            # Ensure we don't go beyond image boundaries\n",
    "            n_rows = min(n_rows, math.ceil(height / stride))\n",
    "            n_cols = min(n_cols, math.ceil(width / stride))\n",
    "            \n",
    "            # Create tiles\n",
    "            for row in range(n_rows):\n",
    "                for col in range(n_cols):\n",
    "                    # Calculate tile boundaries\n",
    "                    start_row = row * stride\n",
    "                    start_col = col * stride\n",
    "                    \n",
    "                    # Ensure we don't exceed image boundaries\n",
    "                    end_row = min(start_row + tile_size, height)\n",
    "                    end_col = min(start_col + tile_size, width)\n",
    "                    \n",
    "                    # Adjust start positions if tile would be too small\n",
    "                    if end_row - start_row < tile_size:\n",
    "                        start_row = max(0, end_row - tile_size)\n",
    "                    if end_col - start_col < tile_size:\n",
    "                        start_col = max(0, end_col - tile_size)\n",
    "                    \n",
    "                    # Final tile dimensions\n",
    "                    tile_height = end_row - start_row\n",
    "                    tile_width = end_col - start_col\n",
    "                    \n",
    "                    # Skip if tile is too small\n",
    "                    if tile_height < tile_size // 2 or tile_width < tile_size // 2:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create window for reading\n",
    "                    window = Window(start_col, start_row, tile_width, tile_height)\n",
    "                    \n",
    "                    # Read tile data\n",
    "                    tile_data = src.read(window=window)\n",
    "                    \n",
    "                    # Calculate new transform for this tile\n",
    "                    tile_transform = rasterio.windows.transform(window, src.transform)\n",
    "                    \n",
    "                    # Pad tile to exact tile_size if necessary\n",
    "                    if tile_height != tile_size or tile_width != tile_size:\n",
    "                        padded_data = np.full(\n",
    "                            (src.count, tile_size, tile_size), \n",
    "                            src.nodata if src.nodata is not None else 0, \n",
    "                            dtype=tile_data.dtype\n",
    "                        )\n",
    "                        padded_data[:, :tile_height, :tile_width] = tile_data\n",
    "                        tile_data = padded_data\n",
    "                    \n",
    "                    # Create output filename\n",
    "                    tile_filename = f\"DEM_row_{row:03d}_col_{col:03d}.tif\"\n",
    "                    tile_path = output_city_dir / tile_filename\n",
    "                    \n",
    "                    # Skip if tile already exists\n",
    "                    if tile_path.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    # Update profile for tile\n",
    "                    tile_profile = src.profile.copy()\n",
    "                    tile_profile.update({\n",
    "                        'height': tile_size,\n",
    "                        'width': tile_size,\n",
    "                        'transform': tile_transform,\n",
    "                        'crs': crs\n",
    "                    })\n",
    "                    \n",
    "                    # Write tile\n",
    "                    with rasterio.open(tile_path, 'w', **tile_profile) as dst:\n",
    "                        dst.write(tile_data)\n",
    "\n",
    "def get_tile_info(data_dir: str):\n",
    "    \"\"\"\n",
    "    Get information about the tiled dataset\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    cities_tiles = data_path / \"Cities_Tiles\"\n",
    "    dem_tiles = data_path / \"DEM_2014_Tiles\"\n",
    "    \n",
    "    print(\"=== TILE DATASET INFORMATION ===\")\n",
    "    \n",
    "    if cities_tiles.exists():\n",
    "        total_scenes = 0\n",
    "        total_tiles = 0\n",
    "        cities = list(cities_tiles.iterdir())\n",
    "        \n",
    "        print(f\"Cities with tiles: {len(cities)}\")\n",
    "        \n",
    "        for city_dir in cities[:3]:  # Show info for first 3 cities\n",
    "            if not city_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            city_name = city_dir.name\n",
    "            timestamps = list(city_dir.iterdir())\n",
    "            city_scenes = len(timestamps)\n",
    "            total_scenes += city_scenes\n",
    "            \n",
    "            # Count tiles in first timestamp\n",
    "            if timestamps:\n",
    "                first_timestamp = timestamps[0]\n",
    "                tiles = list(first_timestamp.glob(\"*.tif\"))\n",
    "                tiles_per_scene = len(tiles)\n",
    "                city_total_tiles = city_scenes * tiles_per_scene\n",
    "                total_tiles += city_total_tiles\n",
    "                \n",
    "                print(f\"  {city_name}: {city_scenes} scenes, ~{tiles_per_scene} tiles/scene, ~{city_total_tiles} total tiles\")\n",
    "        \n",
    "        print(f\"Total scenes: {total_scenes}\")\n",
    "        print(f\"Estimated total tiles: {total_tiles}\")\n",
    "    \n",
    "    if dem_tiles.exists():\n",
    "        dem_cities = list(dem_tiles.iterdir())\n",
    "        total_dem_tiles = 0\n",
    "        \n",
    "        for city_dir in dem_cities:\n",
    "            if city_dir.is_dir():\n",
    "                dem_tiles_count = len(list(city_dir.glob(\"*.tif\")))\n",
    "                total_dem_tiles += dem_tiles_count\n",
    "        \n",
    "        print(f\"DEM cities: {len(dem_cities)}\")\n",
    "        print(f\"Total DEM tiles: {total_dem_tiles}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Convert to tiles\n",
    "    convert_to_tiles(\"./Data/ML\")\n",
    "    \n",
    "    # Show information about the tiled dataset\n",
    "    get_tile_info(\"./Data/ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_majority_class_from_classification(dem_tile_path, class_src):\n",
    "    try:\n",
    "        with rasterio.open(dem_tile_path) as dem_src:\n",
    "            dem_bounds = dem_src.bounds\n",
    "            dem_crs = dem_src.crs\n",
    "            \n",
    "            # Check if DEM and classification have same CRS\n",
    "            if dem_crs != class_src.crs:\n",
    "                from rasterio.warp import transform_bounds\n",
    "                dem_bounds_in_class_crs = transform_bounds(\n",
    "                    dem_crs, class_src.crs, *dem_bounds\n",
    "                )\n",
    "            else:\n",
    "                dem_bounds_in_class_crs = dem_bounds\n",
    "            \n",
    "            # Get the window of class_src that overlaps with dem tile\n",
    "            class_window = rasterio.windows.from_bounds(\n",
    "                *dem_bounds_in_class_crs, \n",
    "                class_src.transform\n",
    "            )\n",
    "            \n",
    "            # Read the classification data for the overlapping area\n",
    "            class_data = class_src.read(1, window=class_window)\n",
    "            \n",
    "            # Get the actual bounds of the read window\n",
    "            window_transform = rasterio.windows.transform(class_window, class_src.transform)    \n",
    "            \n",
    "            # Calculate intersection area for each pixel with DEM bounds\n",
    "            class_counts_weighted = {}            \n",
    "            for i in range(class_data.shape[0]):\n",
    "                for j in range(class_data.shape[1]):\n",
    "                    class_val = class_data[i, j]\n",
    "                    \n",
    "                    # Skip invalid classes\n",
    "                    if class_val < 0 or class_val > 17:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate pixel bounds\n",
    "                    class_pixel_left = window_transform[2] + j * window_transform[0] #0\n",
    "                    class_pixel_right = class_pixel_left + window_transform[0] # 100\n",
    "                    class_pixel_top = window_transform[5] + i * window_transform[4] # 100\n",
    "                    class_pixel_bottom = class_pixel_top + window_transform[4] # 0\n",
    "                    \n",
    "                    # Calculate intersection with DEM bounds\n",
    "                    intersect_left = max(class_pixel_left, dem_bounds_in_class_crs[0]) # 70\n",
    "                    intersect_right = min(class_pixel_right, dem_bounds_in_class_crs[2]) # 130 -> 100\n",
    "                    intersect_top = min(class_pixel_top, dem_bounds_in_class_crs[3]) # 30 -> 30\n",
    "                    intersect_bottom = max(class_pixel_bottom, dem_bounds_in_class_crs[1]) # 0\n",
    "                    \n",
    "                    # Calculate intersection area\n",
    "                    if intersect_right > intersect_left and intersect_top > intersect_bottom:\n",
    "                        intersection_area = (intersect_right - intersect_left) * (intersect_top - intersect_bottom)\n",
    "                        \n",
    "                        if class_val not in class_counts_weighted:\n",
    "                            class_counts_weighted[class_val] = 0\n",
    "                        class_counts_weighted[class_val] += intersection_area\n",
    "            \n",
    "            if not class_counts_weighted:\n",
    "                print(\"No valid classes found\")\n",
    "                return None\n",
    "            \n",
    "            # Get majority class by area\n",
    "            majority_class = max(class_counts_weighted.keys(), \n",
    "                               key=lambda x: class_counts_weighted[x])\n",
    "            return int(majority_class)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dem_tile_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def copy_tile_with_structure(src_path, base_src_dir, base_dst_dir):\n",
    "    \"\"\"\n",
    "    Copy a tile while preserving folder structure.\n",
    "    \n",
    "    Args:\n",
    "        src_path: Source file path\n",
    "        base_src_dir: Base source directory (e.g., './Dataset/Cities_Tiles')\n",
    "        base_dst_dir: Base destination directory (e.g., './Dataset/Clustered/1')\n",
    "    \n",
    "    Returns:\n",
    "        str: Destination path where file was copied\n",
    "    \"\"\"\n",
    "    # Get relative path from base source directory\n",
    "    rel_path = os.path.relpath(src_path, base_src_dir)\n",
    "    \n",
    "    # Create destination path\n",
    "    dst_path = os.path.join(base_dst_dir, rel_path)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "    \n",
    "    # Copy the file\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    \n",
    "    return dst_path\n",
    "\n",
    "def find_corresponding_dem_tile(cities_tile_path, cities_tiles_dir, dem_tiles_dir):\n",
    "    \"\"\"\n",
    "    Find the corresponding DEM tile for a given cities tile.\n",
    "    \n",
    "    Args:\n",
    "        cities_tile_path: Path to cities tile\n",
    "        cities_tiles_dir: Base cities tiles directory\n",
    "        dem_tiles_dir: Base DEM tiles directory\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to corresponding DEM tile or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the cities tile path to extract city and tile coordinates\n",
    "        rel_path = os.path.relpath(cities_tile_path, cities_tiles_dir)\n",
    "        path_parts = rel_path.split(os.sep)\n",
    "        \n",
    "        if len(path_parts) < 3:\n",
    "            return None\n",
    "            \n",
    "        city_name = path_parts[0]\n",
    "        timestamp = path_parts[1]\n",
    "        tile_filename = path_parts[2]\n",
    "        \n",
    "        # Extract row and col from filename (e.g., \"red_row_004_col_005.tif\" -> \"row_004_col_005\")\n",
    "        # Split by underscore and find row/col parts\n",
    "        filename_parts = tile_filename.split('_')\n",
    "        row_col_part = None\n",
    "        \n",
    "        for i, part in enumerate(filename_parts):\n",
    "            if part == 'row' and i + 2 < len(filename_parts) and filename_parts[i + 2] == 'col':\n",
    "                row_col_part = f\"row_{filename_parts[i + 1]}_col_{filename_parts[i + 3]}\"\n",
    "                break\n",
    "        \n",
    "        if not row_col_part:\n",
    "            return None\n",
    "        \n",
    "        # Construct DEM tile path\n",
    "        dem_tile_filename = f\"DEM_{row_col_part}.tif\"\n",
    "        dem_tile_path = os.path.join(dem_tiles_dir, city_name, dem_tile_filename)\n",
    "        \n",
    "        if os.path.exists(dem_tile_path):\n",
    "            return dem_tile_path\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding corresponding DEM tile for {cities_tile_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cluster_tiles_by_classification(dataset_dir, classification_raster_path):\n",
    "    \"\"\"\n",
    "    Main function to cluster tiles by classification.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to Dataset directory\n",
    "        classification_raster_path: Path to classification raster (100m, values 1-5)\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    cities_tiles_dir = dataset_path / \"Cities_Tiles\"\n",
    "    dem_tiles_dir = dataset_path / \"DEM_2014_Tiles\"\n",
    "    clustered_dir = dataset_path / \"Clustered\"\n",
    "    \n",
    "    # Create cluster directories\n",
    "    for class_num in range(1, 6):\n",
    "        cluster_dir = clustered_dir / str(class_num)\n",
    "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for cities and DEM tiles\n",
    "        (cluster_dir / \"Cities_Tiles\").mkdir(exist_ok=True)\n",
    "        (cluster_dir / \"DEM_2014_Tiles\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {i: 0 for i in range(1, 6)}\n",
    "    total_processed = 0\n",
    "    failed_processing = 0\n",
    "    \n",
    "    print(\"Starting tile clustering by classification...\")\n",
    "    print(f\"Classification raster: {classification_raster_path}\")\n",
    "    print(f\"Cities tiles directory: {cities_tiles_dir}\")\n",
    "    print(f\"DEM tiles directory: {dem_tiles_dir}\")\n",
    "    \n",
    "    # Process all cities tiles\n",
    "    cities_tiles_list = []\n",
    "    for city_dir in cities_tiles_dir.iterdir():\n",
    "        if city_dir.is_dir():\n",
    "            for timestamp_dir in city_dir.iterdir():\n",
    "                if timestamp_dir.is_dir():\n",
    "                    for tile_file in timestamp_dir.glob(\"*.tif\"):\n",
    "                        cities_tiles_list.append(tile_file)\n",
    "    \n",
    "    print(f\"Found {len(cities_tiles_list)} cities tiles to process\")\n",
    "    \n",
    "    # Process each cities tile\n",
    "    for cities_tile_path in tqdm(cities_tiles_list, desc=\"Processing tiles\"):\n",
    "        total_processed += 1\n",
    "        \n",
    "        # Get majority classification for this tile\n",
    "        majority_class = get_majority_class_from_classification(\n",
    "            str(cities_tile_path), \n",
    "            classification_raster_path\n",
    "        )\n",
    "        \n",
    "        if majority_class is None:\n",
    "            failed_processing += 1\n",
    "            continue\n",
    "        \n",
    "        # Update statistics\n",
    "        stats[majority_class] += 1\n",
    "        \n",
    "        # Copy cities tile to appropriate cluster directory\n",
    "        cluster_cities_dir = clustered_dir / str(majority_class) / \"Cities_Tiles\"\n",
    "        copy_tile_with_structure(\n",
    "            str(cities_tile_path),\n",
    "            str(cities_tiles_dir),\n",
    "            str(cluster_cities_dir)\n",
    "        )\n",
    "        \n",
    "        # Find and copy corresponding DEM tile\n",
    "        dem_tile_path = find_corresponding_dem_tile(\n",
    "            str(cities_tile_path),\n",
    "            str(cities_tiles_dir),\n",
    "            str(dem_tiles_dir)\n",
    "        )\n",
    "        \n",
    "        if dem_tile_path and os.path.exists(dem_tile_path):\n",
    "            cluster_dem_dir = clustered_dir / str(majority_class) / \"DEM_2014_Tiles\"\n",
    "            copy_tile_with_structure(\n",
    "                dem_tile_path,\n",
    "                str(dem_tiles_dir),\n",
    "                str(cluster_dem_dir)\n",
    "            )\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== CLUSTERING RESULTS ===\")\n",
    "    print(f\"Total tiles processed: {total_processed}\")\n",
    "    print(f\"Failed to process: {failed_processing}\")\n",
    "    print(f\"Successfully clustered: {sum(stats.values())}\")\n",
    "    print(\"\\nTiles per class:\")\n",
    "    for class_num in range(1, 6):\n",
    "        print(f\"  Class {class_num}: {stats[class_num]} tiles\")\n",
    "    \n",
    "    print(f\"\\nClustered tiles saved to: {clustered_dir}\")\n",
    "    \n",
    "    # Verify folder structure\n",
    "    print(\"\\n=== FOLDER STRUCTURE VERIFICATION ===\")\n",
    "    for class_num in range(1, 6):\n",
    "        class_dir = clustered_dir / str(class_num)\n",
    "        cities_count = len(list((class_dir / \"Cities_Tiles\").rglob(\"*.tif\")))\n",
    "        dem_count = len(list((class_dir / \"DEM_2014_Tiles\").rglob(\"*.tif\")))\n",
    "        print(f\"Class {class_num}: {cities_count} cities tiles, {dem_count} DEM tiles\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths here\n",
    "    dataset_directory = \"./Data/Dataset\"\n",
    "    classification_raster = \"./cluster5.tif\"  # Update this path\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not os.path.exists(dataset_directory):\n",
    "        print(f\"Error: Dataset directory not found: {dataset_directory}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not os.path.exists(classification_raster):\n",
    "        print(f\"Error: Classification raster not found: {classification_raster}\")\n",
    "        print(\"Please update the 'classification_raster' variable with the correct path\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Run clustering\n",
    "    cluster_tiles_by_classification(dataset_directory, classification_raster)\n",
    "    \n",
    "    print(\"\\nClustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c01733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "def get_majority_class_from_classification(dem_tile_path, class_src):\n",
    "    try:\n",
    "        with rasterio.open(dem_tile_path) as dem_src:\n",
    "            dem_bounds = dem_src.bounds\n",
    "            dem_crs = dem_src.crs\n",
    "            \n",
    "            # Check if DEM and classification have same CRS\n",
    "            if dem_crs != class_src.crs:\n",
    "                from rasterio.warp import transform_bounds\n",
    "                dem_bounds_in_class_crs = transform_bounds(\n",
    "                    dem_crs, class_src.crs, *dem_bounds\n",
    "                )\n",
    "            else:\n",
    "                dem_bounds_in_class_crs = dem_bounds\n",
    "            \n",
    "            # Get the window of class_src that overlaps with dem tile\n",
    "            class_window = rasterio.windows.from_bounds(\n",
    "                *dem_bounds_in_class_crs, \n",
    "                class_src.transform\n",
    "            )\n",
    "            \n",
    "            # Read the classification data for the overlapping area\n",
    "            class_data = class_src.read(1, window=class_window)\n",
    "            \n",
    "            # Get the actual bounds of the read window\n",
    "            window_transform = rasterio.windows.transform(class_window, class_src.transform)    \n",
    "            \n",
    "            # Calculate intersection area for each pixel with DEM bounds\n",
    "            class_counts_weighted = {}            \n",
    "            for i in range(class_data.shape[0]):\n",
    "                for j in range(class_data.shape[1]):\n",
    "                    class_val = class_data[i, j]\n",
    "                    \n",
    "                    # Skip invalid classes\n",
    "                    if class_val < 0 or class_val > 17:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate pixel bounds\n",
    "                    class_pixel_left = window_transform[2] + j * window_transform[0] #0\n",
    "                    class_pixel_right = class_pixel_left + window_transform[0] # 100\n",
    "                    class_pixel_top = window_transform[5] + i * window_transform[4] # 100\n",
    "                    class_pixel_bottom = class_pixel_top + window_transform[4] # 0\n",
    "                    \n",
    "                    # Calculate intersection with DEM bounds\n",
    "                    intersect_left = max(class_pixel_left, dem_bounds_in_class_crs[0]) # 70\n",
    "                    intersect_right = min(class_pixel_right, dem_bounds_in_class_crs[2]) # 130 -> 100\n",
    "                    intersect_top = min(class_pixel_top, dem_bounds_in_class_crs[3]) # 30 -> 30\n",
    "                    intersect_bottom = max(class_pixel_bottom, dem_bounds_in_class_crs[1]) # 0\n",
    "                    \n",
    "                    # Calculate intersection area\n",
    "                    if intersect_right > intersect_left and intersect_top > intersect_bottom:\n",
    "                        intersection_area = (intersect_right - intersect_left) * (intersect_top - intersect_bottom)\n",
    "                        \n",
    "                        if class_val not in class_counts_weighted:\n",
    "                            class_counts_weighted[class_val] = 0\n",
    "                        class_counts_weighted[class_val] += intersection_area\n",
    "            \n",
    "            if not class_counts_weighted:\n",
    "                print(\"No valid classes found\")\n",
    "                return None\n",
    "            \n",
    "            # Get majority class by area\n",
    "            majority_class = max(class_counts_weighted.keys(), \n",
    "                               key=lambda x: class_counts_weighted[x])\n",
    "            return int(majority_class)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dem_tile_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def count_tiles_by_classification(dataset_dir, classification_raster_path):\n",
    "    \"\"\"\n",
    "    Count tiles by classification without copying them.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to Dataset directory\n",
    "        classification_raster_path: Path to classification raster (100m, values 1-17)\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    dem_tiles_dir = dataset_path / \"DEM_2014_Tiles\"\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {i: 0 for i in range(0, 18)}\n",
    "    total_processed = 0\n",
    "    failed_processing = 0\n",
    "    \n",
    "    print(\"Starting tile counting by classification...\")\n",
    "    print(f\"Classification raster: {classification_raster_path}\")\n",
    "    print(f\"DEM tiles directory: {dem_tiles_dir}\")\n",
    "    \n",
    "    # Process all cities tiles (only red tiles)\n",
    "    dem_tiles_list = []\n",
    "    for city_dir in dem_tiles_dir.iterdir():\n",
    "        if city_dir.is_dir():            \n",
    "            if city_dir.is_dir():\n",
    "                for tile_file in city_dir.glob(\"DEM_*.tif\"):\n",
    "                    dem_tiles_list.append(tile_file)\n",
    "    \n",
    "    print(f\"Found {len(dem_tiles_list)} dem tiles to process\")\n",
    "    \n",
    "    # Open classification raster once and keep it open for all processing\n",
    "    with rasterio.open(classification_raster_path) as class_src:\n",
    "        print(\"Classification raster opened, starting processing...\")\n",
    "        \n",
    "        # Process each cities tile\n",
    "        for dem_tile_path in tqdm(dem_tiles_list, desc=\"Processing tiles\"):\n",
    "            total_processed += 1\n",
    "            \n",
    "            # Get majority classification for this tile\n",
    "            majority_class = get_majority_class_from_classification(\n",
    "                str(dem_tile_path), \n",
    "                class_src  # Pass the open rasterio dataset\n",
    "            )\n",
    "            \n",
    "            if majority_class is None:\n",
    "                failed_processing += 1\n",
    "                continue\n",
    "            \n",
    "            # Update statistics \n",
    "            stats[majority_class] += 1\n",
    "            \n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== COUNTING RESULTS ===\")\n",
    "    print(f\"Total tiles processed: {total_processed}\")\n",
    "    print(f\"Failed to process: {failed_processing}\")\n",
    "    print(f\"Successfully classified: {sum(stats.values())}\")\n",
    "    print(\"\\nTiles per class:\")\n",
    "    for class_num in range(0, 18):\n",
    "        if class_num == 7 or class_num == 9:\n",
    "            continue\n",
    "        percentage = (stats[class_num] / sum(stats.values()) * 100) if sum(stats.values()) > 0 else 0\n",
    "        print(f\"  Class {class_num:2d}: {stats[class_num]:6d} tiles ({percentage:5.1f}%)\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths here\n",
    "    dataset_directory = \"./Data/Dataset\"\n",
    "    classification_raster = \"./original_LCZ.tif\"\n",
    "\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not os.path.exists(dataset_directory):\n",
    "        print(f\"Error: Dataset directory not found: {dataset_directory}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not os.path.exists(classification_raster):\n",
    "        print(f\"Error: Classification raster not found: {classification_raster}\")\n",
    "        print(\"Please update the 'classification_raster' variable with the correct path\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Run counting\n",
    "    count_tiles_by_classification(dataset_directory, classification_raster)\n",
    "    \n",
    "    print(\"\\nCounting complete! *Multiply the tile counts by 12.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34707de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\"./original_LCZ.tif\") as src:\n",
    "    print(f\"NoData value: {src.nodata}\")\n",
    "    data = src.read(1)\n",
    "    unique_vals = np.unique(data[data != src.nodata])  # Exclude NoData\n",
    "    print(f\"Valid class values: {sorted(unique_vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def map_class_to_group(class_val):\n",
    "    \"\"\"\n",
    "    Map original classification values to groups 1-4.\n",
    "    \n",
    "    Args:\n",
    "        class_val: Original classification value (0-17)\n",
    "    \n",
    "    Returns:\n",
    "        int: Group number (1-4) or None if invalid\n",
    "    \"\"\"\n",
    "    if class_val in [1, 2, 3]:\n",
    "        return 1\n",
    "    elif class_val in [4, 5, 6]:\n",
    "        return 2\n",
    "    elif class_val in [7, 8, 9, 10]:\n",
    "        return 3\n",
    "    elif class_val in [11, 12, 13, 14, 15, 16, 17, 0]:\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_majority_class_from_classification(dem_tile_path, class_src):\n",
    "    try:\n",
    "        with rasterio.open(dem_tile_path) as dem_src:\n",
    "            dem_bounds = dem_src.bounds\n",
    "            dem_crs = dem_src.crs\n",
    "            \n",
    "            # Check if DEM and classification have same CRS\n",
    "            if dem_crs != class_src.crs:\n",
    "                from rasterio.warp import transform_bounds\n",
    "                dem_bounds_in_class_crs = transform_bounds(\n",
    "                    dem_crs, class_src.crs, *dem_bounds\n",
    "                )\n",
    "            else:\n",
    "                dem_bounds_in_class_crs = dem_bounds\n",
    "            \n",
    "            # Get the window of class_src that overlaps with dem tile\n",
    "            class_window = rasterio.windows.from_bounds(\n",
    "                *dem_bounds_in_class_crs, \n",
    "                class_src.transform\n",
    "            )\n",
    "            \n",
    "            # Read the classification data for the overlapping area\n",
    "            class_data = class_src.read(1, window=class_window)\n",
    "            \n",
    "            # Get the actual bounds of the read window\n",
    "            window_transform = rasterio.windows.transform(class_window, class_src.transform)    \n",
    "            \n",
    "            # Calculate intersection area for each pixel with DEM bounds\n",
    "            # Now tracking by group (1-4) instead of original class\n",
    "            group_counts_weighted = {}            \n",
    "            for i in range(class_data.shape[0]):\n",
    "                for j in range(class_data.shape[1]):\n",
    "                    class_val = class_data[i, j]\n",
    "                    \n",
    "                    # Map class to group\n",
    "                    group_val = map_class_to_group(class_val)\n",
    "                    if group_val is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate pixel bounds\n",
    "                    class_pixel_left = window_transform[2] + j * window_transform[0]\n",
    "                    class_pixel_right = class_pixel_left + window_transform[0]\n",
    "                    class_pixel_top = window_transform[5] + i * window_transform[4]\n",
    "                    class_pixel_bottom = class_pixel_top + window_transform[4]\n",
    "                    \n",
    "                    # Calculate intersection with DEM bounds\n",
    "                    intersect_left = max(class_pixel_left, dem_bounds_in_class_crs[0])\n",
    "                    intersect_right = min(class_pixel_right, dem_bounds_in_class_crs[2])\n",
    "                    intersect_top = min(class_pixel_top, dem_bounds_in_class_crs[3])\n",
    "                    intersect_bottom = max(class_pixel_bottom, dem_bounds_in_class_crs[1])\n",
    "                    \n",
    "                    # Calculate intersection area\n",
    "                    if intersect_right > intersect_left and intersect_top > intersect_bottom:\n",
    "                        intersection_area = (intersect_right - intersect_left) * (intersect_top - intersect_bottom)\n",
    "                        \n",
    "                        if group_val not in group_counts_weighted:\n",
    "                            group_counts_weighted[group_val] = 0\n",
    "                        group_counts_weighted[group_val] += intersection_area\n",
    "            \n",
    "            if not group_counts_weighted:\n",
    "                print(\"No valid classes found\")\n",
    "                return None\n",
    "            \n",
    "            # Get majority group by area\n",
    "            majority_group = max(group_counts_weighted.keys(), \n",
    "                               key=lambda x: group_counts_weighted[x])\n",
    "            return int(majority_group)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dem_tile_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def copy_tile_with_structure(src_path, base_src_dir, base_dst_dir):\n",
    "    \"\"\"\n",
    "    Copy a tile while preserving folder structure.\n",
    "    \n",
    "    Args:\n",
    "        src_path: Source file path\n",
    "        base_src_dir: Base source directory (e.g., './Dataset/Cities_Tiles')\n",
    "        base_dst_dir: Base destination directory (e.g., './Dataset/Clustered/1')\n",
    "    \n",
    "    Returns:\n",
    "        str: Destination path where file was copied\n",
    "    \"\"\"\n",
    "    # Get relative path from base source directory\n",
    "    rel_path = os.path.relpath(src_path, base_src_dir)\n",
    "    \n",
    "    # Create destination path\n",
    "    dst_path = os.path.join(base_dst_dir, rel_path)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "    \n",
    "    # Copy the file\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    \n",
    "    return dst_path\n",
    "\n",
    "def find_corresponding_dem_tile(cities_tile_path, cities_tiles_dir, dem_tiles_dir):\n",
    "    \"\"\"\n",
    "    Find the corresponding DEM tile for a given cities tile.\n",
    "    \n",
    "    Args:\n",
    "        cities_tile_path: Path to cities tile\n",
    "        cities_tiles_dir: Base cities tiles directory\n",
    "        dem_tiles_dir: Base DEM tiles directory\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to corresponding DEM tile or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the cities tile path to extract city and tile coordinates\n",
    "        rel_path = os.path.relpath(cities_tile_path, cities_tiles_dir)\n",
    "        path_parts = rel_path.split(os.sep)\n",
    "        \n",
    "        if len(path_parts) < 3:\n",
    "            return None\n",
    "            \n",
    "        city_name = path_parts[0]\n",
    "        timestamp = path_parts[1]\n",
    "        tile_filename = path_parts[2]\n",
    "        \n",
    "        # Extract row and col from filename (e.g., \"red_row_004_col_005.tif\" -> \"row_004_col_005\")\n",
    "        # Split by underscore and find row/col parts\n",
    "        filename_parts = tile_filename.split('_')\n",
    "        row_col_part = None\n",
    "        \n",
    "        for i, part in enumerate(filename_parts):\n",
    "            if part == 'row' and i + 2 < len(filename_parts) and filename_parts[i + 2] == 'col':\n",
    "                row_col_part = f\"row_{filename_parts[i + 1]}_col_{filename_parts[i + 3]}\"\n",
    "                break\n",
    "        \n",
    "        if not row_col_part:\n",
    "            return None\n",
    "        \n",
    "        # Construct DEM tile path\n",
    "        dem_tile_filename = f\"DEM_{row_col_part}.tif\"\n",
    "        dem_tile_path = os.path.join(dem_tiles_dir, city_name, dem_tile_filename)\n",
    "        \n",
    "        if os.path.exists(dem_tile_path):\n",
    "            return dem_tile_path\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding corresponding DEM tile for {cities_tile_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cluster_tiles_by_classification(dataset_dir, classification_raster_path):\n",
    "    \"\"\"\n",
    "    Main function to cluster tiles by classification.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to Dataset directory\n",
    "        classification_raster_path: Path to classification raster (original classes 0-17, grouped into 1-4)\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    cities_tiles_dir = dataset_path / \"Cities_Tiles\"\n",
    "    dem_tiles_dir = dataset_path / \"DEM_2014_Tiles\"\n",
    "    clustered_dir = dataset_path / \"Clustered\"\n",
    "    \n",
    "    # Create cluster directories for groups 1-4\n",
    "    for group_num in range(1, 5):  # Changed to groups 1-4\n",
    "        cluster_dir = clustered_dir / str(group_num)\n",
    "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for cities and DEM tiles\n",
    "        (cluster_dir / \"Cities_Tiles\").mkdir(exist_ok=True)\n",
    "        (cluster_dir / \"DEM_2014_Tiles\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Statistics tracking - Updated for groups 1-4\n",
    "    stats = {i: 0 for i in range(1, 5)}  # Changed to groups 1-4\n",
    "    total_processed = 0\n",
    "    failed_processing = 0\n",
    "    \n",
    "    print(\"Starting tile clustering by classification...\")\n",
    "    print(\"Classification mapping:\")\n",
    "    print(\"  Group 1: Classes 1-3\")\n",
    "    print(\"  Group 2: Classes 4-6\") \n",
    "    print(\"  Group 3: Classes 7-10\")\n",
    "    print(\"  Group 4: Classes 11-17, 0\")\n",
    "    print(f\"Classification raster: {classification_raster_path}\")\n",
    "    print(f\"Cities tiles directory: {cities_tiles_dir}\")\n",
    "    print(f\"DEM tiles directory: {dem_tiles_dir}\")\n",
    "    \n",
    "    # Open the classification raster once for all processing\n",
    "    with rasterio.open(classification_raster_path) as class_src:\n",
    "        # Process all cities tiles\n",
    "        cities_tiles_list = []\n",
    "        for city_dir in cities_tiles_dir.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                for timestamp_dir in city_dir.iterdir():\n",
    "                    if timestamp_dir.is_dir():\n",
    "                        for tile_file in timestamp_dir.glob(\"*.tif\"):\n",
    "                            cities_tiles_list.append(tile_file)\n",
    "        \n",
    "        print(f\"Found {len(cities_tiles_list)} cities tiles to process\")\n",
    "        \n",
    "        # Process each cities tile\n",
    "        for cities_tile_path in tqdm(cities_tiles_list, desc=\"Processing tiles\"):\n",
    "            total_processed += 1\n",
    "            \n",
    "            # Get majority group for this tile\n",
    "            majority_group = get_majority_class_from_classification(\n",
    "                str(cities_tile_path), \n",
    "                class_src\n",
    "            )\n",
    "            \n",
    "            if majority_group is None:\n",
    "                failed_processing += 1\n",
    "                continue\n",
    "            \n",
    "            # Validate group is in expected range\n",
    "            if majority_group < 1 or majority_group > 4:\n",
    "                print(f\"Warning: Invalid group {majority_group} for tile {cities_tile_path}\")\n",
    "                failed_processing += 1\n",
    "                continue\n",
    "            \n",
    "            # Update statistics\n",
    "            stats[majority_group] += 1\n",
    "            \n",
    "            # Copy cities tile to appropriate cluster directory\n",
    "            cluster_cities_dir = clustered_dir / str(majority_group) / \"Cities_Tiles\"\n",
    "            copy_tile_with_structure(\n",
    "                str(cities_tile_path),\n",
    "                str(cities_tiles_dir),\n",
    "                str(cluster_cities_dir)\n",
    "            )        \n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== CLUSTERING RESULTS ===\")\n",
    "    print(f\"Total tiles processed: {total_processed}\")\n",
    "    print(f\"Failed to process: {failed_processing}\")\n",
    "    print(f\"Successfully clustered: {sum(stats.values())}\")\n",
    "    print(\"\\nTiles per group:\")\n",
    "    group_descriptions = {\n",
    "        1: \"Classes 1-3\",\n",
    "        2: \"Classes 4-6\", \n",
    "        3: \"Classes 7-10\",\n",
    "        4: \"Classes 11-17, 0\"\n",
    "    }\n",
    "    for group_num in range(1, 5):  # Changed to groups 1-4\n",
    "        print(f\"  Group {group_num} ({group_descriptions[group_num]}): {stats[group_num]} tiles\")\n",
    "    \n",
    "    print(f\"\\nClustered tiles saved to: {clustered_dir}\")\n",
    "    \n",
    "    # Verify folder structure\n",
    "    print(\"\\n=== FOLDER STRUCTURE VERIFICATION ===\")\n",
    "    for group_num in range(1, 5):  # Changed to groups 1-4\n",
    "        class_dir = clustered_dir / str(group_num)\n",
    "        cities_count = len(list((class_dir / \"Cities_Tiles\").rglob(\"*.tif\")))\n",
    "        dem_count = len(list((class_dir / \"DEM_2014_Tiles\").rglob(\"*.tif\")))\n",
    "        print(f\"Group {group_num} ({group_descriptions[group_num]}): {cities_count} cities tiles, {dem_count} DEM tiles\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your paths here\n",
    "    dataset_directory = \"./Data/Dataset\"\n",
    "    classification_raster = \"./original_LCZ.tif\"  # Your classification raster with values 0-17\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not os.path.exists(dataset_directory):\n",
    "        print(f\"Error: Dataset directory not found: {dataset_directory}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not os.path.exists(classification_raster):\n",
    "        print(f\"Error: Classification raster not found: {classification_raster}\")\n",
    "        print(\"Please update the 'classification_raster' variable with the correct path\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Run clustering\n",
    "    cluster_tiles_by_classification(dataset_directory, classification_raster)\n",
    "    \n",
    "    print(\"\\nClustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ff5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
