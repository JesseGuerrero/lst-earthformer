{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9829c732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T18:32:45.543696Z",
     "start_time": "2025-07-13T18:32:19.756041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found tiled dataset at ./Data/ML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./logs/wandb/run-20250717_153140-k49syb2z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z' target=\"_blank\">spring-dust-4</a></strong> to <a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'medium' initialized with 14,978,705 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/native_amp.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 174 interpolated scenes to exclude from ground truth\n",
      "Examples of interpolated scenes:\n",
      "  Arlington_TX/2013-12-15T12:00:00Z\n",
      "  Arlington_TX/2015-02-15T12:00:00Z\n",
      "  Arlington_TX/2015-03-15T12:00:00Z\n",
      "  Arlington_TX/2015-11-15T12:00:00Z\n",
      "  Arlington_TX/2016-04-15T12:00:00Z\n",
      "  ... and 169 more\n",
      "Debug monthly splits for year 2014:\n",
      "  Train months: [1, 2, 3, 4, 5, 6, 7, 8] (Jan-Aug)\n",
      "  Val months: [6, 7, 8, 9, 10] (Jun-Oct)\n",
      "  Test months: [8, 9, 10, 11, 12] (Aug-Dec)\n",
      "  Current split (train): [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "🔄 Building tile sequences for train split using 124 cores...\n",
      "   Excluding 174 interpolated scenes from ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cities (train): 100%|██████████| 124/124 [00:43<00:00,  2.86city/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences by month for train split (year 2014):\n",
      "  01 (Jan): 3062 sequences\n",
      "  02 (Feb): 3862 sequences\n",
      "  03 (Mar): 2790 sequences\n",
      "  04 (Apr): 2813 sequences\n",
      "  05 (May): 3124 sequences\n",
      "  06 (Jun): 0 sequences\n",
      "  07 (Jul): 0 sequences\n",
      "  08 (Aug): 0 sequences\n",
      "\n",
      "=== INTERPOLATED SCENE FILTERING STATS ===\n",
      "Interpolated scenes loaded: 174\n",
      "Valid sequences after filtering: 15651\n",
      "Interpolated scenes affect years in this split: [2014]\n",
      "==================================================\n",
      "DEBUG train split: 124 cities, year 2014, months [1, 2, 3, 4, 5, 6, 7, 8], 15651 tile sequences\n",
      "Loaded 174 interpolated scenes to exclude from ground truth\n",
      "Examples of interpolated scenes:\n",
      "  Arlington_TX/2013-12-15T12:00:00Z\n",
      "  Arlington_TX/2015-02-15T12:00:00Z\n",
      "  Arlington_TX/2015-03-15T12:00:00Z\n",
      "  Arlington_TX/2015-11-15T12:00:00Z\n",
      "  Arlington_TX/2016-04-15T12:00:00Z\n",
      "  ... and 169 more\n",
      "Debug monthly splits for year 2014:\n",
      "  Train months: [1, 2, 3, 4, 5, 6, 7, 8] (Jan-Aug)\n",
      "  Val months: [6, 7, 8, 9, 10] (Jun-Oct)\n",
      "  Test months: [8, 9, 10, 11, 12] (Aug-Dec)\n",
      "  Current split (val): [6, 7, 8, 9, 10]\n",
      "\n",
      "🔄 Building tile sequences for val split using 124 cores...\n",
      "   Excluding 174 interpolated scenes from ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cities (val): 100%|██████████| 124/124 [00:26<00:00,  4.71city/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences by month for val split (year 2014):\n",
      "  06 (Jun): 3106 sequences\n",
      "  07 (Jul): 3114 sequences\n",
      "  08 (Aug): 0 sequences\n",
      "  09 (Sep): 0 sequences\n",
      "  10 (Oct): 0 sequences\n",
      "\n",
      "=== INTERPOLATED SCENE FILTERING STATS ===\n",
      "Interpolated scenes loaded: 174\n",
      "Valid sequences after filtering: 6220\n",
      "Interpolated scenes affect years in this split: [2014]\n",
      "==================================================\n",
      "DEBUG val split: 124 cities, year 2014, months [6, 7, 8, 9, 10], 6220 tile sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:381: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'gradient_clip_algorithm', 'gradient_clip_val'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | model     | CuboidTransformerModel | 15.0 M\n",
      "1 | criterion | MSELoss                | 0     \n",
      "-----------------------------------------------------\n",
      "15.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 M    Total params\n",
      "29.957    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355e0cf38e8148e3b5503beeb2192424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 0\n",
      "✅ Successfully logged validation image at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (39) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a0e24fe98b4de590d17671ca11a529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c33e189db6441a482ba824a63f404f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 0\n",
      "✅ Successfully logged validation image at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.165\n",
      "Epoch 0, global step 19: 'val_loss' reached 0.16480 (best 0.16480), saving model to './checkpoints/spring-dust-4-epoch=00-val_loss=0.165.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5e720557ea4227a464ac7abcfc02fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 0\n",
      "✅ Successfully logged validation image at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.062 >= min_delta = 0.0. New best score: 0.102\n",
      "Epoch 0, global step 38: 'val_loss' reached 0.10231 (best 0.10231), saving model to './checkpoints/spring-dust-4-epoch=00-val_loss=0.102.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49922e2517b04168bf66064178ca5e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 1\n",
      "✅ Successfully logged validation image at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.067\n",
      "Epoch 1, global step 58: 'val_loss' reached 0.06681 (best 0.06681), saving model to './checkpoints/spring-dust-4-epoch=01-val_loss=0.067.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f39f6a96e2d4d8c8ac07eec5e4f9da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 1\n",
      "✅ Successfully logged validation image at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.062\n",
      "Epoch 1, global step 77: 'val_loss' reached 0.06196 (best 0.06196), saving model to './checkpoints/spring-dust-4-epoch=01-val_loss=0.062.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336d2b4b7a146cca09c4c1522ed791b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 2\n",
      "✅ Successfully logged validation image at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.045\n",
      "Epoch 2, global step 97: 'val_loss' reached 0.04524 (best 0.04524), saving model to './checkpoints/spring-dust-4-epoch=02-val_loss=0.045.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcd696159134f58be0297b602191e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Attempting to log images at epoch 2\n",
      "✅ Successfully logged validation image at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.039\n",
      "Epoch 2, global step 116: 'val_loss' reached 0.03937 (best 0.03937), saving model to './checkpoints/spring-dust-4-epoch=02-val_loss=0.039.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Running final test...\n",
      "Loaded 174 interpolated scenes to exclude from ground truth\n",
      "Examples of interpolated scenes:\n",
      "  Arlington_TX/2013-12-15T12:00:00Z\n",
      "  Arlington_TX/2015-02-15T12:00:00Z\n",
      "  Arlington_TX/2015-03-15T12:00:00Z\n",
      "  Arlington_TX/2015-11-15T12:00:00Z\n",
      "  Arlington_TX/2016-04-15T12:00:00Z\n",
      "  ... and 169 more\n",
      "Debug monthly splits for year 2014:\n",
      "  Train months: [1, 2, 3, 4, 5, 6, 7, 8] (Jan-Aug)\n",
      "  Val months: [6, 7, 8, 9, 10] (Jun-Oct)\n",
      "  Test months: [8, 9, 10, 11, 12] (Aug-Dec)\n",
      "  Current split (test): [8, 9, 10, 11, 12]\n",
      "\n",
      "🔄 Building tile sequences for test split using 124 cores...\n",
      "   Excluding 174 interpolated scenes from ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cities (test): 100%|██████████| 124/124 [00:20<00:00,  5.94city/s]\n",
      "Restoring states from the checkpoint path at ./checkpoints/spring-dust-4-epoch=02-val_loss=0.039.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences by month for test split (year 2014):\n",
      "  08 (Aug): 2334 sequences\n",
      "  09 (Sep): 2119 sequences\n",
      "  10 (Oct): 0 sequences\n",
      "  11 (Nov): 0 sequences\n",
      "  12 (Dec): 0 sequences\n",
      "\n",
      "=== INTERPOLATED SCENE FILTERING STATS ===\n",
      "Interpolated scenes loaded: 174\n",
      "Valid sequences after filtering: 4453\n",
      "Interpolated scenes affect years in this split: [2014]\n",
      "==================================================\n",
      "DEBUG test split: 124 cities, year 2014, months [8, 9, 10, 11, 12], 4453 tile sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at ./checkpoints/spring-dust-4-epoch=02-val_loss=0.039.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca534c3fccc449b8097682738aa91af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Test failed (this is okay if no test data): 'LandsatLSTPredictor' object has no attribute 'log_images_to_wandb'\n",
      "\n",
      "🎉 Training completed successfully!\n",
      "📁 Best model saved to: ./checkpoints/spring-dust-4-epoch=02-val_loss=0.039.ckpt\n",
      "🔗 View experiment at: https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▅▅▅███</td></tr><tr><td>lr-AdamW</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>█▁▁</td></tr><tr><td>train_mae</td><td>█▁▁</td></tr><tr><td>train_mae_F</td><td>█▁▁</td></tr><tr><td>train_rmse_F</td><td>█▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▃▃▄▆▆▆▇██</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁</td></tr><tr><td>val_mae</td><td>█▄▂▂▁▁</td></tr><tr><td>val_mae_F</td><td>█▄▂▂▁▁</td></tr><tr><td>val_rmse_F</td><td>█▅▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>lr-AdamW</td><td>0.001</td></tr><tr><td>train_loss</td><td>0.05925</td></tr><tr><td>train_mae</td><td>0.04769</td></tr><tr><td>train_mae_F</td><td>19.07764</td></tr><tr><td>train_rmse_F</td><td>23.69869</td></tr><tr><td>trainer/global_step</td><td>116</td></tr><tr><td>val_loss</td><td>0.03937</td></tr><tr><td>val_mae</td><td>0.0346</td></tr><tr><td>val_mae_F</td><td>13.8379</td></tr><tr><td>val_rmse_F</td><td>15.74246</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-dust-4</strong> at: <a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests/runs/k49syb2z</a><br> View project at: <a href='https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/AAAI-Project-final-tests</a><br>Synced 5 W&B file(s), 7 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./logs/wandb/run-20250717_153140-k49syb2z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "def train_landsat_model(wandb_project: str, dataset_root: str, config: dict):\n",
    "    if config[\"debug_monthly_split\"]:\n",
    "        wandb_tags = [\n",
    "            \"landsat\", \"lst-prediction\", \"earthformer\", \n",
    "            \"debug-monthly-split\", f\"model-{config.get('model_size', 'small')}\"\n",
    "        ]\n",
    "    else:\n",
    "        wandb_tags = [\n",
    "            \"landsat\", \"lst-prediction\", \"earthformer\", \n",
    "            \"year-based-split\", f\"model-{config.get('model_size', 'small')}\"\n",
    "        ]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    log_dir = \"./logs\"\n",
    "    \n",
    "    print(f\"✅ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        cluster=config[\"cluster\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],\n",
    "        output_sequence_length=config[\"output_sequence_length\"],\n",
    "        train_years=config[\"train_years\"],\n",
    "        val_years=config[\"val_years\"],\n",
    "        test_years=config[\"test_years\"],\n",
    "        debug_monthly_split=config[\"debug_monthly_split\"],\n",
    "        debug_year=config[\"debug_year\"],\n",
    "        interpolated_scenes_file=\"./Data/ML/interpolated.txt\",\n",
    "        max_input_nodata_pct=config[\"max_input_nodata_pct\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize model with configurable size\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],\n",
    "        output_sequence_length=config[\"output_sequence_length\"],\n",
    "        model_size=config.get(\"model_size\", \"small\")  # NEW: configurable model size\n",
    "    )\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{logger.experiment.name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15 if not config[\"debug_monthly_split\"] else 10,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator='gpu' if config[\"gpus\"] > 0 else 'cpu',\n",
    "        devices=config[\"gpus\"] if config[\"gpus\"] > 0 else None,\n",
    "        precision=config[\"precision\"],\n",
    "        accumulate_grad_batches=1,\n",
    "        val_check_interval=0.5,\n",
    "        limit_train_batches=config[\"limit_train_batches\"],\n",
    "        limit_val_batches=config[\"limit_val_batches\"],\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,\n",
    "        benchmark=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        print(\"\\n🧪 Running final test...\")\n",
    "        try:\n",
    "            test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "            print(f\"✅ Test completed: {test_results}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Test failed (this is okay if no test data): {e}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"📁 Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"🔗 View experiment at: {logger.experiment.url}\")\n",
    "        \n",
    "        if checkpoint_callback.best_model_path:\n",
    "            wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        print(f\"📁 Last checkpoint saved to: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\"\"\"    \n",
    "    Hyperparameters:\n",
    "        dataset_root: Path to preprocessed dataset with Cities_Tiles and DEM_2014_Tiles\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or 'mixed')\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        train_years: Years to use for training (if None, uses default 70/15/15 split)\n",
    "        val_years: Years to use for validation\n",
    "        test_years: Years to use for testing\n",
    "        use_custom_years: Whether to use custom year splits in experiment name\n",
    "        debug_monthly_split: If True, use monthly splits within debug_year for fast debugging\n",
    "        debug_year: Year to use for debug monthly splits (default: 2014)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    wandb_project = \"AAAI-Project-final-tests\"\n",
    "    dataset_root = \"./Data/ML\"  \n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"gpus\": 1,\n",
    "        \"precision\": 16,\n",
    "        \"debug_monthly_split\": True,\n",
    "        \"debug_year\": 2014,\n",
    "        \"batch_size\": 4, # Get batch size\n",
    "        \"max_epochs\": 3,\n",
    "        \"num_workers\": 8,\n",
    "        \"input_sequence_length\": 3,\n",
    "        \"output_sequence_length\": 1,\n",
    "        \"model_size\": \"medium\",  # \"tiny\", \"small\", \"medium\", \"large\"\n",
    "        \"train_years\": [2013,2014,2015,2016,2017,2018,2019,2020,2021],\n",
    "        \"val_years\": [2022,2023],\n",
    "        \"test_years\": [2024,2025],\n",
    "        \"use_custom_years\": True,\n",
    "        \"limit_train_batches\": 0.01,\n",
    "        \"limit_val_batches\": 0.01,\n",
    "        \"limit_test_batches\": 0.01,\n",
    "        \"max_input_nodata_pct\": 0.95,\n",
    "        \"cluster\": \"all\" #1,2,3,4, all\n",
    "    }\n",
    "    \n",
    "    train_landsat_model(wandb_project, dataset_root, hyperparameters)\n",
    "# Run large 5 epochs full\n",
    "# Run tiny 5 epochs full\n",
    "# run medium 5 epochs full\n",
    "# Run sweep for a month\n",
    "# run sweep for years\n",
    "\n",
    "#You updated model.py after with the gradient explosion stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"AMP available: {hasattr(torch.cuda, 'amp')}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "import pytorch_lightning as pl\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
