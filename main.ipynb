{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"✓ Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "def train_landsat_model(wandb_project: str, dataset_root: str, config: dict):\n",
    "    if config[\"debug_monthly_split\"]:\n",
    "        wandb_tags = [\"landsat\", \"lst-prediction\", \"earthformer\", \"debug-monthly-split\"]\n",
    "    else:\n",
    "        wandb_tags = [\"landsat\", \"lst-prediction\", \"earthformer\", \"year-based-split\"]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    log_dir = \"./logs\"\n",
    "    \n",
    "    # Verify tiled dataset exists\n",
    "    dataset_path = os.path.join(dataset_root)\n",
    "    cities_tiles = os.path.join(dataset_path, \"Cities_Tiles\")\n",
    "    dem_tiles = os.path.join(dataset_path, \"DEM_2014_Tiles\")\n",
    "    \n",
    "    if not os.path.exists(cities_tiles):\n",
    "        raise FileNotFoundError(f\"Cities_Tiles directory not found at {cities_tiles}. Please run convert_to_tiles() first.\")\n",
    "    if not os.path.exists(dem_tiles):\n",
    "        raise FileNotFoundError(f\"DEM_2014_Tiles directory not found at {dem_tiles}. Please run convert_to_tiles() first.\")\n",
    "    \n",
    "    print(f\"✅ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module with year-based or debug monthly splits\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],    # Changed from sequence_length\n",
    "        output_sequence_length=config[\"output_sequence_length\"],  # New parameter\n",
    "        train_years=config[\"train_years\"],\n",
    "        val_years=config[\"val_years\"],\n",
    "        test_years=config[\"test_years\"],\n",
    "        debug_monthly_split=config[\"debug_monthly_split\"],\n",
    "        debug_year=config[\"debug_year\"]\n",
    "    )\n",
    "    \n",
    "    # Test data module setup to catch issues early\n",
    "    if config[\"debug_monthly_split\"]:\n",
    "        print(f\"Testing data module setup with debug monthly splits (year {config['debug_year']})...\")\n",
    "    else:\n",
    "        print(\"Testing data module setup with year-based splits...\")\n",
    "        \n",
    "    try:\n",
    "        data_module.setup(\"fit\")\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        \n",
    "        print(f\"✅ Training batches: {len(train_loader)}\")\n",
    "        print(f\"✅ Validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Print split information\n",
    "        if config[\"debug_monthly_split\"]:\n",
    "            print(f\"✅ Debug year: {config['debug_year']}\")\n",
    "            print(f\"✅ Training months: {sorted(data_module.train_dataset.allowed_months)}\")\n",
    "            print(f\"✅ Validation months: {sorted(data_module.val_dataset.allowed_months)}\")\n",
    "            print(f\"✅ Test months: {sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else 'Not loaded'}\")\n",
    "        else:\n",
    "            print(f\"✅ Training years: {sorted(data_module.train_dataset.train_years)}\")\n",
    "            print(f\"✅ Validation years: {sorted(data_module.train_dataset.val_years)}\")\n",
    "            print(f\"✅ Test years: {sorted(data_module.train_dataset.test_years)}\")\n",
    "        \n",
    "        # Test one batch\n",
    "        if len(train_loader) > 0:\n",
    "            sample_batch = next(iter(train_loader))\n",
    "            inputs, targets = sample_batch\n",
    "            print(f\"✅ Sample batch - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "            \n",
    "            # Show sample sequence information\n",
    "            sample_seq = data_module.train_dataset.tile_sequences[0]\n",
    "            city, tile_row, tile_col, input_months, output_months = sample_seq\n",
    "            print(f\"✅ Sample sequence: {city} tile({tile_row:03d},{tile_col:03d})\")\n",
    "            print(f\"   Input months: {input_months}\")\n",
    "            print(f\"   Output months: {output_months}\")\n",
    "        else:\n",
    "            print(\"⚠️ No training batches found!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data module test failed: {e}\")\n",
    "        raise        \n",
    "    \n",
    "    # Add split-specific configuration\n",
    "    if debug_monthly_split:\n",
    "        config.update({\n",
    "            \"split_type\": \"debug_monthly\",\n",
    "            \"debug_year\": debug_year,\n",
    "            \"train_months\": sorted(data_module.train_dataset.allowed_months),\n",
    "            \"val_months\": sorted(data_module.val_dataset.allowed_months),\n",
    "            \"test_months\": sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else [],\n",
    "            \"temporal_coverage\": f\"{debug_year} (monthly splits)\",\n",
    "        })\n",
    "        \n",
    "        # Add debug info to experiment name\n",
    "        experiment_name = f\"{experiment_name}_debug_monthly_{debug_year}\"\n",
    "        \n",
    "    else:\n",
    "        config.update({\n",
    "            \"split_type\": \"year_based\",\n",
    "            \"train_years\": sorted(data_module.train_dataset.train_years),\n",
    "            \"val_years\": sorted(data_module.train_dataset.val_years),\n",
    "            \"test_years\": sorted(data_module.train_dataset.test_years),\n",
    "            \"train_year_range\": f\"{min(data_module.train_dataset.train_years)}-{max(data_module.train_dataset.train_years)}\",\n",
    "            \"val_year_range\": f\"{min(data_module.train_dataset.val_years)}-{max(data_module.train_dataset.val_years)}\",\n",
    "            \"test_year_range\": f\"{min(data_module.train_dataset.test_years)}-{max(data_module.train_dataset.test_years)}\",\n",
    "            \"temporal_coverage\": f\"{min(data_module.train_dataset.train_years)}-{max(data_module.train_dataset.test_years)}\",\n",
    "        })\n",
    "        \n",
    "        # Add year split info to experiment name if using custom years\n",
    "        if use_custom_years and train_years is not None:\n",
    "            train_range = f\"{min(train_years)}-{max(train_years)}\"\n",
    "            val_range = f\"{min(val_years)}-{max(val_years)}\" if val_years else \"auto\"\n",
    "            experiment_name = f\"{experiment_name}_train{train_range}_val{val_range}\"\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,  # Log model checkpoints to wandb\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],    # New parameter\n",
    "        output_sequence_length=config[\"output_sequence_length\"]   # New parameter\n",
    "    )\n",
    "    \n",
    "    # Test model with sample data\n",
    "    print(\"Testing model with sample data...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs)\n",
    "            print(f\"✅ Model test - Output shape: {test_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{logger.experiment.name}-{{epoch:02d}}-{{val_loss:.3f}}', #fix this, how to get wandb name\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15 if not config[\"debug_monthly_split\"] else 10,  # Shorter patience for debug\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if config[\"gpus\"] > 0 else 'cpu',\n",
    "        devices=gpus if config[\"gpus\"] > 0 else None,\n",
    "        precision=config[\"precision\"],\n",
    "        accumulate_grad_batches=1,\n",
    "        val_check_interval=1.0,\n",
    "        limit_train_batches=config[\"limit_train_batches\"],\n",
    "        limit_val_batches=config[\"limit_val_batches\"],\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,\n",
    "        benchmark=True,\n",
    "    )\n",
    "    \n",
    "    # Print comprehensive training info\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if debug_monthly_split:\n",
    "        print(f\"LANDSAT LST PREDICTION TRAINING - DEBUG MONTHLY SPLITS\")\n",
    "    else:\n",
    "        print(f\"LANDSAT LST PREDICTION TRAINING - YEAR-BASED SPLITS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_root}\")\n",
    "    print(f\"  - Cities Tiles: {cities_tiles}\")\n",
    "    print(f\"  - DEM Tiles: {dem_tiles}\")\n",
    "    \n",
    "    if debug_monthly_split:\n",
    "        print(f\"Debug Monthly Split Configuration (Year {debug_year}):\")\n",
    "        print(f\"  - Training months: {sorted(data_module.train_dataset.allowed_months)} (Jan-Aug)\")\n",
    "        print(f\"  - Validation months: {sorted(data_module.val_dataset.allowed_months)} (Jun-Oct)\")  \n",
    "        print(f\"  - Test months: {sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else 'Not loaded'} (Aug-Dec)\")\n",
    "        print(f\"  - Overlap explanation: Months overlap to ensure sequence continuity\")\n",
    "    else:\n",
    "        print(f\"Temporal Split Configuration:\")\n",
    "        print(f\"  - Training years: {sorted(data_module.train_dataset.train_years)} ({len(data_module.train_dataset.train_years)} years)\")\n",
    "        print(f\"  - Validation years: {sorted(data_module.train_dataset.val_years)} ({len(data_module.train_dataset.val_years)} years)\")\n",
    "        print(f\"  - Test years: {sorted(data_module.train_dataset.test_years)} ({len(data_module.train_dataset.test_years)} years)\")\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Max epochs: {max_epochs}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Precision: {precision}\")\n",
    "    print(f\"  - Devices: {gpus} GPU(s)\" if gpus > 0 else \"  - Device: CPU\")\n",
    "    print(f\"  - Num workers: {num_workers}\")\n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"  - Training batches: {len(train_loader)} ({len(train_loader) * batch_size} samples)\")\n",
    "    print(f\"  - Validation batches: {len(val_loader)} ({len(val_loader) * batch_size} samples)\")\n",
    "    print(f\"  - Data limits: {limit_train_batches*100:.0f}% train, {limit_val_batches*100:.0f}% val\")\n",
    "    print(f\"Logging:\")\n",
    "    print(f\"  - Experiment: {experiment_name}\")\n",
    "    print(f\"  - Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"  - Logs: {log_dir}\")\n",
    "    print(f\"  - Wandb project: {wandb_project}\")\n",
    "    print(f\"  - Wandb tags: {wandb_tags}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        if debug_monthly_split:\n",
    "            print(f\"🚀 Starting debug training with monthly splits (year {debug_year})...\")\n",
    "        else:\n",
    "            print(\"🚀 Starting training with year-based temporal splits...\")\n",
    "            \n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Test the model if we have test data\n",
    "        print(\"\\n🧪 Running final test...\")\n",
    "        try:\n",
    "            test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "            print(f\"✅ Test completed: {test_results}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Test failed (this is okay if no test data): {e}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"📁 Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"🔗 View experiment at: {logger.experiment.url}\")\n",
    "        \n",
    "        # Log final artifacts to wandb\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        print(f\"📁 Last checkpoint saved to: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Log the error to wandb\n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Ensure wandb run is finished\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\"\"\"    \n",
    "    Hyperparameters:\n",
    "        dataset_root: Path to preprocessed dataset with Cities_Tiles and DEM_2014_Tiles\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or 'mixed')\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        train_years: Years to use for training (if None, uses default 70/15/15 split)\n",
    "        val_years: Years to use for validation\n",
    "        test_years: Years to use for testing\n",
    "        use_custom_years: Whether to use custom year splits in experiment name\n",
    "        debug_monthly_split: If True, use monthly splits within debug_year for fast debugging\n",
    "        debug_year: Year to use for debug monthly splits (default: 2014)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    wandb_project = \"AAAI-Project\"\n",
    "    dataset_root = \"./Data/ML\"  \n",
    "    minimal_hyperparameters={\n",
    "        \"dataset_root\": dataset_root,\n",
    "        \"batch_size\": 1,\n",
    "        \"max_epochs\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_workers\": 0,\n",
    "        \"gpus\": 0,  # Use CPU for maximum compatibility\n",
    "        \"precision\": \"32\",\n",
    "        \"limit_train_batches\": 0.15,\n",
    "        \"limit_val_batches\": 0.15,\n",
    "        \"wandb_project\": \"landsat-debug\",\n",
    "        \"wandb_tags\": [\"minimal\", \"debug\", \"monthly\", \"cpu\"],\n",
    "        \"debug_monthly_split\": True,\n",
    "        \"debug_year\": 2014,\n",
    "        \"input_sequence_length\": 3,\n",
    "        \"output_sequence_length\": 1\n",
    "    }  \n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gpus\": 1,\n",
    "        \"precision\": 16,\n",
    "        \"debug_monthly_split\": True,\n",
    "        \"debug_year\": 2014, # Only when debug monthly split is True\n",
    "        \"batch_size\": 16,\n",
    "        \"max_epochs\": 5,\n",
    "        \"num_workers\": 4,\n",
    "        \"input_sequence_length\": 3,\n",
    "        \"output_sequence_length\": 1,\n",
    "        \"in-channels\": 9,\n",
    "        \"out-channels\": 1,\n",
    "        \"train_years\": [], # Disabled when debug monthly_split is True\n",
    "        \"val_years\": [], # Disabled when debug monthly_split is True\n",
    "        \"test_years\": [], # Disabled when debug monthly_split is True\n",
    "        \"use_custom_years\": True, # Disabled when debug monthly_split is True\n",
    "        \"limit_train_batches\": 1.0, # 1.0 for full dataset\n",
    "        \"limit_val_batches\": 1.0, # 1.0 for full dataset\n",
    "    }\n",
    "    train_landsat_model(wandb_project, dataset_root, hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"AMP available: {hasattr(torch.cuda, 'amp')}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "import pytorch_lightning as pl\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b623b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
