{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"‚úì Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"‚úì Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T18:32:45.543696Z",
     "start_time": "2025-07-13T18:32:19.756041Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "def train_landsat_model(wandb_project: str, dataset_root: str, config: dict):\n",
    "    if config[\"debug_monthly_split\"]:\n",
    "        wandb_tags = [\n",
    "            \"landsat\", \"lst-prediction\", \"earthformer\", \n",
    "            \"debug-monthly-split\", f\"model-{config.get('model_size', 'small')}\"\n",
    "        ]\n",
    "    else:\n",
    "        wandb_tags = [\n",
    "            \"landsat\", \"lst-prediction\", \"earthformer\", \n",
    "            \"year-based-split\", f\"model-{config.get('model_size', 'small')}\"\n",
    "        ]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    log_dir = \"./logs\"\n",
    "    \n",
    "    print(f\"‚úÖ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],\n",
    "        output_sequence_length=config[\"output_sequence_length\"],\n",
    "        train_years=config[\"train_years\"],\n",
    "        val_years=config[\"val_years\"],\n",
    "        test_years=config[\"test_years\"],\n",
    "        debug_monthly_split=config[\"debug_monthly_split\"],\n",
    "        debug_year=config[\"debug_year\"],\n",
    "        interpolated_scenes_file=\"./Data/ML/interpolated.txt\",\n",
    "        max_input_nodata_pct=config[\"max_input_nodata_pct\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize model with configurable size\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        input_sequence_length=config[\"input_sequence_length\"],\n",
    "        output_sequence_length=config[\"output_sequence_length\"],\n",
    "        model_size=config.get(\"model_size\", \"small\")  # NEW: configurable model size\n",
    "    )\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{logger.experiment.name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15 if not config[\"debug_monthly_split\"] else 10,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator='gpu' if config[\"gpus\"] > 0 else 'cpu',\n",
    "        devices=config[\"gpus\"] if config[\"gpus\"] > 0 else None,\n",
    "        precision=config[\"precision\"],\n",
    "        accumulate_grad_batches=1,\n",
    "        val_check_interval=0.5,\n",
    "        limit_train_batches=config[\"limit_train_batches\"],\n",
    "        limit_val_batches=config[\"limit_val_batches\"],\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,\n",
    "        benchmark=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # print(\"\\nüß™ Running final test...\")\n",
    "        # try:\n",
    "        #     test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "        #     print(f\"‚úÖ Test completed: {test_results}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"‚ö†Ô∏è Test failed (this is okay if no test data): {e}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Training completed successfully!\")\n",
    "        print(f\"üìÅ Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"üîó View experiment at: {logger.experiment.url}\")\n",
    "        \n",
    "        # if checkpoint_callback.best_model_path:\n",
    "        #     wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "        print(f\"üìÅ Last checkpoint saved to: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\"\"\"    \n",
    "    Hyperparameters:\n",
    "        dataset_root: Path to preprocessed dataset with Cities_Tiles and DEM_2014_Tiles\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or 'mixed')\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        train_years: Years to use for training (if None, uses default 70/15/15 split)\n",
    "        val_years: Years to use for validation\n",
    "        test_years: Years to use for testing\n",
    "        use_custom_years: Whether to use custom year splits in experiment name\n",
    "        debug_monthly_split: If True, use monthly splits within debug_year for fast debugging\n",
    "        debug_year: Year to use for debug monthly splits (default: 2014)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    wandb_project = \"AAAI-Project\"\n",
    "    dataset_root = \"./Data/ML\"  \n",
    "    minimal_hyperparameters={\n",
    "        \"dataset_root\": dataset_root,\n",
    "        \"batch_size\": 1,\n",
    "        \"max_epochs\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_workers\": 0,\n",
    "        \"gpus\": 0,  # Use CPU for maximum compatibility\n",
    "        \"precision\": \"32\",\n",
    "        \"limit_train_batches\": 0.15,\n",
    "        \"limit_val_batches\": 0.15,\n",
    "        \"wandb_project\": \"landsat-debug\",\n",
    "        \"wandb_tags\": [\"minimal\", \"debug\", \"monthly\", \"cpu\"],\n",
    "        \"debug_monthly_split\": True,\n",
    "        \"debug_year\": 2014,\n",
    "        \"input_sequence_length\": 3,\n",
    "        \"output_sequence_length\": 1\n",
    "    }  \n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"gpus\": 1,\n",
    "        \"precision\": 16,\n",
    "        \"debug_monthly_split\": True,\n",
    "        \"debug_year\": 2014,\n",
    "        \"batch_size\": 4,\n",
    "        \"max_epochs\": 25,\n",
    "        \"num_workers\": 8,\n",
    "        \"input_sequence_length\": 3,\n",
    "        \"output_sequence_length\": 1,\n",
    "        \"model_size\": \"tiny\",  # NEW: \"tiny\", \"small\", \"medium\", \"large\"\n",
    "        \"train_years\": [2013,2014,2015],\n",
    "        \"val_years\": [2016],\n",
    "        \"test_years\": [2017],\n",
    "        \"use_custom_years\": True,\n",
    "        \"limit_train_batches\": 0.5,\n",
    "        \"limit_val_batches\": 0.5,\n",
    "        \"limit_test_batches\": 0.5,\n",
    "        \"max_input_nodata_pct\": 0.75\n",
    "    }\n",
    "    \n",
    "    train_landsat_model(wandb_project, dataset_root, hyperparameters)\n",
    "# Run large 5 epochs full\n",
    "# Run tiny 5 epochs full\n",
    "# run medium 5 epochs full\n",
    "# Run sweep for a month\n",
    "# run sweep for years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"AMP available: {hasattr(torch.cuda, 'amp')}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "import pytorch_lightning as pl\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
