{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"✓ Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9829c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Running enhanced debug training with quarter dataset...\n",
      "✅ Found tiled dataset at ./Data/Dataset\n",
      "Testing data module setup...\n",
      "train split: 86 cities, 1351 tile sequences\n",
      "val split: 19 cities, 212 tile sequences\n",
      "✅ Training batches: 338\n",
      "✅ Validation batches: 53\n",
      "✅ Sample batch - Inputs: torch.Size([4, 3, 128, 128, 9]), Targets: torch.Size([4, 3, 128, 128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjesus-guerrero\u001b[0m (\u001b[33mjesus-guerrero-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./logs/wandb/run-20250701_141447-7rg8pmlv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv' target=\"_blank\">enhanced_debug_quarter_dataset</a></strong> to <a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/landsat-debug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 9,036,109 parameters\n",
      "Testing model with sample data...\n",
      "✅ Model test - Output shape: torch.Size([4, 3, 128, 128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LANDSAT LST PREDICTION TRAINING - TILED DATASET\n",
      "======================================================================\n",
      "Dataset: ./Data/Dataset\n",
      "  - Cities Tiles: ./Data/Dataset/Cities_Tiles\n",
      "  - DEM Tiles: ./Data/Dataset/DEM_2014_Tiles\n",
      "Batch size: 4\n",
      "Max epochs: 5\n",
      "Learning rate: 0.001\n",
      "Precision: 32\n",
      "Devices: 1 GPU(s)\n",
      "Num workers: 2\n",
      "Experiment: enhanced_debug_quarter_dataset\n",
      "Data limits: 25% train, 25% val\n",
      "Checkpoints: ./checkpoints\n",
      "Logs: ./logs\n",
      "Wandb project: landsat-debug\n",
      "Wandb tags: ['enhanced-debug', 'quarter-dataset', 'realistic-test']\n",
      "======================================================================\n",
      "\n",
      "🚀 Starting training...\n",
      "train split: 86 cities, 1351 tile sequences\n",
      "val split: 19 cities, 212 tile sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | model     | CuboidTransformerModel | 9.0 M \n",
      "1 | criterion | MSELoss                | 0     \n",
      "-----------------------------------------------------\n",
      "9.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.0 M     Total params\n",
      "36.144    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e3ae59cac84d3291ff251e80947a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: LST range 66.0°F to 100.0°F\n",
      "Sample 1: LST range 85.0°F to 119.0°F\n",
      "Sample 2: LST range 75.0°F to 102.0°F\n",
      "Sample 3: LST range 80.0°F to 98.0°F\n",
      "=== LST DATA ANALYSIS ===\n",
      "Timestep 0:\n",
      "  Input LST  - Min: 9.0, Max: 19.0, Mean: 14.5\n",
      "  Target LST - Min: 87.0, Max: 118.0, Mean: 100.6\n",
      "  Pred LST   - Min: -0.9, Max: 1.2, Mean: -0.6\n",
      "Timestep 1:\n",
      "  Input LST  - Min: 57.0, Max: 77.0, Mean: 65.7\n",
      "  Target LST - Min: 66.0, Max: 100.0, Mean: 80.2\n",
      "  Pred LST   - Min: -1.0, Max: 1.1, Mean: -0.4\n",
      "Timestep 2:\n",
      "  Input LST  - Min: 70.0, Max: 100.0, Mean: 83.2\n",
      "  Target LST - Min: 85.0, Max: 119.0, Mean: 98.9\n",
      "  Pred LST   - Min: -1.1, Max: 0.9, Mean: -0.5\n",
      "Global temp range: -0.8°F to 111.0°F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd934cd509c045149b29fd09a217d032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: LST range 79.0°F to 128.0°F\n",
      "Sample 1: LST range 104.0°F to 134.0°F\n",
      "Sample 2: LST range 108.0°F to 125.0°F\n",
      "Sample 3: LST range 104.0°F to 126.0°F\n",
      "=== LST DATA ANALYSIS ===\n",
      "Timestep 0:\n",
      "  Input LST  - Min: 56.0, Max: 105.0, Mean: 83.7\n",
      "  Target LST - Min: 84.0, Max: 129.0, Mean: 109.8\n",
      "  Pred LST   - Min: -1.4, Max: 0.9, Mean: -0.3\n",
      "Timestep 1:\n",
      "  Input LST  - Min: 76.0, Max: 124.0, Mean: 103.3\n",
      "  Target LST - Min: 79.0, Max: 128.0, Mean: 107.3\n",
      "  Pred LST   - Min: -1.3, Max: 1.2, Mean: -0.2\n",
      "Timestep 2:\n",
      "  Input LST  - Min: 86.0, Max: 128.0, Mean: 110.1\n",
      "  Target LST - Min: 75.0, Max: 125.0, Mean: 103.8\n",
      "  Pred LST   - Min: -1.5, Max: 1.0, Mean: -0.4\n",
      "Global temp range: -0.9°F to 125.0°F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb446fba482a4894b6eb8a255416bcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: LST range 66.0°F to 100.0°F\n",
      "Sample 1: LST range 85.0°F to 119.0°F\n",
      "Sample 2: LST range 75.0°F to 102.0°F\n",
      "Sample 3: LST range 80.0°F to 98.0°F\n",
      "=== LST DATA ANALYSIS ===\n",
      "Timestep 0:\n",
      "  Input LST  - Min: 9.0, Max: 19.0, Mean: 14.5\n",
      "  Target LST - Min: 87.0, Max: 118.0, Mean: 100.6\n",
      "  Pred LST   - Min: 2.7, Max: 7.5, Mean: 7.4\n",
      "Timestep 1:\n",
      "  Input LST  - Min: 57.0, Max: 77.0, Mean: 65.7\n",
      "  Target LST - Min: 66.0, Max: 100.0, Mean: 80.2\n",
      "  Pred LST   - Min: 2.7, Max: 7.5, Mean: 7.4\n",
      "Timestep 2:\n",
      "  Input LST  - Min: 70.0, Max: 100.0, Mean: 83.2\n",
      "  Target LST - Min: 85.0, Max: 119.0, Mean: 98.9\n",
      "  Pred LST   - Min: 2.7, Max: 7.6, Mean: 7.4\n",
      "Global temp range: 4.4°F to 111.0°F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 6438.942\n",
      "Epoch 0, global step 42: 'val_loss' reached 6438.94238 (best 6438.94238), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=00-val_loss=6438.942.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216c925f29314a64826aaf08f2b6dd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: LST range 66.0°F to 100.0°F\n",
      "Sample 1: LST range 85.0°F to 119.0°F\n",
      "Sample 2: LST range 75.0°F to 102.0°F\n",
      "Sample 3: LST range 80.0°F to 98.0°F\n",
      "=== LST DATA ANALYSIS ===\n",
      "Timestep 0:\n",
      "  Input LST  - Min: 9.0, Max: 19.0, Mean: 14.5\n",
      "  Target LST - Min: 87.0, Max: 118.0, Mean: 100.6\n",
      "  Pred LST   - Min: 6.1, Max: 11.8, Mean: 10.3\n",
      "Timestep 1:\n",
      "  Input LST  - Min: 57.0, Max: 77.0, Mean: 65.7\n",
      "  Target LST - Min: 66.0, Max: 100.0, Mean: 80.2\n",
      "  Pred LST   - Min: 6.1, Max: 11.8, Mean: 10.3\n",
      "Timestep 2:\n",
      "  Input LST  - Min: 70.0, Max: 100.0, Mean: 83.2\n",
      "  Target LST - Min: 85.0, Max: 119.0, Mean: 98.9\n",
      "  Pred LST   - Min: 6.1, Max: 11.8, Mean: 10.3\n",
      "Global temp range: 8.6°F to 111.0°F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 444.007 >= min_delta = 0.0. New best score: 5994.935\n",
      "Epoch 0, global step 84: 'val_loss' reached 5994.93506 (best 5994.93506), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=00-val_loss=5994.935.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dcc214c091410db2c8fba360cf9637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 491.391 >= min_delta = 0.0. New best score: 5503.544\n",
      "Epoch 1, global step 126: 'val_loss' reached 5503.54395 (best 5503.54395), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=01-val_loss=5503.544.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbed9eea39741bda995b2a405f5f411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 541.646 >= min_delta = 0.0. New best score: 4961.897\n",
      "Epoch 1, global step 168: 'val_loss' reached 4961.89746 (best 4961.89746), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=01-val_loss=4961.897.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c3ad94f9840ce89bffae2b88cc75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 572.389 >= min_delta = 0.0. New best score: 4389.508\n",
      "Epoch 2, global step 210: 'val_loss' reached 4389.50830 (best 4389.50830), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=02-val_loss=4389.508.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c86aa440ba54092bb9cdaaa9c955453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 586.537 >= min_delta = 0.0. New best score: 3802.971\n",
      "Epoch 2, global step 252: 'val_loss' reached 3802.97119 (best 3802.97119), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=02-val_loss=3802.971.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf571349cc0f46a8bacb98569cabd5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 583.048 >= min_delta = 0.0. New best score: 3219.924\n",
      "Epoch 3, global step 294: 'val_loss' reached 3219.92358 (best 3219.92358), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=03-val_loss=3219.924.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c3088c90094ac7bbc776266352eee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 555.848 >= min_delta = 0.0. New best score: 2664.076\n",
      "Epoch 3, global step 336: 'val_loss' reached 2664.07593 (best 2664.07593), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=03-val_loss=2664.076.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11438c07fe12461abbd20741068b0ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 521.288 >= min_delta = 0.0. New best score: 2142.788\n",
      "Epoch 4, global step 378: 'val_loss' reached 2142.78760 (best 2142.78760), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=04-val_loss=2142.788.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ef6fb671b491693155bcca0352e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 463.485 >= min_delta = 0.0. New best score: 1679.302\n",
      "Epoch 4, global step 420: 'val_loss' reached 1679.30225 (best 1679.30225), saving model to './checkpoints/enhanced_debug_quarter_dataset-epoch=04-val_loss=1679.302.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "Restoring states from the checkpoint path at ./checkpoints/enhanced_debug_quarter_dataset-epoch=04-val_loss=1679.302.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Running final test...\n",
      "test split: 19 cities, 0 tile sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/utilities/cloud_io.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at ./checkpoints/enhanced_debug_quarter_dataset-epoch=04-val_loss=1679.302.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test completed: []\n",
      "\n",
      "🎉 Training completed successfully!\n",
      "📁 Best model saved to: ./checkpoints/enhanced_debug_quarter_dataset-epoch=04-val_loss=1679.302.ckpt\n",
      "🔗 View experiment at: https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/earthformer15/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:153: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆█████</td></tr><tr><td>lr-AdamW</td><td>▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▇▇▄▂▁</td></tr><tr><td>train_mae_epoch</td><td>█▇▅▃▁</td></tr><tr><td>train_mae_step</td><td>█▇▄█▇▄▂▁</td></tr><tr><td>train_temp_mae_scaled</td><td>█▇▅▃▁</td></tr><tr><td>train_temp_rmse_scaled</td><td>█▇▅▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇███</td></tr><tr><td>val_correlation</td><td>▁▂▄▇▇█████</td></tr><tr><td>val_loss</td><td>█▇▇▆▅▄▃▂▂▁</td></tr><tr><td>val_mae</td><td>██▇▆▆▅▄▃▂▁</td></tr><tr><td>val_temp_mae_scaled</td><td>██▇▆▆▅▄▃▂▁</td></tr><tr><td>val_temp_rmse_scaled</td><td>█▇▇▆▆▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>lr-AdamW</td><td>0.001</td></tr><tr><td>train_loss_epoch</td><td>4936.82715</td></tr><tr><td>train_loss_step</td><td>3678.20776</td></tr><tr><td>train_mae_epoch</td><td>67.90926</td></tr><tr><td>train_mae_step</td><td>57.67937</td></tr><tr><td>train_temp_mae_scaled</td><td>67.90926</td></tr><tr><td>train_temp_rmse_scaled</td><td>69.93489</td></tr><tr><td>trainer/global_step</td><td>419</td></tr><tr><td>val_correlation</td><td>0.2399</td></tr><tr><td>val_loss</td><td>1679.30225</td></tr><tr><td>val_mae</td><td>39.99492</td></tr><tr><td>val_temp_mae_scaled</td><td>39.99492</td></tr><tr><td>val_temp_rmse_scaled</td><td>40.93497</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">enhanced_debug_quarter_dataset</strong> at: <a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/landsat-debug/runs/7rg8pmlv</a><br> View project at: <a href='https://wandb.ai/jesus-guerrero-ml/landsat-debug' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/landsat-debug</a><br>Synced 5 W&B file(s), 16 media file(s), 14 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./logs/wandb/run-20250701_141447-7rg8pmlv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced debug training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "def train_landsat_model(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    num_workers: int = 4,\n",
    "    gpus: int = 1,\n",
    "    precision: str = \"32\",  # Start with 32-bit precision for stability\n",
    "    accumulate_grad_batches: int = 1,\n",
    "    val_check_interval: float = 1.0,\n",
    "    limit_train_batches: float = 1.0,\n",
    "    limit_val_batches: float = 1.0,\n",
    "    experiment_name: str = \"landsat_lst_prediction\",\n",
    "    checkpoint_dir: str = \"./checkpoints\",\n",
    "    log_dir: str = \"./logs\",\n",
    "    wandb_project: str = \"landsat-lst-forecasting\",\n",
    "    wandb_tags: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for Landsat LST prediction\n",
    "    \n",
    "    Args:\n",
    "        dataset_root: Path to preprocessed dataset with Cities_Tiles and DEM_2014_Tiles\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or '16-mixed')\n",
    "        accumulate_grad_batches: Gradient accumulation steps\n",
    "        val_check_interval: Validation frequency\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        log_dir: Directory for logs\n",
    "        wandb_project: Weights & Biases project name\n",
    "        wandb_tags: List of tags for the experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up default tags\n",
    "    if wandb_tags is None:\n",
    "        wandb_tags = [\"landsat\", \"lstm-prediction\", \"earthformer\"]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Verify tiled dataset exists\n",
    "    dataset_path = os.path.join(dataset_root)\n",
    "    cities_tiles = os.path.join(dataset_path, \"Cities_Tiles\")\n",
    "    dem_tiles = os.path.join(dataset_path, \"DEM_2014_Tiles\")\n",
    "    \n",
    "    if not os.path.exists(cities_tiles):\n",
    "        raise FileNotFoundError(f\"Cities_Tiles directory not found at {cities_tiles}. Please run convert_to_tiles() first.\")\n",
    "    if not os.path.exists(dem_tiles):\n",
    "        raise FileNotFoundError(f\"DEM_2014_Tiles directory not found at {dem_tiles}. Please run convert_to_tiles() first.\")\n",
    "    \n",
    "    print(f\"✅ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sequence_length=3\n",
    "    )\n",
    "    \n",
    "    # Test data module setup to catch issues early\n",
    "    print(\"Testing data module setup...\")\n",
    "    try:\n",
    "        data_module.setup(\"fit\")\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        \n",
    "        print(f\"✅ Training batches: {len(train_loader)}\")\n",
    "        print(f\"✅ Validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Test one batch\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        inputs, targets = sample_batch\n",
    "        print(f\"✅ Sample batch - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data module test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Create comprehensive config for wandb\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"precision\": precision,\n",
    "        \"accumulate_grad_batches\": accumulate_grad_batches,\n",
    "        \"val_check_interval\": val_check_interval,\n",
    "        \"limit_train_batches\": limit_train_batches,\n",
    "        \"limit_val_batches\": limit_val_batches,\n",
    "        \"dataset_root\": dataset_root,\n",
    "        \"model_type\": \"CuboidTransformer\",\n",
    "        \"input_shape\": [3, 128, 128, 9],\n",
    "        \"target_shape\": [3, 128, 128, 1],\n",
    "        \"sequence_length\": 3,\n",
    "        \"train_batches\": len(train_loader),\n",
    "        \"val_batches\": len(val_loader),\n",
    "        \"total_train_samples\": len(train_loader) * batch_size,\n",
    "        \"total_val_samples\": len(val_loader) * batch_size,\n",
    "    }\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        name=experiment_name,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,  # Log model checkpoints to wandb\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    # Test model with sample data\n",
    "    print(\"Testing model with sample data...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs)\n",
    "            print(f\"✅ Model test - Output shape: {test_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True  # Added verbose for better feedback\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else None,\n",
    "        # precision=precision,  # Re-enabled precision setting\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        val_check_interval=val_check_interval,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,  # Set to True for reproducibility, False for speed\n",
    "        benchmark=True,  # Optimize for consistent input sizes\n",
    "    )\n",
    "    \n",
    "    # Print comprehensive training info\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LANDSAT LST PREDICTION TRAINING - TILED DATASET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Dataset: {dataset_root}\")\n",
    "    print(f\"  - Cities Tiles: {cities_tiles}\")\n",
    "    print(f\"  - DEM Tiles: {dem_tiles}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Max epochs: {max_epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Devices: {gpus} GPU(s)\" if gpus > 0 else \"CPU\")\n",
    "    print(f\"Num workers: {num_workers}\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"Data limits: {limit_train_batches*100:.0f}% train, {limit_val_batches*100:.0f}% val\")\n",
    "    print(f\"Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"Logs: {log_dir}\")\n",
    "    print(f\"Wandb project: {wandb_project}\")\n",
    "    print(f\"Wandb tags: {wandb_tags}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        print(\"🚀 Starting training...\")\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Test the model if we have test data\n",
    "        print(\"\\n🧪 Running final test...\")\n",
    "        try:\n",
    "            test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "            print(f\"✅ Test completed: {test_results}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Test failed (this is okay if no test data): {e}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"📁 Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"🔗 View experiment at: {logger.experiment.url}\")\n",
    "        \n",
    "        # Log final artifacts to wandb\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        print(f\"📁 Last checkpoint saved to: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Log the error to wandb\n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Ensure wandb run is finished\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "# Quick test/debug function with conservative settings\n",
    "def debug_training(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Quick debug run with small dataset fraction and conservative settings\"\"\"\n",
    "    print(\"🔧 Running debug training with tiled dataset...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=2,\n",
    "        max_epochs=3,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # Disable multiprocessing for debugging\n",
    "        gpus=1,\n",
    "        precision=\"32\",  # Use 32-bit for stability\n",
    "        limit_train_batches=0.1,  # Use only 10% of data\n",
    "        limit_val_batches=0.1,\n",
    "        experiment_name=\"debug_tiled_landsat\",\n",
    "        val_check_interval=0.5,  # Check validation twice per epoch\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"debug\", \"tiled\", \"quick-test\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Debug training completed!\")\n",
    "\n",
    "\n",
    "# Enhanced debug function with quarter dataset\n",
    "def debug_with_enhanced_logging(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Debug run using a quarter of the dataset with enhanced logging\"\"\"\n",
    "    print(\"🔧 Running enhanced debug training with quarter dataset...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=4,\n",
    "        max_epochs=5,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=2,  # Some multiprocessing for realistic testing\n",
    "        gpus=1,\n",
    "        precision=\"32\",  # Use 32-bit for stability\n",
    "        limit_train_batches=0.25,  # Use quarter of training data\n",
    "        limit_val_batches=0.25,   # Use quarter of validation data\n",
    "        experiment_name=\"enhanced_debug_quarter_dataset\",\n",
    "        val_check_interval=0.5,  # Check validation twice per epoch\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"enhanced-debug\", \"quarter-dataset\", \"realistic-test\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Enhanced debug training completed!\")\n",
    "\n",
    "\n",
    "# Even more minimal debug function\n",
    "def minimal_debug_training(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Minimal debug run with absolute minimum settings\"\"\"\n",
    "    print(\"🔧 Running minimal debug training...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=1,  # Smallest possible batch\n",
    "        max_epochs=1,  # Just one epoch\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # No multiprocessing\n",
    "        gpus=0,  # Use CPU to avoid GPU issues\n",
    "        precision=\"32\",  # Standard precision\n",
    "        limit_train_batches=0.05,  # Use only 5% of data\n",
    "        limit_val_batches=0.05,\n",
    "        experiment_name=\"minimal_debug_tiled\",\n",
    "        val_check_interval=1.0,\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"minimal\", \"debug\", \"cpu\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Minimal debug training completed!\")\n",
    "\n",
    "\n",
    "# Full training configurations\n",
    "def full_training_gpu(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Full training with optimized GPU settings\"\"\"\n",
    "    print(\"🚀 Starting full GPU training...\")\n",
    "    \n",
    "    return train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=8,  # Larger batch for better GPU utilization\n",
    "        max_epochs=50,\n",
    "        learning_rate=2e-4,\n",
    "        num_workers=4,\n",
    "        gpus=1,\n",
    "        precision=\"16-mixed\",  # Mixed precision for speed\n",
    "        experiment_name=\"landsat_full_training\",\n",
    "        val_check_interval=1.0,\n",
    "        wandb_project=\"landsat-lst-forecasting\",\n",
    "        wandb_tags=[\"full-training\", \"production\", \"earthformer\", \"gpu\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For debugging with enhanced logging:\n",
    "    debug_with_enhanced_logging()\n",
    "    \n",
    "    # For full training with enhanced logging:\n",
    "    # full_training_with_enhanced_logging()\n",
    "    \n",
    "    # For dataset analysis only:\n",
    "    # analyze_dataset_structure_only()\n",
    "    \n",
    "    # For exporting metadata to JSON:\n",
    "    # export_dataset_metadata_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
