{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"✓ Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "def train_landsat_model(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    num_workers: int = 4,\n",
    "    gpus: int = 1,\n",
    "    precision: int = 16,  \n",
    "    accumulate_grad_batches: int = 1,\n",
    "    val_check_interval: float = 1.0,\n",
    "    limit_train_batches: float = 1.0,\n",
    "    limit_val_batches: float = 1.0,\n",
    "    experiment_name: str = \"landsat_lst_prediction\",\n",
    "    checkpoint_dir: str = \"./checkpoints\",\n",
    "    log_dir: str = \"./logs\",\n",
    "    wandb_project: str = \"landsat-lst-forecasting\",\n",
    "    wandb_tags: list = None,\n",
    "    # Year-based split parameters\n",
    "    train_years: Optional[List[int]] = None,\n",
    "    val_years: Optional[List[int]] = None,\n",
    "    test_years: Optional[List[int]] = None,\n",
    "    use_custom_years: bool = False,\n",
    "    # New debug monthly split parameters\n",
    "    debug_monthly_split: bool = False,\n",
    "    debug_year: int = 2014,\n",
    "    input_sequence_length: int = 3,\n",
    "    output_sequence_length: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for Landsat LST prediction with year-based or debug monthly splits\n",
    "    \n",
    "    Args:\n",
    "        dataset_root: Path to preprocessed dataset with Cities_Tiles and DEM_2014_Tiles\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or 'mixed')\n",
    "        accumulate_grad_batches: Gradient accumulation steps\n",
    "        val_check_interval: Validation frequency\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        log_dir: Directory for logs\n",
    "        wandb_project: Weights & Biases project name\n",
    "        wandb_tags: List of tags for the experiment\n",
    "        train_years: Years to use for training (if None, uses default 70/15/15 split)\n",
    "        val_years: Years to use for validation\n",
    "        test_years: Years to use for testing\n",
    "        use_custom_years: Whether to use custom year splits in experiment name\n",
    "        debug_monthly_split: If True, use monthly splits within debug_year for fast debugging\n",
    "        debug_year: Year to use for debug monthly splits (default: 2014)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up default tags\n",
    "    if wandb_tags is None:\n",
    "        if debug_monthly_split:\n",
    "            wandb_tags = [\"landsat\", \"lst-prediction\", \"earthformer\", \"debug-monthly-split\"]\n",
    "        else:\n",
    "            wandb_tags = [\"landsat\", \"lst-prediction\", \"earthformer\", \"year-based-split\"]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Verify tiled dataset exists\n",
    "    dataset_path = os.path.join(dataset_root)\n",
    "    cities_tiles = os.path.join(dataset_path, \"Cities_Tiles\")\n",
    "    dem_tiles = os.path.join(dataset_path, \"DEM_2014_Tiles\")\n",
    "    \n",
    "    if not os.path.exists(cities_tiles):\n",
    "        raise FileNotFoundError(f\"Cities_Tiles directory not found at {cities_tiles}. Please run convert_to_tiles() first.\")\n",
    "    if not os.path.exists(dem_tiles):\n",
    "        raise FileNotFoundError(f\"DEM_2014_Tiles directory not found at {dem_tiles}. Please run convert_to_tiles() first.\")\n",
    "    \n",
    "    print(f\"✅ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module with year-based or debug monthly splits\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        input_sequence_length=input_sequence_length,    # Changed from sequence_length\n",
    "        output_sequence_length=output_sequence_length,  # New parameter\n",
    "        train_years=train_years,\n",
    "        val_years=val_years,\n",
    "        test_years=test_years,\n",
    "        debug_monthly_split=debug_monthly_split,\n",
    "        debug_year=debug_year\n",
    "    )\n",
    "    \n",
    "    # Test data module setup to catch issues early\n",
    "    if debug_monthly_split:\n",
    "        print(f\"Testing data module setup with debug monthly splits (year {debug_year})...\")\n",
    "    else:\n",
    "        print(\"Testing data module setup with year-based splits...\")\n",
    "        \n",
    "    try:\n",
    "        data_module.setup(\"fit\")\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        \n",
    "        print(f\"✅ Training batches: {len(train_loader)}\")\n",
    "        print(f\"✅ Validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Print split information\n",
    "        if debug_monthly_split:\n",
    "            print(f\"✅ Debug year: {debug_year}\")\n",
    "            print(f\"✅ Training months: {sorted(data_module.train_dataset.allowed_months)}\")\n",
    "            print(f\"✅ Validation months: {sorted(data_module.val_dataset.allowed_months)}\")\n",
    "            print(f\"✅ Test months: {sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else 'Not loaded'}\")\n",
    "        else:\n",
    "            print(f\"✅ Training years: {sorted(data_module.train_dataset.train_years)}\")\n",
    "            print(f\"✅ Validation years: {sorted(data_module.train_dataset.val_years)}\")\n",
    "            print(f\"✅ Test years: {sorted(data_module.train_dataset.test_years)}\")\n",
    "        \n",
    "        # Test one batch\n",
    "        if len(train_loader) > 0:\n",
    "            sample_batch = next(iter(train_loader))\n",
    "            inputs, targets = sample_batch\n",
    "            print(f\"✅ Sample batch - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "            \n",
    "            # Show sample sequence information\n",
    "            sample_seq = data_module.train_dataset.tile_sequences[0]\n",
    "            city, tile_row, tile_col, input_months, output_months = sample_seq\n",
    "            print(f\"✅ Sample sequence: {city} tile({tile_row:03d},{tile_col:03d})\")\n",
    "            print(f\"   Input months: {input_months}\")\n",
    "            print(f\"   Output months: {output_months}\")\n",
    "        else:\n",
    "            print(\"⚠️ No training batches found!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data module test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Create comprehensive config for wandb including split information\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"precision\": precision,\n",
    "        \"accumulate_grad_batches\": accumulate_grad_batches,\n",
    "        \"val_check_interval\": val_check_interval,\n",
    "        \"limit_train_batches\": limit_train_batches,\n",
    "        \"limit_val_batches\": limit_val_batches,\n",
    "        \"dataset_root\": dataset_root,\n",
    "        \"model_type\": \"CuboidTransformer\",\n",
    "        \"input_shape\": [input_sequence_length, 128, 128, 9],      # Updated\n",
    "        \"target_shape\": [output_sequence_length, 128, 128, 1],   # Updated\n",
    "        \"input_sequence_length\": input_sequence_length,           # New\n",
    "        \"output_sequence_length\": output_sequence_length,         # New\n",
    "        \"train_batches\": len(train_loader),\n",
    "        \"val_batches\": len(val_loader),\n",
    "        \"total_train_samples\": len(train_loader) * batch_size,\n",
    "        \"total_val_samples\": len(val_loader) * batch_size,\n",
    "        \"debug_monthly_split\": debug_monthly_split,\n",
    "    }\n",
    "    \n",
    "    # Add split-specific configuration\n",
    "    if debug_monthly_split:\n",
    "        config.update({\n",
    "            \"split_type\": \"debug_monthly\",\n",
    "            \"debug_year\": debug_year,\n",
    "            \"train_months\": sorted(data_module.train_dataset.allowed_months),\n",
    "            \"val_months\": sorted(data_module.val_dataset.allowed_months),\n",
    "            \"test_months\": sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else [],\n",
    "            \"temporal_coverage\": f\"{debug_year} (monthly splits)\",\n",
    "        })\n",
    "        \n",
    "        # Add debug info to experiment name\n",
    "        experiment_name = f\"{experiment_name}_debug_monthly_{debug_year}\"\n",
    "        \n",
    "    else:\n",
    "        config.update({\n",
    "            \"split_type\": \"year_based\",\n",
    "            \"train_years\": sorted(data_module.train_dataset.train_years),\n",
    "            \"val_years\": sorted(data_module.train_dataset.val_years),\n",
    "            \"test_years\": sorted(data_module.train_dataset.test_years),\n",
    "            \"train_year_range\": f\"{min(data_module.train_dataset.train_years)}-{max(data_module.train_dataset.train_years)}\",\n",
    "            \"val_year_range\": f\"{min(data_module.train_dataset.val_years)}-{max(data_module.train_dataset.val_years)}\",\n",
    "            \"test_year_range\": f\"{min(data_module.train_dataset.test_years)}-{max(data_module.train_dataset.test_years)}\",\n",
    "            \"temporal_coverage\": f\"{min(data_module.train_dataset.train_years)}-{max(data_module.train_dataset.test_years)}\",\n",
    "        })\n",
    "        \n",
    "        # Add year split info to experiment name if using custom years\n",
    "        if use_custom_years and train_years is not None:\n",
    "            train_range = f\"{min(train_years)}-{max(train_years)}\"\n",
    "            val_range = f\"{min(val_years)}-{max(val_years)}\" if val_years else \"auto\"\n",
    "            experiment_name = f\"{experiment_name}_train{train_range}_val{val_range}\"\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        name=experiment_name,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,  # Log model checkpoints to wandb\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=max_epochs,\n",
    "        input_sequence_length=input_sequence_length,    # New parameter\n",
    "        output_sequence_length=output_sequence_length   # New parameter\n",
    "    )\n",
    "    \n",
    "    # Test model with sample data\n",
    "    print(\"Testing model with sample data...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs)\n",
    "            print(f\"✅ Model test - Output shape: {test_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15 if not debug_monthly_split else 10,  # Shorter patience for debug\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else None,\n",
    "        precision=precision,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        val_check_interval=val_check_interval,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,\n",
    "        benchmark=True,\n",
    "    )\n",
    "    \n",
    "    # Print comprehensive training info\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if debug_monthly_split:\n",
    "        print(f\"LANDSAT LST PREDICTION TRAINING - DEBUG MONTHLY SPLITS\")\n",
    "    else:\n",
    "        print(f\"LANDSAT LST PREDICTION TRAINING - YEAR-BASED SPLITS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_root}\")\n",
    "    print(f\"  - Cities Tiles: {cities_tiles}\")\n",
    "    print(f\"  - DEM Tiles: {dem_tiles}\")\n",
    "    \n",
    "    if debug_monthly_split:\n",
    "        print(f\"Debug Monthly Split Configuration (Year {debug_year}):\")\n",
    "        print(f\"  - Training months: {sorted(data_module.train_dataset.allowed_months)} (Jan-Aug)\")\n",
    "        print(f\"  - Validation months: {sorted(data_module.val_dataset.allowed_months)} (Jun-Oct)\")  \n",
    "        print(f\"  - Test months: {sorted(data_module.test_dataset.allowed_months) if hasattr(data_module, 'test_dataset') else 'Not loaded'} (Aug-Dec)\")\n",
    "        print(f\"  - Overlap explanation: Months overlap to ensure sequence continuity\")\n",
    "    else:\n",
    "        print(f\"Temporal Split Configuration:\")\n",
    "        print(f\"  - Training years: {sorted(data_module.train_dataset.train_years)} ({len(data_module.train_dataset.train_years)} years)\")\n",
    "        print(f\"  - Validation years: {sorted(data_module.train_dataset.val_years)} ({len(data_module.train_dataset.val_years)} years)\")\n",
    "        print(f\"  - Test years: {sorted(data_module.train_dataset.test_years)} ({len(data_module.train_dataset.test_years)} years)\")\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Max epochs: {max_epochs}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Precision: {precision}\")\n",
    "    print(f\"  - Devices: {gpus} GPU(s)\" if gpus > 0 else \"  - Device: CPU\")\n",
    "    print(f\"  - Num workers: {num_workers}\")\n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"  - Training batches: {len(train_loader)} ({len(train_loader) * batch_size} samples)\")\n",
    "    print(f\"  - Validation batches: {len(val_loader)} ({len(val_loader) * batch_size} samples)\")\n",
    "    print(f\"  - Data limits: {limit_train_batches*100:.0f}% train, {limit_val_batches*100:.0f}% val\")\n",
    "    print(f\"Logging:\")\n",
    "    print(f\"  - Experiment: {experiment_name}\")\n",
    "    print(f\"  - Checkpoints: {checkpoint_dir}\")\n",
    "    print(f\"  - Logs: {log_dir}\")\n",
    "    print(f\"  - Wandb project: {wandb_project}\")\n",
    "    print(f\"  - Wandb tags: {wandb_tags}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        if debug_monthly_split:\n",
    "            print(f\"🚀 Starting debug training with monthly splits (year {debug_year})...\")\n",
    "        else:\n",
    "            print(\"🚀 Starting training with year-based temporal splits...\")\n",
    "            \n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Test the model if we have test data\n",
    "        print(\"\\n🧪 Running final test...\")\n",
    "        try:\n",
    "            test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "            print(f\"✅ Test completed: {test_results}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Test failed (this is okay if no test data): {e}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"📁 Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"🔗 View experiment at: {logger.experiment.url}\")\n",
    "        \n",
    "        # Log final artifacts to wandb\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        print(f\"📁 Last checkpoint saved to: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Log the error to wandb\n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Ensure wandb run is finished\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# DEBUG TRAINING FUNCTIONS (using monthly splits)\n",
    "# ================================================================================\n",
    "\n",
    "def debug_monthly_training(dataset_root: str = \"./Data/Dataset\", debug_year: int = 2014):\n",
    "    \"\"\"Quick debug run with monthly splits for rapid prototyping\"\"\"\n",
    "    print(f\"🔧 Running debug training with monthly splits (year {debug_year})...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=16,\n",
    "        max_epochs=5,  # Few epochs for quick testing\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # Disable multiprocessing for debugging\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        limit_train_batches = 0.15,\n",
    "        limit_val_batches = 0.15,\n",
    "        experiment_name=\"debug_monthly_split\",\n",
    "        val_check_interval=0.5,\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"debug\", \"monthly-split\", f\"year-{debug_year}\"],\n",
    "        debug_monthly_split=True,\n",
    "        debug_year=debug_year\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Debug monthly training completed!\")\n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "def debug_monthly_minimal(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Minimal debug run for testing basic functionality\"\"\"\n",
    "    print(\"🔧 Running minimal debug with monthly splits...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=1,\n",
    "        max_epochs=2,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,\n",
    "        gpus=0,  # Use CPU for maximum compatibility\n",
    "        precision=\"32\",\n",
    "        limit_train_batches = 0.15,\n",
    "        limit_val_batches = 0.15,\n",
    "        experiment_name=\"minimal_monthly_debug\",\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"minimal\", \"debug\", \"monthly\", \"cpu\"],\n",
    "        debug_monthly_split=True,\n",
    "        debug_year=2014\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Minimal monthly debug completed!\")\n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "def debug_monthly_different_years(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Test monthly splits with different years\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for year in [2014, 2015, 2016]:\n",
    "        print(f\"\\n🔧 Testing monthly splits for year {year}...\")\n",
    "        \n",
    "        try:\n",
    "            trainer, model, data_module = train_landsat_model(\n",
    "                dataset_root=dataset_root,\n",
    "                batch_size=2,\n",
    "                max_epochs=3,\n",
    "                learning_rate=1e-3,\n",
    "                num_workers=0,\n",
    "                gpus=1,\n",
    "                precision=16,\n",
    "                experiment_name=f\"debug_monthly_{year}\",\n",
    "                wandb_project=\"landsat-debug\",\n",
    "                wandb_tags=[\"debug\", \"monthly\", f\"year-{year}\", \"comparison\"],\n",
    "                debug_monthly_split=True,\n",
    "                debug_year=year\n",
    "            )\n",
    "            \n",
    "            results.append((year, trainer, model, data_module))\n",
    "            print(f\"✅ Year {year} completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Year {year} failed: {e}\")\n",
    "            results.append((year, None, None, None))\n",
    "    \n",
    "    print(\"\\n📊 Multi-year debug results:\")\n",
    "    for year, trainer, model, data_module in results:\n",
    "        if trainer is not None:\n",
    "            print(f\"  {year}: SUCCESS\")\n",
    "        else:\n",
    "            print(f\"  {year}: FAILED\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# REGULAR TRAINING FUNCTIONS (using year-based splits)\n",
    "# ================================================================================\n",
    "\n",
    "def debug_training(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Quick debug run with year-based splits and conservative settings\"\"\"\n",
    "    print(\"🔧 Running debug training with year-based splits...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=2,\n",
    "        max_epochs=3,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        limit_train_batches=0.1,\n",
    "        limit_val_batches=0.1,\n",
    "        experiment_name=\"debug_year_based_split\",\n",
    "        val_check_interval=0.5,\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"debug\", \"year-based\", \"quick-test\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Debug training completed!\")\n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "def debug_with_enhanced_logging(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Debug run using a quarter of the dataset with enhanced logging\"\"\"\n",
    "    print(\"🔧 Running enhanced debug training with year-based splits...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=4,\n",
    "        max_epochs=5,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=2,\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        limit_train_batches=0.25,\n",
    "        limit_val_batches=0.25,\n",
    "        experiment_name=\"enhanced_debug_year_split\",\n",
    "        val_check_interval=0.5,\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"enhanced-debug\", \"year-based\", \"realistic-test\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Enhanced debug training completed!\")\n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "def full_training_gpu(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Full training with optimized GPU settings and default year splits\"\"\"\n",
    "    print(\"🚀 Starting full GPU training with year-based splits...\")\n",
    "    \n",
    "    return train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=8,\n",
    "        max_epochs=50,\n",
    "        learning_rate=2e-4,\n",
    "        num_workers=4,\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        experiment_name=\"landsat_full_training_year_split\",\n",
    "        val_check_interval=1.0,\n",
    "        wandb_project=\"landsat-lst-forecasting\",\n",
    "        wandb_tags=[\"full-training\", \"production\", \"earthformer\", \"gpu\", \"year-based-split\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def custom_year_training(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    train_years: List[int] = None,\n",
    "    val_years: List[int] = None,\n",
    "    test_years: List[int] = None\n",
    "):\n",
    "    \"\"\"Training with custom year splits\"\"\"\n",
    "    \n",
    "    # Default to research timeline if not specified\n",
    "    if train_years is None:\n",
    "        train_years = [2013, 2014, 2015, 2016, 2017]\n",
    "    if val_years is None:\n",
    "        val_years = [2022, 2023]\n",
    "    if test_years is None:\n",
    "        test_years = [2024, 2025]\n",
    "    \n",
    "    print(f\"🚀 Starting training with custom year splits...\")\n",
    "    print(f\"   Training: {train_years}\")\n",
    "    print(f\"   Validation: {val_years}\")\n",
    "    print(f\"   Test: {test_years}\")\n",
    "    \n",
    "    return train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=6,\n",
    "        max_epochs=40,\n",
    "        learning_rate=1e-4,\n",
    "        num_workers=4,\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        experiment_name=\"landsat_custom_year_split\",\n",
    "        val_check_interval=1.0,\n",
    "        wandb_project=\"landsat-lst-forecasting\",\n",
    "        wandb_tags=[\"custom-years\", \"research-timeline\", \"earthformer\"],\n",
    "        train_years=train_years,\n",
    "        val_years=val_years,\n",
    "        test_years=test_years,\n",
    "        use_custom_years=True\n",
    "    )\n",
    "\n",
    "def debug_monthly_training(dataset_root: str = \"./Data/Dataset\", debug_year: int = 2014):\n",
    "    \"\"\"Quick debug run with monthly splits for rapid prototyping\"\"\"\n",
    "    print(f\"🔧 Running debug training with monthly splits (year {debug_year})...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=16,\n",
    "        max_epochs=5,  # Few epochs for quick testing\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # Disable multiprocessing for debugging\n",
    "        gpus=1,\n",
    "        precision=16,\n",
    "        experiment_name=\"debug_monthly_split\",\n",
    "        val_check_interval=0.5,\n",
    "        wandb_project=\"landsat-debug\",\n",
    "        wandb_tags=[\"debug\", \"monthly-split\", f\"year-{debug_year}\"],\n",
    "        debug_monthly_split=True,\n",
    "        debug_year=debug_year\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Debug monthly training completed!\")\n",
    "    return trainer, model, data_module\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    # debug_monthly_minimal()\n",
    "    debug_monthly_training()\n",
    "    # research_timeline_training()\n",
    "    # full_training_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"AMP available: {hasattr(torch.cuda, 'amp')}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "import pytorch_lightning as pl\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b623b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
