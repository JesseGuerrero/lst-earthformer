{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"✓ Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "def train_landsat_model(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    num_workers: int = 4,\n",
    "    gpus: int = 1,\n",
    "    precision: str = \"32\",  # Start with 32-bit precision for stability\n",
    "    accumulate_grad_batches: int = 1,\n",
    "    val_check_interval: float = 1.0,\n",
    "    limit_train_batches: float = 1.0,\n",
    "    limit_val_batches: float = 1.0,\n",
    "    experiment_name: str = \"landsat_lst_prediction\",\n",
    "    checkpoint_dir: str = \"./checkpoints\",\n",
    "    log_dir: str = \"./logs\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for Landsat LST prediction\n",
    "    \n",
    "    Args:\n",
    "        dataset_root: Path to preprocessed dataset\n",
    "        batch_size: Training batch size\n",
    "        max_epochs: Maximum training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        num_workers: Number of data loading workers\n",
    "        gpus: Number of GPUs to use\n",
    "        precision: Training precision ('32', '16', or '16-mixed')\n",
    "        accumulate_grad_batches: Gradient accumulation steps\n",
    "        val_check_interval: Validation frequency\n",
    "        limit_train_batches: Fraction of training data to use (for debugging)\n",
    "        limit_val_batches: Fraction of validation data to use (for debugging)\n",
    "        experiment_name: Name for logging\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        log_dir: Directory for logs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sequence_length=3\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')  # Changed to epoch for simplicity\n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        name=experiment_name,\n",
    "        version=None\n",
    "    )\n",
    "    \n",
    "    # Trainer - simplified configuration\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else None,\n",
    "        # precision=precision,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        val_check_interval=val_check_interval,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        # Additional stability settings\n",
    "        deterministic=False,  # Set to True for reproducibility, False for speed\n",
    "        benchmark=True,  # Optimize for consistent input sizes\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LANDSAT LST PREDICTION TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Dataset: {dataset_root}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Max epochs: {max_epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Devices: {gpus} GPU(s)\" if gpus > 0 else \"CPU\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Test the model\n",
    "        print(\"\\nRunning final test...\")\n",
    "        trainer.test(model, data_module, ckpt_path='best')\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining failed with error: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "# Quick test/debug function with conservative settings\n",
    "def debug_training(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Quick debug run with small dataset fraction and conservative settings\"\"\"\n",
    "    print(\"Running debug training...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=2,\n",
    "        max_epochs=3,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # Disable multiprocessing for debugging\n",
    "        gpus=1,\n",
    "        precision=\"32\",  # Use 32-bit for stability\n",
    "        limit_train_batches=0.5,  # Use only 10% of data\n",
    "        limit_val_batches=0.5,\n",
    "        experiment_name=\"debug_landsat\",\n",
    "        val_check_interval=0.5,  # Check validation twice per epoch\n",
    "    )\n",
    "    \n",
    "    print(\"Debug training completed!\")\n",
    "\n",
    "\n",
    "# Even more minimal debug function\n",
    "def minimal_debug_training(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Minimal debug run with absolute minimum settings\"\"\"\n",
    "    print(\"Running minimal debug training...\")\n",
    "    \n",
    "    trainer, model, data_module = train_landsat_model(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=1,  # Smallest possible batch\n",
    "        max_epochs=1,  # Just one epoch\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,  # No multiprocessing\n",
    "        gpus=0,  # Use CPU to avoid GPU issues\n",
    "        precision=\"32\",  # Standard precision\n",
    "        limit_train_batches=0.5,  # Use only 5% of data\n",
    "        limit_val_batches=0.5,\n",
    "        experiment_name=\"minimal_debug\",\n",
    "        val_check_interval=1.0,\n",
    "    )\n",
    "    \n",
    "    print(\"Minimal debug training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For debugging, try in this order:\n",
    "    \n",
    "    # 1. First try the standard debug\n",
    "    try:\n",
    "        debug_training()\n",
    "    except Exception as e:\n",
    "        print(f\"Debug training failed: {e}\")\n",
    "        print(\"Trying minimal debug on CPU...\")\n",
    "        \n",
    "        # 2. If that fails, try minimal debug\n",
    "        try:\n",
    "            minimal_debug_training()\n",
    "        except Exception as e:\n",
    "            print(f\"Minimal debug also failed: {e}\")\n",
    "            print(\"Please check your dataset path and dependencies.\")\n",
    "    \n",
    "    # For full training, uncomment this:\n",
    "    # train_landsat_model(\n",
    "    #     dataset_root=\"./Data/Dataset\",\n",
    "    #     batch_size=8,\n",
    "    #     max_epochs=50,\n",
    "    #     learning_rate=2e-4,\n",
    "    #     gpus=1,\n",
    "    #     precision=\"16-mixed\",  # Use mixed precision for full training\n",
    "    #     experiment_name=\"landsat_experiment_1\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2939061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from dataset import LandsatDataModule\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Force CPU mode before any operations\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "torch.set_default_device('cpu')\n",
    "\n",
    "def force_cpu_mode():\n",
    "    \"\"\"Force everything to CPU and clear GPU memory\"\"\"\n",
    "    print(\"🔧 FORCING CPU MODE\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"  ✅ GPU cache cleared\")\n",
    "    \n",
    "    # Set environment variables to force CPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    print(\"  ✅ CUDA_VISIBLE_DEVICES set to empty\")\n",
    "    \n",
    "    # Set default device to CPU\n",
    "    torch.set_default_device('cpu')\n",
    "    print(\"  ✅ Default device set to CPU\")\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check the environment for potential issues\"\"\"\n",
    "    print(\"🔍 ENVIRONMENT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import pytorch_lightning as pl\n",
    "    \n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}\")\n",
    "    print(f\"Default device: {torch.get_default_device()}\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "    \n",
    "    # Check for any CUDA tensors\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Current CUDA memory usage: {torch.cuda.memory_allocated()} bytes\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def check_dataset_files(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Check if dataset files exist and are accessible\"\"\"\n",
    "    print(f\"\\n🔍 CHECKING DATASET FILES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dataset_path = Path(dataset_root)\n",
    "    if not dataset_path.exists():\n",
    "        print(f\"❌ Dataset root does not exist: {dataset_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✅ Dataset root exists: {dataset_path}\")\n",
    "    \n",
    "    # Check for expected subdirectories\n",
    "    cities_dir = dataset_path / \"Cities_Preprocessed\"\n",
    "    dem_dir = dataset_path / \"DEM_2014_Preprocessed\"\n",
    "    \n",
    "    if cities_dir.exists():\n",
    "        cities = list(cities_dir.iterdir())\n",
    "        print(f\"✅ Cities directory found with {len(cities)} cities\")\n",
    "        \n",
    "        # Check a few city directories\n",
    "        for i, city in enumerate(cities[:3]):  # Check first 3 cities\n",
    "            if city.is_dir():\n",
    "                scenes = list(city.iterdir())\n",
    "                print(f\"  City {city.name}: {len(scenes)} scenes\")\n",
    "                \n",
    "                # Check first scene\n",
    "                if scenes:\n",
    "                    scene = scenes[0]\n",
    "                    if scene.is_dir():\n",
    "                        files = list(scene.glob(\"*.tif\"))\n",
    "                        print(f\"    Scene {scene.name}: {len(files)} .tif files\")\n",
    "                        for file in files[:3]:  # Show first 3 files\n",
    "                            print(f\"      {file.name}\")\n",
    "    else:\n",
    "        print(f\"❌ Cities directory not found: {cities_dir}\")\n",
    "        return False\n",
    "    \n",
    "    if dem_dir.exists():\n",
    "        dem_cities = list(dem_dir.iterdir())\n",
    "        print(f\"✅ DEM directory found with {len(dem_cities)} cities\")\n",
    "    else:\n",
    "        print(f\"❌ DEM directory not found: {dem_dir}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def minimal_tensor_test():\n",
    "    \"\"\"Test basic tensor operations to isolate the issue\"\"\"\n",
    "    print(\"\\n🔍 MINIMAL TENSOR TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Basic tensor creation\n",
    "        x = torch.randn(1, 3, 128, 128, 9)\n",
    "        print(f\"✅ Test 1 passed: Basic tensor creation {x.shape}\")\n",
    "        \n",
    "        # Test 2: Indexing operations that might fail\n",
    "        try:\n",
    "            selected = x[:, :, :64, :64, :]  # Spatial cropping\n",
    "            print(f\"✅ Test 2 passed: Spatial indexing {selected.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 2 failed: Spatial indexing - {e}\")\n",
    "        \n",
    "        # Test 3: Channel selection (common source of indexing errors)\n",
    "        try:\n",
    "            channels = x[:, :, :, :, [0, 1, 2]]  # Select first 3 channels\n",
    "            print(f\"✅ Test 3 passed: Channel selection {channels.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 3 failed: Channel selection - {e}\")\n",
    "        \n",
    "        # Test 4: Temporal operations\n",
    "        try:\n",
    "            temporal = x[:, 1:, :, :, :]  # Skip first timestep\n",
    "            print(f\"✅ Test 4 passed: Temporal indexing {temporal.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 4 failed: Temporal indexing - {e}\")\n",
    "            \n",
    "        # Test 5: Reshaping operations\n",
    "        try:\n",
    "            reshaped = x.reshape(1, -1)\n",
    "            print(f\"✅ Test 5 passed: Reshaping {reshaped.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 5 failed: Reshaping - {e}\")\n",
    "            \n",
    "        # Test 6: Permutation operations (common in transformers)\n",
    "        try:\n",
    "            permuted = x.permute(0, 4, 1, 2, 3)  # (B, C, T, H, W)\n",
    "            print(f\"✅ Test 6 passed: Permutation {permuted.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 6 failed: Permutation - {e}\")\n",
    "            \n",
    "        # Test 7: Complex indexing (advanced selection)\n",
    "        try:\n",
    "            indices = torch.tensor([0, 2, 4])\n",
    "            advanced = x[:, :, :, :, indices]\n",
    "            print(f\"✅ Test 7 passed: Advanced indexing {advanced.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Test 7 failed: Advanced indexing - {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Minimal tensor tests failed: {e}\")\n",
    "\n",
    "def debug_data_shapes(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Debug script to check data shapes and identify the indexing issue\"\"\"\n",
    "    \n",
    "    print(\"🔍 DEBUGGING DATA SHAPES AND TENSOR DIMENSIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Force CPU operation\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        # Initialize data module with minimal settings\n",
    "        data_module = LandsatDataModule(\n",
    "            dataset_root=dataset_root,\n",
    "            batch_size=1,\n",
    "            num_workers=0,  # Single threaded\n",
    "            sequence_length=3\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Data module created successfully\")\n",
    "        \n",
    "        # Setup data module\n",
    "        data_module.setup(\"fit\")\n",
    "        print(\"✅ Data module setup completed\")\n",
    "        \n",
    "        # Get train dataloader\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        print(f\"✅ Train dataloader created with {len(train_loader)} batches\")\n",
    "        \n",
    "        # Try to load one batch\n",
    "        print(\"\\n🔍 EXAMINING FIRST BATCH:\")\n",
    "        inputs = None\n",
    "        targets = None\n",
    "        \n",
    "        for batch_idx, (batch_inputs, batch_targets) in enumerate(train_loader):\n",
    "            inputs = batch_inputs\n",
    "            targets = batch_targets\n",
    "            \n",
    "            print(f\"\\nBatch {batch_idx}:\")\n",
    "            print(f\"  Input tensor shape: {inputs.shape}\")\n",
    "            print(f\"  Input tensor dtype: {inputs.dtype}\")\n",
    "            print(f\"  Input tensor device: {inputs.device}\")\n",
    "            print(f\"  Input tensor min/max: {inputs.min():.3f} / {inputs.max():.3f}\")\n",
    "            print(f\"  Input contains NaN: {torch.isnan(inputs).any()}\")\n",
    "            print(f\"  Input contains Inf: {torch.isinf(inputs).any()}\")\n",
    "            \n",
    "            print(f\"\\n  Target tensor shape: {targets.shape}\")\n",
    "            print(f\"  Target tensor dtype: {targets.dtype}\")\n",
    "            print(f\"  Target tensor device: {targets.device}\")\n",
    "            print(f\"  Target tensor min/max: {targets.min():.3f} / {targets.max():.3f}\")\n",
    "            print(f\"  Target contains NaN: {torch.isnan(targets).any()}\")\n",
    "            print(f\"  Target contains Inf: {torch.isinf(targets).any()}\")\n",
    "            \n",
    "            # Check individual dimensions\n",
    "            print(f\"\\n  Detailed input shape analysis:\")\n",
    "            print(f\"    Batch size: {inputs.shape[0]}\")\n",
    "            print(f\"    Time steps: {inputs.shape[1]}\")\n",
    "            print(f\"    Height: {inputs.shape[2]}\")\n",
    "            print(f\"    Width: {inputs.shape[3]}\")\n",
    "            print(f\"    Channels: {inputs.shape[4]}\")\n",
    "            \n",
    "            # Check if any dimension is 0 or unexpected\n",
    "            for i, dim in enumerate(inputs.shape):\n",
    "                if dim == 0:\n",
    "                    print(f\"    ⚠️  WARNING: Dimension {i} is 0!\")\n",
    "                if dim > 1000:\n",
    "                    print(f\"    ⚠️  WARNING: Dimension {i} is very large: {dim}\")\n",
    "            \n",
    "            # Check data range for each channel\n",
    "            print(f\"\\n  Channel-wise statistics (input):\")\n",
    "            for c in range(inputs.shape[4]):\n",
    "                channel_data = inputs[0, :, :, :, c]\n",
    "                print(f\"    Channel {c}: min={channel_data.min():.3f}, max={channel_data.max():.3f}, mean={channel_data.mean():.3f}\")\n",
    "            \n",
    "            # Only examine first batch to avoid overwhelming output\n",
    "            if batch_idx == 0:\n",
    "                break\n",
    "        \n",
    "        print(f\"✅ Successfully examined {batch_idx + 1} batch(es)\")\n",
    "        \n",
    "        # Test validation data too\n",
    "        print(f\"\\n🔍 EXAMINING VALIDATION DATA:\")\n",
    "        try:\n",
    "            val_loader = data_module.val_dataloader()\n",
    "            print(f\"✅ Validation dataloader created with {len(val_loader)} batches\")\n",
    "            \n",
    "            for batch_idx, (val_inputs, val_targets) in enumerate(val_loader):\n",
    "                print(f\"\\nValidation Batch {batch_idx}:\")\n",
    "                print(f\"  Input shape: {val_inputs.shape}\")\n",
    "                print(f\"  Target shape: {val_targets.shape}\")\n",
    "                \n",
    "                if batch_idx == 0:  # Just check first validation batch\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Validation data loading failed: {e}\")\n",
    "        \n",
    "        # Now test model components\n",
    "        print(\"\\n🔍 TESTING MODEL COMPONENTS:\")\n",
    "        try:\n",
    "            from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "            \n",
    "            # Get the actual data shape from our loaded batch\n",
    "            if inputs is not None:\n",
    "                sample_input = inputs[:1].cpu()  # Take first sample, ensure CPU\n",
    "                \n",
    "                print(f\"  Using sample input shape: {sample_input.shape}\")\n",
    "                \n",
    "                # Test with minimal config that matches your data exactly\n",
    "                minimal_config = {\n",
    "                    'input_shape': tuple(sample_input.shape[1:]),  # Remove batch dimension\n",
    "                    'target_shape': (3, 128, 128, 1),\n",
    "                    'base_units': 32,  # Much smaller\n",
    "                    'num_heads': 4,\n",
    "                    'enc_depth': [1],  # Single level\n",
    "                    'dec_depth': [1],\n",
    "                    'attn_drop': 0.0,\n",
    "                    'proj_drop': 0.0,\n",
    "                    'ffn_drop': 0.0,\n",
    "                    'num_global_vectors': 4,\n",
    "                    'use_dec_self_global': False,  # Disable to simplify\n",
    "                    'use_dec_cross_global': False,\n",
    "                    'pos_embed_type': 't+hw',\n",
    "                    'use_relative_pos': False,  # Disable to simplify\n",
    "                    'ffn_activation': 'gelu',\n",
    "                    'enc_cuboid_size': [(1, 4, 4)],  # Very conservative\n",
    "                    'enc_cuboid_strategy': [('l', 'l', 'l')],\n",
    "                    'dec_cross_cuboid_hw': [(4, 4)],\n",
    "                    'dec_cross_n_temporal': [1],\n",
    "                }\n",
    "                \n",
    "                print(f\"  Creating minimal model with config:\")\n",
    "                for key, value in minimal_config.items():\n",
    "                    print(f\"    {key}: {value}\")\n",
    "                \n",
    "                model = CuboidTransformerModel(**minimal_config)\n",
    "                model.eval()\n",
    "                model = model.cpu()  # Force CPU\n",
    "                \n",
    "                print(f\"  ✅ Minimal model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "                \n",
    "                # Test forward pass with actual data\n",
    "                print(f\"  Testing forward pass with real data...\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        print(f\"    Input tensor device: {sample_input.device}\")\n",
    "                        print(f\"    Input tensor shape: {sample_input.shape}\")\n",
    "                        print(f\"    Model device: {next(model.parameters()).device}\")\n",
    "                        \n",
    "                        # Ensure everything is on CPU\n",
    "                        sample_input = sample_input.cpu()\n",
    "                        model = model.cpu()\n",
    "                        \n",
    "                        outputs = model(sample_input)\n",
    "                        print(f\"  ✅ Forward pass successful! Output shape: {outputs.shape}\")\n",
    "                        \n",
    "                        # Check output validity\n",
    "                        print(f\"    Output device: {outputs.device}\")\n",
    "                        print(f\"    Output min/max: {outputs.min():.3f} / {outputs.max():.3f}\")\n",
    "                        print(f\"    Output contains NaN: {torch.isnan(outputs).any()}\")\n",
    "                        print(f\"    Output contains Inf: {torch.isinf(outputs).any()}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ❌ Forward pass failed: {e}\")\n",
    "                        print(f\"     Error type: {type(e).__name__}\")\n",
    "                        \n",
    "                        # More detailed debugging\n",
    "                        print(f\"  🔍 Debugging model internals...\")\n",
    "                        \n",
    "                        import traceback\n",
    "                        print(\"  Full traceback:\")\n",
    "                        traceback.print_exc()\n",
    "            else:\n",
    "                print(\"  ❌ No input data available for model testing\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"  ❌ Could not import CuboidTransformerModel: {e}\")\n",
    "            print(f\"     Check that earthformer is properly installed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Model creation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data loading failed: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main debugging function\"\"\"\n",
    "    print(\"🚀 STARTING COMPREHENSIVE DEBUG\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Force CPU mode first\n",
    "    force_cpu_mode()\n",
    "    \n",
    "    # Check environment\n",
    "    check_environment()\n",
    "    \n",
    "    # Check dataset files\n",
    "    if not check_dataset_files():\n",
    "        print(\"❌ Dataset check failed - cannot proceed with data debugging\")\n",
    "        return\n",
    "    \n",
    "    # Run minimal tensor tests\n",
    "    minimal_tensor_test()\n",
    "    \n",
    "    # Debug data shapes\n",
    "    debug_data_shapes()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🏁 DEBUGGING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"If all tests pass, the issue might be in the model configuration.\")\n",
    "    print(\"If tests fail, the issue is likely in data preprocessing or tensor operations.\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. If data loading works but model fails -> check model config\")\n",
    "    print(\"2. If data loading fails -> check dataset preprocessing\")\n",
    "    print(\"3. If tensor tests fail -> check PyTorch installation\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
