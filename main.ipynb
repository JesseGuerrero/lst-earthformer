{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "import torch\n",
    "\n",
    "# Optimized config for Landsat 3-timestep forecasting\n",
    "landsat_config = {\n",
    "    'input_shape': (3, 128, 128, 9),    # 3 input timesteps, 128x128, 9 Landsat bands\n",
    "    'target_shape': (3, 128, 128, 1),   # 3 output timesteps\n",
    "    \n",
    "    # Small model for prototyping\n",
    "    'base_units': 96,                    # Small but efficient\n",
    "    'num_heads': 6,                      # Divisible by base_units\n",
    "    'enc_depth': [2, 2],                 # 2-level hierarchy (sufficient for short sequences)\n",
    "    'dec_depth': [1, 1],                 # Matching decoder depth\n",
    "    \n",
    "    # Dropout for better generalization during prototyping\n",
    "    'attn_drop': 0.1,\n",
    "    'proj_drop': 0.1,\n",
    "    'ffn_drop': 0.1,\n",
    "    \n",
    "    # Global vectors for capturing Landsat scene patterns\n",
    "    'num_global_vectors': 8,\n",
    "    'use_dec_self_global': True,\n",
    "    'use_dec_cross_global': True,\n",
    "    \n",
    "    # Optimized for satellite imagery\n",
    "    'pos_embed_type': 't+hw',            # Separate temporal and spatial embeddings\n",
    "    'use_relative_pos': True,            # Good for satellite spatial patterns\n",
    "    'ffn_activation': 'gelu',            # Works well for vision tasks\n",
    "    \n",
    "    # Cuboid settings optimized for short temporal sequences\n",
    "    'enc_cuboid_size': [(2, 4, 4), (2, 4, 4)],     # Small temporal cuboids for 3 timesteps\n",
    "    'enc_cuboid_strategy': [('l', 'l', 'l'), ('d', 'd', 'd')],\n",
    "    \n",
    "    # Cross-attention settings for decoder\n",
    "    'dec_cross_cuboid_hw': [(4, 4), (4, 4)],\n",
    "    'dec_cross_n_temporal': [1, 2],      # Use 1-2 temporal frames for cross-attention\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = CuboidTransformerModel(**landsat_config)\n",
    "print(f\"✓ Landsat model created! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test with dummy Landsat data\n",
    "batch_size = 4  # You can use larger batches with 40GB VRAM\n",
    "dummy_landsat = torch.randn(batch_size, 3, 128, 128, 9)\n",
    "print(f\"Input shape: {dummy_landsat.shape}\")\n",
    "\n",
    "# Forward pass test\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_landsat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "\n",
    "# Memory usage estimate\n",
    "def estimate_memory_usage(model, input_shape, batch_size=1):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(batch_size, *input_shape)\n",
    "    \n",
    "    # Rough memory estimate\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1e9  # GB\n",
    "    input_memory = dummy_input.numel() * 4 / 1e9  # GB\n",
    "    \n",
    "    print(f\"Estimated memory usage:\")\n",
    "    print(f\"  Parameters: {param_memory:.2f} GB\")\n",
    "    print(f\"  Input (batch={batch_size}): {input_memory:.2f} GB\")\n",
    "    print(f\"  Activation estimate: ~{param_memory * 2:.2f} GB\")\n",
    "    print(f\"  Total estimate: ~{param_memory * 3 + input_memory:.2f} GB\")\n",
    "\n",
    "estimate_memory_usage(model, (3, 128, 128, 9), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import LandsatLSTPredictor  # Your enhanced model\n",
    "from dataset import LandsatDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "def train_landsat_model_with_images(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    num_workers: int = 4,\n",
    "    gpus: int = 1,\n",
    "    precision: str = \"32\",\n",
    "    accumulate_grad_batches: int = 1,\n",
    "    val_check_interval: float = 1.0,\n",
    "    limit_train_batches: float = 1.0,\n",
    "    limit_val_batches: float = 1.0,\n",
    "    experiment_name: str = \"landsat_lst_prediction_with_viz\",\n",
    "    checkpoint_dir: str = \"./checkpoints\",\n",
    "    log_dir: str = \"./logs\",\n",
    "    wandb_project: str = \"landsat-lst-forecasting\",\n",
    "    wandb_tags: list = None,\n",
    "    log_images_every_n_epochs: int = 5,\n",
    "    max_images_to_log: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced training pipeline with WandB image logging\n",
    "    \n",
    "    Args:\n",
    "        log_images_every_n_epochs: How often to log images (every N epochs)\n",
    "        max_images_to_log: Maximum number of samples to visualize per batch\n",
    "    \"\"\"\n",
    "    \n",
    "    if wandb_tags is None:\n",
    "        wandb_tags = [\"landsat\", \"lst-prediction\", \"earthformer\", \"with-viz\"]\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Verify dataset\n",
    "    dataset_path = os.path.join(dataset_root)\n",
    "    cities_tiles = os.path.join(dataset_path, \"Cities_Tiles\")\n",
    "    dem_tiles = os.path.join(dataset_path, \"DEM_2014_Tiles\")\n",
    "    \n",
    "    if not os.path.exists(cities_tiles):\n",
    "        raise FileNotFoundError(f\"Cities_Tiles directory not found at {cities_tiles}\")\n",
    "    if not os.path.exists(dem_tiles):\n",
    "        raise FileNotFoundError(f\"DEM_2014_Tiles directory not found at {dem_tiles}\")\n",
    "    \n",
    "    print(f\"✅ Found tiled dataset at {dataset_root}\")\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sequence_length=3\n",
    "    )\n",
    "    \n",
    "    # Test data module\n",
    "    print(\"Testing data module setup...\")\n",
    "    try:\n",
    "        data_module.setup(\"fit\")\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        \n",
    "        print(f\"✅ Training batches: {len(train_loader)}\")\n",
    "        print(f\"✅ Validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Test one batch\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        inputs, targets = sample_batch\n",
    "        print(f\"✅ Sample batch - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data module test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Enhanced config for wandb\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"precision\": precision,\n",
    "        \"accumulate_grad_batches\": accumulate_grad_batches,\n",
    "        \"val_check_interval\": val_check_interval,\n",
    "        \"limit_train_batches\": limit_train_batches,\n",
    "        \"limit_val_batches\": limit_val_batches,\n",
    "        \"dataset_root\": dataset_root,\n",
    "        \"model_type\": \"CuboidTransformer\",\n",
    "        \"input_shape\": [3, 128, 128, 9],\n",
    "        \"target_shape\": [3, 128, 128, 1],\n",
    "        \"sequence_length\": 3,\n",
    "        \"train_batches\": len(train_loader),\n",
    "        \"val_batches\": len(val_loader),\n",
    "        \"total_train_samples\": len(train_loader) * batch_size,\n",
    "        \"total_val_samples\": len(val_loader) * batch_size,\n",
    "        \"log_images_every_n_epochs\": log_images_every_n_epochs,\n",
    "        \"max_images_to_log\": max_images_to_log,\n",
    "        \"band_names\": ['DEM', 'LST', 'Red', 'Green', 'Blue', 'NDVI', 'NDWI', 'NDBI', 'Albedo']\n",
    "    }\n",
    "    \n",
    "    # Initialize Weights & Biases logger\n",
    "    logger = WandbLogger(\n",
    "        project=wandb_project,\n",
    "        name=experiment_name,\n",
    "        tags=wandb_tags,\n",
    "        config=config,\n",
    "        save_dir=log_dir,\n",
    "        log_model=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize enhanced model with image logging\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=max_epochs,\n",
    "        log_images_every_n_epochs=log_images_every_n_epochs,\n",
    "        max_images_to_log=max_images_to_log\n",
    "    )\n",
    "    \n",
    "    # Test model\n",
    "    print(\"Testing model with sample data...\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs)\n",
    "            print(f\"✅ Model test - Output shape: {test_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model test failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else None,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        val_check_interval=val_check_interval,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=False,\n",
    "        benchmark=True,\n",
    "    )\n",
    "    \n",
    "    # Print training info\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LANDSAT LST PREDICTION TRAINING - WITH IMAGE VISUALIZATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Dataset: {dataset_root}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Max epochs: {max_epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Devices: {gpus} GPU(s)\" if gpus > 0 else \"CPU\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"Image logging: Every {log_images_every_n_epochs} epochs\")\n",
    "    print(f\"Max images per log: {max_images_to_log}\")\n",
    "    print(f\"Wandb project: {wandb_project}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        print(\"🚀 Starting training with image logging...\")\n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        # Test the model\n",
    "        print(\"\\n🧪 Running final test...\")\n",
    "        try:\n",
    "            test_results = trainer.test(model, data_module, ckpt_path='best')\n",
    "            print(f\"✅ Test completed: {test_results}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Test failed: {e}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"📁 Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"🔗 View experiment with images at: {logger.experiment.url}\")\n",
    "        \n",
    "        # Log final artifacts\n",
    "        if checkpoint_callback.best_model_path:\n",
    "            wandb.save(checkpoint_callback.best_model_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Training interrupted by user\")\n",
    "        print(f\"📁 Last checkpoint: {checkpoint_callback.last_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if 'logger' in locals():\n",
    "            wandb.log({\"error\": str(e)})\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if 'logger' in locals():\n",
    "            wandb.finish()\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "\n",
    "def debug_with_images(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Debug run with image logging\"\"\"\n",
    "    print(\"🔧 Running debug training with image visualization...\")\n",
    "    \n",
    "    return train_landsat_model_with_images(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=2,\n",
    "        max_epochs=3,\n",
    "        learning_rate=1e-3,\n",
    "        num_workers=0,\n",
    "        gpus=1,\n",
    "        precision=\"32\",\n",
    "        limit_train_batches=0.1,\n",
    "        limit_val_batches=0.1,\n",
    "        experiment_name=\"debug_with_images\",\n",
    "        val_check_interval=0.5,\n",
    "        wandb_project=\"landsat-debug-viz\",\n",
    "        wandb_tags=[\"debug\", \"visualization\", \"landsat\"],\n",
    "        log_images_every_n_epochs=1,  # Log every epoch for debugging\n",
    "        max_images_to_log=2\n",
    "    )\n",
    "\n",
    "\n",
    "def full_training_with_viz(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Full training with image visualization\"\"\"\n",
    "    print(\"🚀 Starting full training with visualization...\")\n",
    "    \n",
    "    return train_landsat_model_with_images(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=8,\n",
    "        max_epochs=50,\n",
    "        learning_rate=2e-4,\n",
    "        num_workers=4,\n",
    "        gpus=1,\n",
    "        precision=\"16-mixed\",\n",
    "        experiment_name=\"landsat_full_training_with_viz\",\n",
    "        val_check_interval=1.0,\n",
    "        wandb_project=\"landsat-lst-forecasting\",\n",
    "        wandb_tags=[\"full-training\", \"visualization\", \"earthformer\", \"gpu\"],\n",
    "        log_images_every_n_epochs=5,  # Log images every 5 epochs\n",
    "        max_images_to_log=4\n",
    "    )\n",
    "\n",
    "\n",
    "# Additional utility function for creating sample visualizations\n",
    "def create_sample_visualization(dataset_root: str = \"./Data/Dataset\"):\n",
    "    \"\"\"Create and log a sample visualization without training\"\"\"\n",
    "    from dataset import LandsatDataModule\n",
    "    import matplotlib.pyplot as plt\n",
    "    import wandb\n",
    "    \n",
    "    print(\"📊 Creating sample visualization...\")\n",
    "    \n",
    "    # Initialize wandb for standalone visualization\n",
    "    wandb.init(\n",
    "        project=\"landsat-sample-viz\",\n",
    "        name=\"sample_tiles_preview\",\n",
    "        tags=[\"sample\", \"preview\", \"no-training\"]\n",
    "    )\n",
    "    \n",
    "    # Load sample data\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=4,\n",
    "        num_workers=0,\n",
    "        sequence_length=3\n",
    "    )\n",
    "    \n",
    "    data_module.setup(\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    \n",
    "    # Get one batch\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    inputs, targets = sample_batch\n",
    "    \n",
    "    print(f\"Sample data shapes - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
    "    \n",
    "    # Create temporary model for visualization methods\n",
    "    temp_model = LandsatLSTPredictor()\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig1 = temp_model.create_landsat_visualization(inputs, targets, None, 0, 4)\n",
    "    fig2 = temp_model.create_temporal_sequence_viz(inputs, targets, None, 0)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"sample_landsat_tiles\": wandb.Image(fig1, caption=\"Sample Landsat Tiles\"),\n",
    "        \"sample_temporal_sequence\": wandb.Image(fig2, caption=\"Sample Temporal Sequence\")\n",
    "    })\n",
    "    \n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    print(\"✅ Sample visualization created and logged to WandB!\")\n",
    "    print(f\"🔗 View at: {wandb.run.url}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Then add this function to your training script:\n",
    "def train_with_visualization(\n",
    "    dataset_root: str = \"./Data/Dataset\",\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 50,\n",
    "    learning_rate: float = 2e-4,\n",
    "    num_workers: int = 4,\n",
    "    gpus: int = 1,\n",
    "    experiment_name: str = \"landsat_with_viz\",\n",
    "    log_images_every_n_epochs: int = 5,\n",
    "    max_images_to_log: int = 4\n",
    "):\n",
    "    \"\"\"Enhanced training with visualization\"\"\"\n",
    "    \n",
    "    # Your existing data module setup\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=dataset_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        sequence_length=3\n",
    "    )\n",
    "    \n",
    "    # Enhanced WandB logger\n",
    "    logger = WandbLogger(\n",
    "        project=\"landsat-lst-forecasting\",\n",
    "        name=experiment_name,\n",
    "        tags=[\"earthformer\", \"visualization\", \"landsat\"],\n",
    "        save_dir=\"./logs\",\n",
    "        log_model=True,\n",
    "    )\n",
    "    \n",
    "    # Enhanced model with visualization\n",
    "    model = LandsatLSTPredictor(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=1000,\n",
    "        max_epochs=max_epochs,\n",
    "        log_images_every_n_epochs=log_images_every_n_epochs,\n",
    "        max_images_to_log=max_images_to_log\n",
    "    )\n",
    "    \n",
    "    # Your existing callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=f'{experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n",
    "        save_top_k=3,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Your existing trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else None,\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "    )\n",
    "    \n",
    "    # Train as usual\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    return trainer, model, data_module\n",
    "\n",
    "# Quick debug function\n",
    "def debug_visualization():\n",
    "    \"\"\"Quick test with image logging\"\"\"\n",
    "    return train_with_visualization(\n",
    "        batch_size=2,\n",
    "        max_epochs=3,\n",
    "        experiment_name=\"debug_viz\",\n",
    "        log_images_every_n_epochs=1,  # Log every epoch for debugging\n",
    "        max_images_to_log=2\n",
    "    )\n",
    "\n",
    "def test_visualization_only():\n",
    "    \"\"\"Just create sample visualizations to test\"\"\"\n",
    "    from dataset import LandsatDataModule\n",
    "    \n",
    "    # Initialize wandb for testing\n",
    "    wandb.init(project=\"landsat-viz-test\", name=\"test_viz\")\n",
    "    \n",
    "    # Load sample data\n",
    "    data_module = LandsatDataModule(\n",
    "        dataset_root=\"./Data/Dataset\",\n",
    "        batch_size=2,\n",
    "        num_workers=0\n",
    "    )\n",
    "    data_module.setup(\"fit\")\n",
    "    \n",
    "    # Get one batch\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    \n",
    "    # Create model just for visualization\n",
    "    model = LandsatLSTPredictor()\n",
    "    \n",
    "    # Create dummy predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs)\n",
    "    \n",
    "    # Test visualization\n",
    "    fig = model.create_landsat_visualization(inputs, targets, predictions)\n",
    "    wandb.log({\"test_visualization\": wandb.Image(fig)})\n",
    "    \n",
    "    plt.close(fig)\n",
    "    wandb.finish()\n",
    "    print(\"✅ Test visualization created and logged to WandB!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test_visualization_only()\n",
    "    # debug_visualization()\n",
    "\n",
    "    # For debugging with image logging:\n",
    "    debug_with_images()\n",
    "    \n",
    "    # For full training with visualization:\n",
    "    # full_training_with_viz()\n",
    "    \n",
    "    # For just creating sample visualizations:\n",
    "    # create_sample_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
