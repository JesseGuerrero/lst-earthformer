{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2025-07-07 02:16:05\n",
      "INFO: Starting Abilene_TX 2013 month 6, retry -> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cloud cover for 8 scenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating cloud cover: 100%|██████████| 8/8 [00:04<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Selected scene with 0.0% cloud cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 scenes\n",
      "Processing date: 2013-06-19T17:10:19Z\n",
      "Output directory: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z\n",
      "Directory exists: True\n",
      "INFO: File already exists: ./Data/Dataset/DEM_2014/Abilene_TX/DEM.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/LST.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/red.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/green.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/blue.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/ndvi.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/ndwi.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/ndbi.tif\n",
      "INFO: Saved: ./Data/Dataset/Cities/Abilene_TX/2013-06-19T17:10:19Z/albedo.tif\n",
      "INFO: Scene save complete!\n",
      "INFO: 2025-07-07 02:16:18\n",
      "INFO: Starting Abilene_TX 2013 month 7, retry -> 0\n",
      "Evaluating cloud cover for 8 scenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating cloud cover:  25%|██▌       | 2/8 [00:01<00:04,  1.28it/s]"
     ]
    }
   ],
   "source": [
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from odc.stac import stac_load\n",
    "import numpy as np\n",
    "import os\n",
    "import rioxarray\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import struct\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('log.txt'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def calculate_cloud_cover_percentage(qa_band):\n",
    "    \"\"\"\n",
    "    Calculate cloud cover percentage from Landsat Collection 2 QA_PIXEL band\n",
    "    \n",
    "    QA_PIXEL bit meanings:\n",
    "    - Bit 3: Cloud\n",
    "    - Bit 4: Cloud shadow\n",
    "    - Bit 1: Dilated cloud\n",
    "    \"\"\"\n",
    "    # Create cloud mask (bits 1, 3, 4 indicate clouds/shadows)\n",
    "    cloud_mask = (\n",
    "        ((qa_band & (1 << 1)) != 0) |  # Dilated cloud\n",
    "        ((qa_band & (1 << 3)) != 0) |  # Cloud\n",
    "        ((qa_band & (1 << 4)) != 0)    # Cloud shadow\n",
    "    )\n",
    "    \n",
    "    total_pixels = qa_band.size\n",
    "    cloud_pixels = cloud_mask.sum().item()\n",
    "    \n",
    "    return (cloud_pixels / total_pixels) * 100\n",
    "\n",
    "def get_scenes_by_cloud_cover(query_results, bbox):\n",
    "    \"\"\"\n",
    "    Load scenes and sort by cloud cover percentage\n",
    "    \n",
    "    Args:\n",
    "        query_results: STAC query results\n",
    "        bbox: Bounding box for the area of interest\n",
    "        max_scenes: Maximum number of scenes to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (scene_data, cloud_cover_percentage)\n",
    "    \"\"\"\n",
    "    scenes_with_cloud_cover = []\n",
    "    \n",
    "    # Get all items and limit to max_scenes to avoid excessive processing\n",
    "    items = list(query_results.items())\n",
    "    \n",
    "    print(f\"Evaluating cloud cover for {len(items)} scenes...\")\n",
    "    \n",
    "    for i, item in enumerate(tqdm(items, desc=\"Calculating cloud cover\")):\n",
    "        try:\n",
    "            # Load just the QA band for this scene\n",
    "            scene_qa = stac_load(\n",
    "                [item],\n",
    "                bands=[\"qa_pixel\"],\n",
    "                bbox=bbox,\n",
    "                resolution=30,\n",
    "                crs=\"EPSG:3857\",\n",
    "                skip_broken=True,\n",
    "                fail_on_error=False\n",
    "            )\n",
    "            \n",
    "            if len(scene_qa.time) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate cloud cover percentage\n",
    "            qa_band = scene_qa['qa_pixel'].isel(time=0)\n",
    "            cloud_cover = calculate_cloud_cover_percentage(qa_band)\n",
    "            \n",
    "            # Store the item with its cloud cover percentage\n",
    "            scenes_with_cloud_cover.append((item, cloud_cover))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing scene {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort by cloud cover (ascending - least cloudy first)\n",
    "    scenes_with_cloud_cover.sort(key=lambda x: x[1])\n",
    "    \n",
    "    return scenes_with_cloud_cover\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file=\"./Data/checkpoint.json\"):\n",
    "    \"\"\"Load existing checkpoint or create new one\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"completed_years\": [], \"completed_cities\": {}, \"errored_cities\": {}, \"missing_months\": {}, \"current_year\": None, \"current_city\": None}\n",
    "\n",
    "def save_checkpoint(checkpoint_data, checkpoint_file=\"./Data/checkpoint.json\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint_data, f, indent=2)\n",
    "\n",
    "years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]#, \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "NODATA = 0     \n",
    "checkpoint = load_checkpoint()\n",
    "start_year_idx = 0\n",
    "if checkpoint[\"current_year\"]:\n",
    "    try:\n",
    "        start_year_idx = years.index(checkpoint[\"current_year\"])\n",
    "    except ValueError:\n",
    "        pass\n",
    "shapes_folder = \"./Data/City_Shapes\"\n",
    "cities = {}\n",
    "def read_shapefile_bounds(shp_path):\n",
    "    \"\"\"Read bounding box from shapefile header (bytes 36-68)\"\"\"\n",
    "    with open(shp_path, 'rb') as f:\n",
    "        f.seek(36)\n",
    "        bounds = struct.unpack('<4d', f.read(32))\n",
    "        return bounds  # [xmin, ymin, xmax, ymax]\n",
    "for shp_file in glob.glob(f\"{shapes_folder}/*.shp\"):\n",
    "    city_name = Path(shp_file).stem\n",
    "    xmin, ymin, xmax, ymax = read_shapefile_bounds(shp_file)\n",
    "    cities[city_name] = [xmin, ymin, xmax, ymax]\n",
    "for year_idx, year in enumerate(years[start_year_idx:], start_year_idx):\n",
    "    checkpoint[\"current_year\"] = year\n",
    "    city_items = sorted(list(cities.items()))\n",
    "    start_city_idx = 0\n",
    "    if year == checkpoint.get(\"current_year\") and checkpoint.get(\"current_city\"):\n",
    "        city_names = [name for name, _ in city_items]\n",
    "        try:\n",
    "            start_city_idx = city_names.index(checkpoint[\"current_city\"]) #If None go to ValueError\n",
    "        except ValueError: # Just default to the upcoming city and keep start_city_idx as 0\n",
    "            pass\n",
    "    for city_idx, (city, bbox) in enumerate(city_items[start_city_idx:], start_city_idx):\n",
    "        checkpoint[\"current_city\"] = city # This becomes default\n",
    "        save_checkpoint(checkpoint)\n",
    "        if year in checkpoint[\"completed_cities\"] and city in checkpoint[\"completed_cities\"][str(year)]:\n",
    "            continue\n",
    "        try:\n",
    "            firstMonth, lastMonth = 1, 12\n",
    "            if year == \"2013\": #Set limits for beginning of Landsat and present timing\n",
    "                firstMonth = 6\n",
    "            if year == \"2025\":\n",
    "                lastMonth = 6\n",
    "            for month in range(firstMonth,lastMonth+1):            \n",
    "                for lstRetry in range(0,20):  \n",
    "                    logging.info(datetime.now().strftime('%Y-%m-%d %H:%M:%S')) #Print Timestamp once\n",
    "                    logging.info(f\"Starting {city} {year} month {month}, retry -> {lstRetry}\")\n",
    "                    #Restart here to continue script with retry+=1\n",
    "                    last_day = calendar.monthrange(int(year), int(month))[1]\n",
    "                    client = pystac_client.Client.open(\n",
    "                        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "                        modifier=pc.sign_inplace,\n",
    "                    )\n",
    "\n",
    "                    query_landsat = client.search(\n",
    "                        collections=[\"landsat-c2-l2\"],\n",
    "                        bbox=bbox,\n",
    "                        datetime=f\"{year}-{month:02d}-01/{year}-{month:02d}-{last_day}\",   \n",
    "                        query={\"platform\": {\"in\": [\"landsat-8\", \"landsat-9\"]}}                 \n",
    "                    )\n",
    "                    query_dem = client.search(\n",
    "                        collections=[\"nasadem\"],\n",
    "                        bbox=bbox,\n",
    "                    )\n",
    "                    landsat_items = list(query_landsat.items())\n",
    "                    if len(landsat_items) == 0:\n",
    "                        logging.warning(f\"No Landsat scenes found for {city}. Skipping to next month/city.\")\n",
    "                        continue\n",
    "                    dem_items = list(query_dem.items())\n",
    "                    if len(dem_items) == 0:\n",
    "                        print(f\"No DEM data found for {city}. Skipping to next month/city.\")\n",
    "                        continue                \n",
    "                    scenes_with_cloud_cover = get_scenes_by_cloud_cover(query_landsat, bbox)\n",
    "                    if lstRetry > len(scenes_with_cloud_cover)-1:\n",
    "                        logging.warning(\"Retry thermal had no scenes left\")\n",
    "                        if year not in checkpoint[\"missing_months\"]:\n",
    "                            checkpoint[\"missing_months\"][year] = {}\n",
    "                        if city not in checkpoint[\"missing_months\"][year]:\n",
    "                            checkpoint[\"missing_months\"][year][city] = \"1\"\n",
    "                        elif city in checkpoint[\"missing_months\"][year]:\n",
    "                            checkpoint[\"missing_months\"][year][city] = str(int(checkpoint[\"missing_months\"][year][city]) + 1)\n",
    "                        break\n",
    "                    if not scenes_with_cloud_cover:                                            \n",
    "                        logging.warning(f\"No scenes found for {city} in {year}-{month}. Skipping to next month or year.\")\n",
    "                        if year not in checkpoint[\"missing_months\"]:\n",
    "                            checkpoint[\"missing_months\"][year] = {}\n",
    "                        if city not in checkpoint[\"missing_months\"][year]:\n",
    "                            checkpoint[\"missing_months\"][year][city] = \"1\"\n",
    "                        if city in checkpoint[\"missing_months\"][year]:\n",
    "                            checkpoint[\"missing_months\"][year][city] = str(int(checkpoint[\"missing_months\"][year][city]) + 1)                        \n",
    "                        break\n",
    "                        \n",
    "                    best_scene_item = scenes_with_cloud_cover[lstRetry][0]  # [0][0] gets the item from first tuple\n",
    "                    logging.info(f\"Selected scene with {scenes_with_cloud_cover[0][1]:.1f}% cloud cover\")\n",
    "\n",
    "                    # Load just that one scene\n",
    "                    landsat_rasters = stac_load(\n",
    "                        [best_scene_item],  # Load only the best scene\n",
    "                        bands=[\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"lwir11\", \"qa_pixel\"],\n",
    "                        bbox=bbox,\n",
    "                        resolution=30,\n",
    "                        crs=\"EPSG:3857\",\n",
    "                        progress=tqdm,\n",
    "                        skip_broken=True,\n",
    "                        fail_on_error=True\n",
    "                    )\n",
    "                    dem_rasters = stac_load(\n",
    "                        query_dem.items(),\n",
    "                        bands=[\"elevation\"],\n",
    "                        bbox=bbox,\n",
    "                        resolution=30,\n",
    "                        crs=\"EPSG:3857\",\n",
    "                        progress=tqdm,\n",
    "                        skip_broken=True,\n",
    "                        fail_on_error=True\n",
    "                    )\n",
    "                    print(f\"Loaded {len(landsat_rasters.time)} scenes\")\n",
    "\n",
    "                    def calculate_lst_fahrenheit(thermal_band):\n",
    "                        lst_kelvin = thermal_band * 0.00341802 + 149.0\n",
    "                        \n",
    "                        # Convert Kelvin to Fahrenheit: F = (K - 273.15) × 9/5 + 32\n",
    "                        lst_fahrenheit = (lst_kelvin - 273.15) * 9/5 + 32\n",
    "                        return lst_fahrenheit\n",
    "\n",
    "                    def convert_to_surface_reflectance(band_data):\n",
    "                        # Collection 2 Level 2 scaling factors\n",
    "                        scale_factor = 0.0000275\n",
    "                        add_offset = -0.2\n",
    "                        \n",
    "                        # Apply the conversion formula\n",
    "                        reflectance = band_data * scale_factor + add_offset\n",
    "                        \n",
    "                        # Clip to valid reflectance range [0, 1]\n",
    "                        reflectance = reflectance.clip(0, 1)\n",
    "                        return reflectance\n",
    "\n",
    "                    def calculate_ndvi(nir_band, red_band):\n",
    "                        nir_reflectance = convert_to_surface_reflectance(nir_band)\n",
    "                        red_reflectance = convert_to_surface_reflectance(red_band)\n",
    "                        numerator = nir_reflectance - red_reflectance\n",
    "                        denominator = nir_reflectance + red_reflectance\n",
    "                        ndvi = numerator.where(denominator != 0) / denominator.where(denominator != 0, 1)\n",
    "                        ndvi = np.clip(ndvi, -1, 1)\n",
    "                        return ndvi * 10_000\n",
    "\n",
    "                    def calculate_ndwi(nir_band, green_band):\n",
    "                        nir_reflectance = convert_to_surface_reflectance(nir_band)\n",
    "                        green_reflectance = convert_to_surface_reflectance(green_band)\n",
    "                        numerator = green_reflectance - nir_reflectance\n",
    "                        denominator = green_reflectance + nir_reflectance\n",
    "                        ndwi = numerator.where(denominator != 0) / denominator.where(denominator != 0, 1)\n",
    "                        ndwi = np.clip(ndwi, -1, 1)\n",
    "                        return ndwi * 10000\n",
    "\n",
    "                    def calculate_ndbi(nir_band, swir_band):\n",
    "                        nir_reflectance = convert_to_surface_reflectance(nir_band)\n",
    "                        swir_reflectance = convert_to_surface_reflectance(swir_band)\n",
    "                        numerator = swir_reflectance - nir_reflectance\n",
    "                        denominator = swir_reflectance + nir_reflectance\n",
    "                        ndbi = numerator.where(denominator != 0) / denominator.where(denominator != 0, 1)\n",
    "                        ndbi = np.clip(ndbi, -1, 1)\n",
    "                        return ndbi * 10000\n",
    "\n",
    "                    def calculate_albedo(blue_band, green_band, red_band, nir_band, swir_band):\n",
    "                        blue_refl = convert_to_surface_reflectance(blue_band).clip(0, 1)\n",
    "                        green_refl = convert_to_surface_reflectance(green_band).clip(0, 1)\n",
    "                        red_refl = convert_to_surface_reflectance(red_band).clip(0, 1)\n",
    "                        nir_refl = convert_to_surface_reflectance(nir_band).clip(0, 1)\n",
    "                        swir_refl = convert_to_surface_reflectance(swir_band).clip(0, 1)\n",
    "                        albedo = (\n",
    "                            0.356 * blue_refl +\n",
    "                            0.130 * green_refl +\n",
    "                            0.373 * red_refl +\n",
    "                            0.085 * nir_refl +\n",
    "                            0.072 * swir_refl -\n",
    "                            0.018\n",
    "                        )\n",
    "                        return albedo.clip(0, 1) * 10_000\n",
    "\n",
    "                    def calculate_color_band(color_band):\n",
    "                        reflectance = convert_to_surface_reflectance(color_band)\n",
    "                        return reflectance * 10000\n",
    "                    try:\n",
    "                        date = landsat_rasters.time.values[0]\n",
    "                        scene = landsat_rasters.isel(time=0)\n",
    "                        dem_band = dem_rasters['elevation']\n",
    "                        thermal_band = scene['lwir11']\n",
    "                        red_band = scene['red']\n",
    "                        green_band = scene['green']\n",
    "                        blue_band = scene['blue']\n",
    "                        nir_band = scene['nir08']\n",
    "                        swir_band = scene['swir16']\n",
    "                        qa_band = scene['qa_pixel']\n",
    "                        \n",
    "                        # Calculate integer format\n",
    "                        dem = dem_band + 10_000\n",
    "                        lst_fahrenheit = calculate_lst_fahrenheit(thermal_band)\n",
    "                        red255 = calculate_color_band(red_band)\n",
    "                        green255 = calculate_color_band(green_band)\n",
    "                        blue255 = calculate_color_band(blue_band)\n",
    "                        ndvi = calculate_ndvi(nir_band, red_band)\n",
    "                        ndwi = calculate_ndwi(nir_band, green_band)\n",
    "                        ndbi = calculate_ndbi(nir_band, swir_band)\n",
    "                        albedo = calculate_albedo(blue_band, green_band, red_band, nir_band, swir_band)\n",
    "\n",
    "                        # Create mask for valid pixels\n",
    "                        valid_thermal = thermal_band > 0\n",
    "                        clear_pixels = (qa_band & 0b00011000) == 0\n",
    "                        valid_mask = valid_thermal & clear_pixels\n",
    "                        \n",
    "                        # Apply mask (set invalid pixels to 0 instead of NaN)\n",
    "                        lst_fahrenheit_masked = lst_fahrenheit.where(valid_mask, NODATA)\n",
    "                        red_masked = red255.where(valid_mask, NODATA)\n",
    "                        green_masked = green255.where(valid_mask, NODATA)\n",
    "                        blue_masked = blue255.where(valid_mask, NODATA)\n",
    "                        ndvi_masked = ndvi.where(valid_mask, NODATA)\n",
    "                        ndwi_masked = ndwi.where(valid_mask, NODATA)\n",
    "                        ndbi_masked = ndbi.where(valid_mask, NODATA)\n",
    "                        albedo_masked = albedo.where(valid_mask, NODATA)\n",
    "\n",
    "                        # Create directory structure\n",
    "                        date_str = pd.to_datetime(date).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "                        output_dir = f\"./Data/Dataset/Cities/{city}/{date_str}\"\n",
    "                        output_dir_dem = f\"./Data/Dataset/DEM_2014/{city}\"                        \n",
    "\n",
    "                        # Now define file paths\n",
    "                        filename_dem = f\"./Data/Dataset/DEM_2014/{city}/DEM.tif\"\n",
    "                        filename_fahrenheit = f\"./Data/Dataset/Cities/{city}/{date_str}/LST.tif\"\n",
    "                        filename_red = f\"./Data/Dataset/Cities/{city}/{date_str}/red.tif\"\n",
    "                        filename_green = f\"./Data/Dataset/Cities/{city}/{date_str}/green.tif\"\n",
    "                        filename_blue = f\"./Data/Dataset/Cities/{city}/{date_str}/blue.tif\"\n",
    "                        filename_ndvi = f\"./Data/Dataset/Cities/{city}/{date_str}/ndvi.tif\"\n",
    "                        filename_ndwi = f\"./Data/Dataset/Cities/{city}/{date_str}/ndwi.tif\"\n",
    "                        filename_ndbi = f\"./Data/Dataset/Cities/{city}/{date_str}/ndbi.tif\"\n",
    "                        filename_albedo = f\"./Data/Dataset/Cities/{city}/{date_str}/albedo.tif\"\n",
    "\n",
    "                        # Set nodata values\n",
    "                        lst_fahrenheit_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        red_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        green_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        blue_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        ndvi_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        ndwi_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        ndbi_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "                        albedo_masked.rio.write_nodata(NODATA, inplace=True)\n",
    "\n",
    "                        valid_TIRS = lst_fahrenheit_masked.values[lst_fahrenheit_masked.values != NODATA]\n",
    "                        valid_OLI = albedo_masked.values[albedo_masked.values != NODATA]\n",
    "                        if len(valid_TIRS) > 0 and len(valid_OLI) > 0:\n",
    "                            os.makedirs(output_dir, exist_ok=True)\n",
    "                            os.makedirs(output_dir_dem, exist_ok=True)\n",
    "                            print(f\"Processing date: {date_str}\")\n",
    "                            print(f\"Output directory: {output_dir}\")\n",
    "                            print(f\"Directory exists: {os.path.exists(output_dir)}\")\n",
    "                            masked_paths = [dem, lst_fahrenheit_masked, red_masked, green_masked, blue_masked, ndvi_masked, ndwi_masked, ndbi_masked, albedo_masked]\n",
    "                            file_paths = [filename_dem, filename_fahrenheit, filename_red, filename_green, filename_blue, filename_ndvi, filename_ndwi, filename_ndbi, filename_albedo]\n",
    "                            for i in range(len(file_paths)):\n",
    "                                try:\n",
    "                                    if not os.path.exists(file_paths[i]):\n",
    "                                        masked_paths[i].rio.to_raster(file_paths[i], compress=\"LZW\", dtype='int16')\n",
    "                                        logging.info(f\"Saved: {file_paths[i]}\")\n",
    "                                    else:\n",
    "                                        logging.info(f\"File already exists: {file_paths[i]}\")\n",
    "                                except Exception as e:\n",
    "                                    logging.error(f\"Error saving file: {file_paths[i]}: {e}\")\n",
    "                            logging.info(\"Scene save complete!\")        \n",
    "                            break        \n",
    "                        else:\n",
    "                            if len(valid_TIRS) <= 0:\n",
    "                                logging.warning(f\"No valid temperature data for {city} {date_str}\")\n",
    "                            if len(valid_OLI) <= 0:                                \n",
    "                                logging.warning(f\"No valid spectral data for {city} {date_str}\")\n",
    "                            # Go back to after retry was set to 0 so we can try again                            \n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving scene {year}, {month}, {city}: {e}\")\n",
    "                        continue\n",
    "                # print(f\"Successfully processed and saved {rastersSaved} complete images\")\n",
    "                # Mark city as completed for this year\n",
    "            if year not in checkpoint[\"completed_cities\"]:\n",
    "                checkpoint[\"completed_cities\"][year] = []\n",
    "            checkpoint[\"completed_cities\"][year].append(city)\n",
    "            if year not in checkpoint[\"errored_cities\"]:\n",
    "                checkpoint[\"errored_cities\"][year] = []\n",
    "            if city in checkpoint[\"errored_cities\"][year]:\n",
    "                checkpoint[\"errored_cities\"][year].remove(city)\n",
    "            save_checkpoint(checkpoint)\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"Interrupted while processing {city} {year}\")\n",
    "            save_checkpoint(checkpoint)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            if year not in checkpoint[\"errored_cities\"]:\n",
    "                checkpoint[\"errored_cities\"][year] = []\n",
    "            checkpoint[\"errored_cities\"][year].append(city)\n",
    "            save_checkpoint(checkpoint)\n",
    "            logging.error(f\"Error processing {city} {year}: {e}\")\n",
    "            continue\n",
    "    if year not in checkpoint[\"errored_cities\"]:\n",
    "        checkpoint[\"errored_cities\"][year] = []\n",
    "    if len(checkpoint[\"errored_cities\"][year]) == 0:\n",
    "        checkpoint[\"completed_years\"].append(year)\n",
    "    checkpoint[\"current_city\"] = None\n",
    "    save_checkpoint(checkpoint)\n",
    "print(\"All processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7c719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
